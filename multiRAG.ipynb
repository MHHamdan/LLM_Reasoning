{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQucK2rlShNuN05OYdhfZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MHHamdan/LLM_Reasoning/blob/main/multiRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "-VX8GWVStSe9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FWfRtOkgtQNk"
      },
      "outputs": [],
      "source": [
        "\n",
        "#!pip install langchain openai faiss-cpu python-dotenv\n",
        "#!pip install langchain_community\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dependencies"
      ],
      "metadata": {
        "id": "SQl9v4bhtmZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "oJkR6ZhatYYd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up environment variables"
      ],
      "metadata": {
        "id": "xRlaBQqpuToW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-MohEMWmXXXXXXXXXXXXXX\"  # Replace with your API key\n"
      ],
      "metadata": {
        "id": "sk5p6HCRtt28"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement RetrievalRouterAgent"
      ],
      "metadata": {
        "id": "k3Ve1Axuux0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrievalRouterAgent:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "    def route_query(self, query: str) -> List[str]:\n",
        "        prompt = f\"\"\"Analyze this query and determine which data sources to search:\n",
        "        Query: {query}\n",
        "        Available sources: vector_db, web_search, email, chat\n",
        "        Return only the source names separated by comma.\"\"\"\n",
        "\n",
        "        response = self.llm(prompt)\n",
        "        return [source.strip() for source in response.split(',')]\n"
      ],
      "metadata": {
        "id": "RoP4Gvbdurlx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement RetrievalAgent"
      ],
      "metadata": {
        "id": "uPBpHLTPvBol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrievalAgent:\n",
        "    def __init__(self, source_type: str, tools: List[Tool]):\n",
        "        self.source_type = source_type\n",
        "        self.tools = tools\n",
        "\n",
        "    async def retrieve(self, query: str) -> Dict:\n",
        "        if self.source_type == \"vector_db\":\n",
        "            return await self._vector_search(query)\n",
        "        elif self.source_type == \"web_search\":\n",
        "            return await self._web_search(query)\n",
        "\n",
        "    async def _vector_search(self, query: str) -> Dict:\n",
        "        embeddings = OpenAIEmbeddings()\n",
        "        # Demo vector store with sample data\n",
        "        texts = [\"Sample document 1\", \"Sample document 2\"]\n",
        "        vector_store = FAISS.from_texts(texts, embeddings)\n",
        "        results = vector_store.similarity_search(query)\n",
        "        return {\"source\": \"vector_db\", \"results\": str(results)}\n",
        "\n",
        "    async def _web_search(self, query: str) -> Dict:\n",
        "        # Simplified web search simulation\n",
        "        return {\"source\": \"web_search\", \"results\": f\"Web search results for: {query}\"}\n"
      ],
      "metadata": {
        "id": "_FwW1Wj9u9sS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement MultiAgentRAG"
      ],
      "metadata": {
        "id": "gYtlTfcivOEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAgentRAG:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.router = RetrievalRouterAgent(llm)\n",
        "        self.retrieval_agents = {\n",
        "            \"vector_db\": RetrievalAgent(\"vector_db\", []),\n",
        "            \"web_search\": RetrievalAgent(\"web_search\", []),\n",
        "        }\n",
        "\n",
        "    async def process_query(self, query: str) -> str:\n",
        "        sources = self.router.route_query(query)\n",
        "\n",
        "        results = []\n",
        "        for source in sources:\n",
        "            if source in self.retrieval_agents:\n",
        "                result = await self.retrieval_agents[source].retrieve(query)\n",
        "                results.append(result)\n",
        "\n",
        "        return self._synthesize_results(query, results)\n",
        "\n",
        "    def _synthesize_results(self, query: str, results: List[Dict]) -> str:\n",
        "        context = \"\\n\".join([f\"From {r['source']}: {r['results']}\" for r in results])\n",
        "        prompt = f\"\"\"Based on the following information, answer the query:\n",
        "        Query: {query}\n",
        "        Context: {context}\n",
        "        \"\"\"\n",
        "        return self.llm(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "oEDjSR_WvIyn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n"
      ],
      "metadata": {
        "id": "biQNghu-vZtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_system():\n",
        "    llm = OpenAI(temperature=0)\n",
        "    rag_system = MultiAgentRAG(llm)\n",
        "    query = \"What are the latest developments in AI?\"\n",
        "    response = await rag_system.process_query(query)\n",
        "    print(f\"Query: {query}\\nResponse: {response}\")\n"
      ],
      "metadata": {
        "id": "2PlClubEvWPL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the test"
      ],
      "metadata": {
        "id": "TIwUJOyKvi7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "await test_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlMB5SfuviAI",
        "outputId": "539cb41a-e02e-4e6e-c272-010a73d583f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-f44870883a2c>:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI(temperature=0)\n",
            "<ipython-input-8-dbcdf4e7837d>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  self.memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "<ipython-input-8-dbcdf4e7837d>:12: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = self.llm(prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the latest developments in AI?\n",
            "Response: \n",
            "The latest developments in AI include advancements in natural language processing, computer vision, and machine learning algorithms. Some notable developments include the use of deep learning techniques for more accurate and efficient data analysis, the integration of AI into various industries such as healthcare and finance, and the development of AI-powered virtual assistants and chatbots. Additionally, there have been advancements in ethical considerations and regulations surrounding AI, as well as increased research and investment in the field.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41nrfXCUvsbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}