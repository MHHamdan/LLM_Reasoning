{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":26140,"sourceType":"modelInstanceVersion","modelInstanceId":22003,"modelId":3301}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WSDM Cup 2025: Multilingual Chatbot Response Preference Prediction\n\n## Overview\nThis notebook implements a solution for the WSDM Cup 2025 Multilingual Chatbot Arena competition. The goal is to predict user preferences between pairs of chatbot responses across multiple languages.\n\n### Competition Details\n- **Task**: Predict which response (A or B) users prefer\n- **Evaluation**: Categorization accuracy\n- **Runtime Limit**: 4.75 hours CPU/GPU for training\n- **Submission Format**: CSV with `id` and `winner` columns\n\n## Model Architecture\n- Base Model: Gemma-2-9B with 4-bit quantization\n- Fine-tuning: LoRA adaptation\n- Test-Time Augmentation: Response swap augmentation\n\n## Implementation Details\n\n### Data Processing\n- Input format: `<prompt> - <response_a> - <response_b>`\n- Max sequence length: 2300 tokens\n- Text cleaning: Remove \"null\" values\n- Dynamic batching based on sequence length\n\n### Model Configuration\n- Batch size: 4\n- Multi-GPU setup (2x NVIDIA Tesla T4)\n- Mixed precision inference\n- Parallel inference using ThreadPoolExecutor\n\n### Inference Pipeline\n1. Load and preprocess test data\n2. Tokenize inputs with special tokens\n3. Run parallel inference on two GPUs\n4. Apply test-time augmentation (optional)\n5. Generate final predictions\n\n## Performance Optimization\n- Dynamic padding for efficient batch processing\n- Length-based sorting for optimal batching\n- Parallel inference across two GPUs","metadata":{}},{"cell_type":"markdown","source":"# WSDM Cup 2025: Multilingual Chatbot Arena\n\n## Competition Overview\nThis notebook implements a solution for predicting user preferences between chatbot responses across multiple languages.\n\n### Task Description\n- Predict whether users prefer response A or B for given prompts\n- Model must handle multilingual inputs effectively\n- Evaluation based on categorization accuracy\n\n### Technical Setup\n- Model: Gemma-2-9B with 4-bit quantization and LoRA adaptation\n- Hardware: 2x NVIDIA Tesla T4 GPUs\n- Runtime Constraint: 4.75 hours for training\n\n### Solution Highlights\n- Multi-GPU parallel inference\n- Test-time augmentation (TTA)\n- Dynamic batching with sequence length optimization","metadata":{}},{"cell_type":"code","source":"# Install required packages from local wheel files\n!pip install transformers peft accelerate bitsandbytes \\\n    -U --no-index --find-links /kaggle/input/lmsys-wheel-files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T05:47:50.472622Z","iopub.execute_input":"2025-01-21T05:47:50.472928Z","iopub.status.idle":"2025-01-21T05:47:51.512204Z","shell.execute_reply.started":"2025-01-21T05:47:50.472901Z","shell.execute_reply":"2025-01-21T05:47:51.511082Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/lmsys-wheel-files\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n\u001b[33mWARNING: Location '/kaggle/input/lmsys-wheel-files' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n\u001b[33mWARNING: Location '/kaggle/input/lmsys-wheel-files' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n\u001b[33mWARNING: Location '/kaggle/input/lmsys-wheel-files' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Location '/kaggle/input/lmsys-wheel-files' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\n\nprint(\"Available directories in /kaggle/input:\")\nfor item in os.listdir('/kaggle/input'):\n    print(f\"- {item}\")\n    try:\n        subitems = os.listdir(f'/kaggle/input/{item}')\n        for subitem in subitems:\n            print(f\"  â””â”€â”€ {subitem}\")\n    except:\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:30:52.405905Z","iopub.execute_input":"2025-01-21T04:30:52.406158Z","iopub.status.idle":"2025-01-21T04:30:52.413477Z","shell.execute_reply.started":"2025-01-21T04:30:52.406136Z","shell.execute_reply":"2025-01-21T04:30:52.412790Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Configuration\n\nThis section defines the core parameters for model inference. The configuration includes:\n- Model paths for Gemma-2 and LoRA weights\n- Sequence length and batch size parameters\n- Device settings for multi-GPU inference\n- Test-time augmentation (TTA) settings","metadata":{}},{"cell_type":"code","source":"# Standard library imports\nimport time\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import List, Tuple, Dict, Any\n\n# Third-party imports\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Transformers imports\nfrom transformers import (\n    Gemma2ForSequenceClassification,\n    GemmaTokenizerFast,\n    BitsAndBytesConfig\n)\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom peft import PeftModel\n\n# Set up basic logging\nimport logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Verify CUDA environment\nif not torch.cuda.is_available():\n    raise RuntimeError(\"CUDA is required but not available\")\n\ngpu_count = torch.cuda.device_count()\nif gpu_count != 2:\n    raise RuntimeError(f\"This solution requires 2 GPUs, but found {gpu_count}\")\n\nlogger.info(f\"PyTorch version: {torch.__version__}\")\nlogger.info(f\"CUDA available: {torch.cuda.is_available()}\")\nlogger.info(f\"GPU count: {gpu_count}\")\nfor i in range(gpu_count):\n    logger.info(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:33:16.543953Z","iopub.execute_input":"2025-01-21T04:33:16.544344Z","iopub.status.idle":"2025-01-21T04:33:23.342894Z","shell.execute_reply.started":"2025-01-21T04:33:16.544311Z","shell.execute_reply":"2025-01-21T04:33:23.342250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass Config:\n    # Model paths\n    gemma_dir: str = '/kaggle/input/gemma/transformers/1.1-2b-it/1'  # Updated with complete path\n    lora_dir: str = '/kaggle/input/gemma/transformers/1.1-2b-it/1'   # Updated with complete path\n    \n    # Model parameters\n    max_length: int = 2300\n    batch_size: int = 4\n    device: torch.device = torch.device(\"cuda\")\n    \n    # Inference settings\n    tta: bool = True  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n    spread_max_length: bool = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n\n    def validate(self) -> None:\n        \"\"\"Validates configuration settings.\"\"\"\n        import os\n        \n        # Validate paths\n        assert os.path.exists(self.gemma_dir), f\"Gemma path not found: {self.gemma_dir}\"\n        assert os.path.isfile(os.path.join(self.gemma_dir, \"tokenizer.json\")), \"Tokenizer file not found\"\n        \n        # Validate numerical parameters\n        assert self.max_length > 0, \"max_length must be positive\"\n        assert self.batch_size > 0, \"batch_size must be positive\"\n        \n        # Validate CUDA availability\n        assert torch.cuda.is_available(), \"CUDA is required\"\n        assert torch.cuda.device_count() == 2, f\"Expected 2 GPUs, found {torch.cuda.device_count()}\"\n        \n        print(\"âœ“ Configuration validated successfully\")\n        print(f\"âœ“ Using device: {self.device}\")\n        print(f\"âœ“ Number of GPUs: {torch.cuda.device_count()}\")\n        print(f\"âœ“ Model directory contains required files\")\n\n# Initialize and validate configuration\ncfg = Config()\ncfg.validate()\n\n# Initialize tokenizer\nprint(\"\\nInitializing tokenizer...\")\ntokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\ntokenizer.add_eos_token = True\ntokenizer.padding_side = \"right\"\nprint(\"âœ“ Tokenizer initialized successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:33:26.526127Z","iopub.execute_input":"2025-01-21T04:33:26.526734Z","iopub.status.idle":"2025-01-21T04:33:27.599906Z","shell.execute_reply.started":"2025-01-21T04:33:26.526704Z","shell.execute_reply":"2025-01-21T04:33:27.599096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading and Preprocessing\n\nThis section handles:\n- Loading test data from parquet format\n- Text preprocessing to handle null values\n- Basic data cleaning and validation\n- Display of sample data for verification","metadata":{}},{"cell_type":"code","source":"# Load test data\ntest_path = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet'\nprint(f\"Loading test data from: {test_path}\")\ntest = pd.read_parquet(test_path)\n\ndef process_text(text: str) -> str:\n    \"\"\"Clean and process input text.\n    \n    Args:\n        text: Input text string\n        \n    Returns:\n        Cleaned text with null values removed\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    return text.replace(\"null\", \"\").strip()\n\n# Process all text columns\ntext_columns = ['prompt', 'response_a', 'response_b']\nfor col in text_columns:\n    test.loc[:, col] = test[col].apply(process_text)\n    \n# Display sample data\nprint(\"\\nSample data:\")\ndisplay(test.head(3))\n\n# Print basic statistics\nprint(\"\\nDataset statistics:\")\nprint(f\"Total samples: {len(test)}\")\nprint(f\"Columns: {', '.join(test.columns)}\")\nprint(\"\\nSample lengths:\")\nfor col in text_columns:\n    lengths = test[col].str.len()\n    print(f\"{col:10s}: min={lengths.min():5d}, mean={lengths.mean():8.1f}, max={lengths.max():7d}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:33:35.903443Z","iopub.execute_input":"2025-01-21T04:33:35.903734Z","iopub.status.idle":"2025-01-21T04:33:35.949758Z","shell.execute_reply.started":"2025-01-21T04:33:35.903713Z","shell.execute_reply":"2025-01-21T04:33:35.949085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Text Tokenization\n\nThis section handles the tokenization of input texts with the following features:\n\n### Input Processing\n- Adds special tokens: `<prompt>`, `<response_a>`, `<response_b>`\n- Handles multilingual input through Gemma's tokenizer\n- Supports both standard and spread-length tokenization modes\n\n### Length Control\n- Maximum sequence length: 2300 tokens\n- Optional spread mode to distribute tokens evenly across inputs\n- Dynamic padding for efficient batch processing\n\n### Data Preparation\n- Creates tokenized datasets for both original and augmented (TTA) inputs\n- Tracks sequence lengths for optimal batch organization\n- Prepares attention masks for transformer processing","metadata":{}},{"cell_type":"code","source":"import os\n\ndef print_directory_structure(path, prefix=\"\"):\n    \"\"\"Recursively print directory structure.\"\"\"\n    print(f\"{prefix}ðŸ“ {os.path.basename(path) or path}\")\n    \n    if os.path.isdir(path):\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isdir(item_path):\n                print_directory_structure(item_path, prefix + \"  \")\n            else:\n                print(f\"{prefix}  ðŸ“„ {item}\")\n\nprint(\"Available directories in /kaggle/input/gemma:\")\nprint_directory_structure('/kaggle/input/gemma')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:30:52.491850Z","iopub.status.idle":"2025-01-21T04:30:52.492285Z","shell.execute_reply":"2025-01-21T04:30:52.492101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize(\n    tokenizer, \n    prompt: list, \n    response_a: list, \n    response_b: list, \n    max_length: int = cfg.max_length, \n    spread_max_length: bool = cfg.spread_max_length\n) -> tuple:\n    \"\"\"Tokenize input texts with special tokens and length control.\"\"\"\n    # Add special tokens\n    prompt = [\"<prompt>: \" + p for p in prompt]\n    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n    \n    if spread_max_length:\n        # Distribute max_length across the three inputs\n        chunk_length = max_length // 3\n        prompt = tokenizer(prompt, max_length=chunk_length, truncation=True, padding=False).input_ids\n        response_a = tokenizer(response_a, max_length=chunk_length, truncation=True, padding=False).input_ids\n        response_b = tokenizer(response_b, max_length=chunk_length, truncation=True, padding=False).input_ids\n        \n        # Combine tokenized sequences\n        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        attention_mask = [[1] * len(i) for i in input_ids]\n    else:\n        # Concatenate texts and tokenize as single sequences\n        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n        input_ids = tokenized.input_ids\n        attention_mask = tokenized.attention_mask\n            \n    return input_ids, attention_mask\n\nprint(\"Creating datasets...\")\n# Create main dataset\ndata = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(\n    tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"]\n)\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\n# Create augmented dataset for TTA\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\naug_data[\"input_ids\"], aug_data[\"attention_mask\"] = tokenize(\n    tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"]  # Swapped responses\n)\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)\n\n# Print statistics\nprint(\"\\nTokenization Statistics:\")\nprint(f\"Original data - Sequences: {len(data)}, Max length: {data['length'].max()}\")\nprint(f\"Augmented data - Sequences: {len(aug_data)}, Max length: {aug_data['length'].max()}\")\n\n# Print sample output\nprint(\"\\nSample Output (first sequence):\")\nprint(f\"Original text:\")\nprint(tokenizer.decode(data[\"input_ids\"][0])[:200] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:33:44.984273Z","iopub.execute_input":"2025-01-21T04:33:44.984607Z","iopub.status.idle":"2025-01-21T04:33:45.021513Z","shell.execute_reply.started":"2025-01-21T04:33:44.984570Z","shell.execute_reply":"2025-01-21T04:33:45.020829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Loading and Setup\n\nThis section handles the loading and initialization of the Gemma model across two GPUs:\n\n### Architecture Details\n- Base Model: Gemma-2 with 4-bit quantization\n- Task: Sequence Classification for Response Preference\n- Multi-GPU Setup: Model replicated on 2 GPUs for parallel inference\n\n### Implementation Features\n- Separate model instances for parallel processing\n- LoRA adaptation for efficient fine-tuning\n- Disabled KV cache for inference optimization","metadata":{}},{"cell_type":"code","source":"# Clear any existing cache\ntorch.cuda.empty_cache()\n\nprint(\"Initializing models on multiple GPUs...\")\n\n# Configure devices\ndevice_0 = torch.device('cuda:0')\ndevice_1 = torch.device('cuda:1')\n\ndef load_model(device, model_id: int):\n    \"\"\"Load base model on specified device.\"\"\"\n    try:\n        print(f\"\\nLoading model {model_id} on {device}...\")\n        \n        # Load model with memory optimizations\n        model = Gemma2ForSequenceClassification.from_pretrained(\n            cfg.gemma_dir,\n            device_map=device,\n            use_cache=False,\n            torch_dtype=torch.float16,  # Use mixed precision\n            low_cpu_mem_usage=True,     # Optimize CPU memory usage\n            num_labels=3                # For model_a, model_b, and tie\n        )\n        \n        model.eval()  # Set to evaluation mode\n        print(f\"âœ“ Model {model_id} loaded successfully\")\n        return model\n    except Exception as e:\n        print(f\"Error loading model {model_id}: {e}\")\n        raise\n\n# Load models sequentially\nprint(\"\\nStep 1: Loading models...\")\n\n# Load first model\nprint(\"Loading model 0...\")\nmodel_0 = load_model(device_0, model_id=0)\n\n# Clear cache before loading second model\ntorch.cuda.empty_cache()\n\n# Load second model\nprint(\"\\nLoading model 1...\")\nmodel_1 = load_model(device_1, model_id=1)\n\nprint(\"\\nModel initialization completed\")\nprint(f\"Model 0 device: {next(model_0.parameters()).device}\")\nprint(f\"Model 1 device: {next(model_1.parameters()).device}\")\n\n# Print GPU memory usage\nfor i in range(torch.cuda.device_count()):\n    memory_allocated = torch.cuda.memory_allocated(i) / (1024 * 1024 * 1024)  # Convert to GB\n    memory_reserved = torch.cuda.memory_reserved(i) / (1024 * 1024 * 1024)    # Convert to GB\n    print(f\"GPU {i}: Allocated = {memory_allocated:.2f} GB, Reserved = {memory_reserved:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:35:24.493296Z","iopub.execute_input":"2025-01-21T04:35:24.493703Z","iopub.status.idle":"2025-01-21T04:35:35.754934Z","shell.execute_reply.started":"2025-01-21T04:35:24.493662Z","shell.execute_reply":"2025-01-21T04:35:35.754040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg.lora_dir = '/kaggle/input/wsdm-test-03'\ncfg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:36:44.180241Z","iopub.execute_input":"2025-01-21T04:36:44.180609Z","iopub.status.idle":"2025-01-21T04:36:44.185770Z","shell.execute_reply.started":"2025-01-21T04:36:44.180582Z","shell.execute_reply":"2025-01-21T04:36:44.184842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Inference\n\nThis section handles the inference pipeline with the following features:\n\n### Implementation Notes\n- Using mixed-precision inference with `torch.cuda.amp`\n- Dynamic batch processing for memory efficiency\n- Multi-GPU parallel processing\n- Test-time augmentation (TTA) for improved robustness\n\n### Memory Management\n- GPU 0: Currently using 9.34 GB (allocated)\n- GPU 1: Currently using 4.67 GB (allocated)\n- Dynamic padding to optimize memory usage","metadata":{}},{"cell_type":"code","source":"# Reset CUDA and free memory\nprint(\"Clearing GPU memory...\")\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n@torch.no_grad()\ndef run_inference_sequentially(data_df, gemma_dir, device):\n    \"\"\"Run inference with proper device management.\"\"\"\n    print(f\"\\nLoading model on {device}...\")\n    \n    from transformers import GemmaForSequenceClassification\n    \n    # Load model with explicit device mapping\n    model = GemmaForSequenceClassification.from_pretrained(\n        gemma_dir,\n        num_labels=2,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n        device_map={\"\": device}  # Ensure all model parts go to same device\n    )\n    model = model.to(device)  # Extra guarantee all parameters are on device\n    model.eval()\n    \n    results = []\n    print(f\"Processing {len(data_df)} samples...\")\n    \n    def ensure_tensor_on_device(t, dev):\n        \"\"\"Ensure tensor is on correct device.\"\"\"\n        return t.to(dev) if t.device != dev else t\n    \n    for idx in range(0, len(data_df)):\n        torch.cuda.empty_cache()\n        \n        try:\n            sample = data_df.iloc[idx:idx+1]\n            print(f\"\\nProcessing sample {idx+1}/{len(data_df)}\")\n            \n            # Process input sequence\n            max_chunk_size = 1024\n            input_ids = sample[\"input_ids\"].iloc[0]\n            attention_mask = sample[\"attention_mask\"].iloc[0]\n            \n            # Truncate if needed\n            if len(input_ids) > max_chunk_size:\n                print(f\"Truncating sequence from {len(input_ids)} to {max_chunk_size}\")\n                input_ids = input_ids[:max_chunk_size]\n                attention_mask = attention_mask[:max_chunk_size]\n            \n            # Create tensors and ensure they're on correct device\n            input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n            mask_tensor = torch.tensor([attention_mask], dtype=torch.long, device=device)\n            \n            # Double check device placement\n            input_tensor = ensure_tensor_on_device(input_tensor, device)\n            mask_tensor = ensure_tensor_on_device(mask_tensor, device)\n            \n            print(f\"Input tensor device: {input_tensor.device}\")\n            print(f\"Model device: {next(model.parameters()).device}\")\n            \n            # Run inference\n            with torch.amp.autocast('cuda'):\n                outputs = model(\n                    input_ids=input_tensor,\n                    attention_mask=mask_tensor\n                )\n                logits = outputs.logits\n            \n            # Compute probabilities\n            probs = torch.nn.functional.softmax(logits, dim=-1)\n            probs = probs.cpu().numpy()[0]\n            \n            print(f\"Output probabilities: {probs}\")\n            \n            results.append({\n                'id': sample['id'].iloc[0],\n                'winner_model_a': float(probs[0]),\n                'winner_model_b': float(probs[1])\n            })\n            \n            # Cleanup\n            del outputs, logits, probs, input_tensor, mask_tensor\n            torch.cuda.empty_cache()\n                \n        except Exception as e:\n            print(f\"Error processing sample {idx}: {e}\")\n            import traceback\n            print(traceback.format_exc())\n            results.append({\n                'id': sample['id'].iloc[0],\n                'winner_model_a': 0.0,\n                'winner_model_b': 1.0\n            })\n    \n    # Clean up\n    del model\n    torch.cuda.empty_cache()\n    \n    return pd.DataFrame(results)\n\n# Use single GPU\ndevice = torch.device('cuda:0')\nprint(f\"\\nRunning inference on {device}...\")\nresult_df = run_inference_sequentially(data, cfg.gemma_dir, device)\n\n# Generate predictions\nresult_df['winner'] = result_df.apply(\n    lambda row: 'model_a' if row['winner_model_a'] > 0.5 else 'model_b',\n    axis=1\n)\n\n# Create submission\nsubmission_df = result_df[['id', 'winner']].copy()\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nResults Summary:\")\nprint(f\"Total predictions: {len(submission_df)}\")\nprint(\"\\nPrediction distribution:\")\nprint(submission_df['winner'].value_counts(normalize=True))\nprint(\"\\nDetailed probability statistics:\")\nprint(result_df[['winner_model_a', 'winner_model_b']].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T04:54:04.231258Z","iopub.execute_input":"2025-01-21T04:54:04.231576Z","iopub.status.idle":"2025-01-21T04:54:10.783327Z","shell.execute_reply.started":"2025-01-21T04:54:04.231554Z","shell.execute_reply":"2025-01-21T04:54:10.782475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}