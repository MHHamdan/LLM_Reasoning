{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Types of Unconditioned Prediction in Neural Networks\n",
        "### Author: Mohammed HAMDAN © 2025\n",
        "\n",
        "This notebook provides practical implementations and visualizations of different types of unconditioned prediction in neural networks. We'll explore:\n",
        "1. Left-to-right Autoregressive Prediction\n",
        "2. Left-to-right Markov Chain\n",
        "3. Independent Prediction\n",
        "4. Bidirectional Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision matplotlib seaborn numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Autoregressive Prediction\n",
        "The autoregressive model predicts each token based on all previous tokens, similar to how humans read text from left to right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class AutoregressivePredictor(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        # Embedding layer converts token IDs to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        # GRU processes tokens sequentially\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        # Output layer converts hidden states to token predictions\n",
        "        self.output = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        return self.output(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Markov Chain Prediction\n",
        "The Markov model uses a fixed-size window of previous tokens, trading long-range understanding for computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MarkovPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, context_size: int = 3):\n",
        "        super().__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        # Uses concatenated embeddings from the context window\n",
        "        self.fc = nn.Linear(context_size * hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        embedded = self.embedding(x)\n",
        "        # Create sliding windows of context\n",
        "        batch_size, seq_len, hidden_size = embedded.shape\n",
        "        padded = F.pad(embedded, (0, 0, self.context_size-1, 0))\n",
        "        windows = torch.stack([padded[:, i:i+seq_len] \n",
        "                              for i in range(self.context_size)], dim=2)\n",
        "        flattened = windows.reshape(batch_size, seq_len, -1)\n",
        "        return self.fc(flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Independent Prediction\n",
        "The independent model makes predictions for each position without considering context, like a unigram model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class IndependentPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        # Direct mapping from embedding to prediction\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        embedded = self.embedding(x)\n",
        "        return self.fc(embedded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Bidirectional Prediction\n",
        "The bidirectional model uses both past and future context, like BERT's masked language modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BidirectionalPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        # Transformer layer allows attention to all positions\n",
        "        self.transformer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=4,\n",
        "            dim_feedforward=hidden_size*4,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        embedded = self.embedding(x)\n",
        "        encoded = self.transformer(embedded)\n",
        "        return self.output(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Functions\n",
        "These functions help us understand how each model uses context for predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_dependencies(sequence_length: int = 6):\n",
        "    \"\"\"Creates visualizations matching our poster's design\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    # Autoregressive pattern\n",
        "    auto_matrix = np.tril(np.ones((sequence_length, sequence_length)))\n",
        "    sns.heatmap(auto_matrix, ax=axes[0], cmap='Blues', cbar=False,\n",
        "                xticklabels=False, yticklabels=False)\n",
        "    axes[0].set_title('Autoregressive')\n",
        "    \n",
        "    # Markov pattern\n",
        "    markov_matrix = np.zeros((sequence_length, sequence_length))\n",
        "    context_size = 3\n",
        "    for i in range(sequence_length):\n",
        "        start = max(0, i - context_size + 1)\n",
        "        markov_matrix[i, start:i+1] = 1\n",
        "    sns.heatmap(markov_matrix, ax=axes[1], cmap='Blues', cbar=False,\n",
        "                xticklabels=False, yticklabels=False)\n",
        "    axes[1].set_title('Markov Chain')\n",
        "    \n",
        "    # Independent pattern\n",
        "    ind_matrix = np.eye(sequence_length)\n",
        "    sns.heatmap(ind_matrix, ax=axes[2], cmap='Blues', cbar=False,\n",
        "                xticklabels=False, yticklabels=False)\n",
        "    axes[2].set_title('Independent')\n",
        "    \n",
        "    # Bidirectional pattern\n",
        "    bi_matrix = np.ones((sequence_length, sequence_length))\n",
        "    sns.heatmap(bi_matrix, ax=axes[3], cmap='Blues', cbar=False,\n",
        "                xticklabels=False, yticklabels=False)\n",
        "    axes[3].set_title('Bidirectional')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstration\n",
        "Let's see how each model makes predictions on a sample sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_models():\n",
        "    # Set up parameters\n",
        "    vocab_size = 100\n",
        "    hidden_size = 64\n",
        "    sequence_length = 10\n",
        "    batch_size = 1\n",
        "    \n",
        "    # Create sample data\n",
        "    x = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "    \n",
        "    # Initialize models\n",
        "    models = {\n",
        "        'Autoregressive': AutoregressivePredictor(vocab_size, hidden_size),\n",
        "        'Markov': MarkovPredictor(vocab_size, hidden_size),\n",
        "        'Independent': IndependentPredictor(vocab_size, hidden_size),\n",
        "        'Bidirectional': BidirectionalPredictor(vocab_size, hidden_size)\n",
        "    }\n",
        "    \n",
        "    # Make predictions\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            predictions = torch.argmax(probs, dim=-1)\n",
        "            results[name] = predictions\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run demonstration\n",
        "results = demonstrate_models()\n",
        "\n",
        "# Visualize dependencies\n",
        "plt.figure(figsize=(12, 12))\n",
        "visualize_dependencies()\n",
        "plt.show()\n",
        "\n",
        "# Print results summary\n",
        "print(\"\\nPrediction Analysis Summary\")\n",
        "print(\"=\" * 50)\n",
        "for name, preds in results.items():\n",
        "    print(f\"\\n{name} Model:\")\n",
        "    print(f\"- Prediction shape: {preds.shape}\")\n",
        "    print(f\"- Unique predictions: {len(preds.unique())}\")\n",
        "\n",
        "print(\"\\nMohammed HAMDAN © 2025 • An analysis of prediction architectures in neural networks\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}