{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U langchain-community langchain-openai pypdf unstructured pandas numpy matplotlib seaborn scikit-learn nest-asyncio\n"
      ],
      "metadata": {
        "id": "_Rj9H6PlPoxA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m0ld3X4oPaoP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Union, Any\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# LangChain components\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    UnstructuredPDFLoader,\n",
        "    CSVLoader,\n",
        "    UnstructuredImageLoader,\n",
        "    JSONLoader\n",
        ")\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Analysis tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Document:\n",
        "    \"\"\"Base structure for document representation\"\"\"\n",
        "    content: str\n",
        "    metadata: Dict[str, Any]\n",
        "    doc_type: str\n",
        "    embedding: Optional[List[float]] = None\n",
        "\n",
        "@dataclass\n",
        "class QueryResult:\n",
        "    \"\"\"Enhanced query result structure\"\"\"\n",
        "    query: str\n",
        "    response: str\n",
        "    sources: List[Document]\n",
        "    response_time: float\n",
        "    confidence: float\n",
        "    analysis: Dict[str, Any]\n",
        "\n",
        "@dataclass\n",
        "class SystemMetrics:\n",
        "    \"\"\"Comprehensive system performance tracking\"\"\"\n",
        "    query_count: int\n",
        "    average_response_time: float\n",
        "    source_distribution: Dict[str, int]\n",
        "    success_rate: float\n",
        "    error_rate: float\n",
        "    document_stats: Dict[str, Any]"
      ],
      "metadata": {
        "id": "br_PwA4tPmYd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentManager:\n",
        "    \"\"\"Unified document management system for handling multiple document types\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings_model: OpenAIEmbeddings):\n",
        "        self.embeddings = embeddings_model\n",
        "        self.vector_store = None\n",
        "        self.document_cache = {}\n",
        "        self.supported_formats = {\n",
        "            '.pdf': UnstructuredPDFLoader,\n",
        "            '.csv': CSVLoader,\n",
        "            '.txt': TextLoader,\n",
        "            '.jpg': UnstructuredImageLoader,\n",
        "            '.png': UnstructuredImageLoader,\n",
        "            '.json': JSONLoader\n",
        "        }\n",
        "        # Initialize text splitter for document chunking\n",
        "        self.text_splitter = CharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separator=\"\\n\"\n",
        "        )\n",
        "\n",
        "    async def _load_document(self, loader: Any) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Load a document using the appropriate loader\n",
        "\n",
        "        Args:\n",
        "            loader: Document loader instance\n",
        "\n",
        "        Returns:\n",
        "            List of loaded documents\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Handle synchronous loaders\n",
        "            if hasattr(loader, 'load'):\n",
        "                documents = loader.load()\n",
        "            # Handle asynchronous loaders\n",
        "            elif hasattr(loader, 'aload'):\n",
        "                documents = await loader.aload()\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported loader type\")\n",
        "\n",
        "            return documents\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Document loading failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def _process_content(self, raw_docs: List[Any], doc_type: str) -> Document:\n",
        "        \"\"\"\n",
        "        Process raw document content into standardized format\n",
        "\n",
        "        Args:\n",
        "            raw_docs: List of raw document objects\n",
        "            doc_type: Type of document being processed\n",
        "\n",
        "        Returns:\n",
        "            Processed Document object\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Split documents into chunks\n",
        "            chunks = self.text_splitter.split_documents(raw_docs)\n",
        "\n",
        "            # Combine chunks with metadata\n",
        "            processed_content = \"\"\n",
        "            combined_metadata = {}\n",
        "\n",
        "            for chunk in chunks:\n",
        "                processed_content += chunk.page_content + \"\\n\"\n",
        "                # Merge metadata from all chunks\n",
        "                combined_metadata.update(chunk.metadata)\n",
        "\n",
        "            return Document(\n",
        "                content=processed_content.strip(),\n",
        "                metadata=combined_metadata,\n",
        "                doc_type=doc_type,\n",
        "                embedding=None  # Will be generated later\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Content processing failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def _generate_embedding(self, content: str) -> List[float]:\n",
        "        \"\"\"\n",
        "        Generate embeddings for document content\n",
        "\n",
        "        Args:\n",
        "            content: Document content to embed\n",
        "\n",
        "        Returns:\n",
        "            List of embedding values\n",
        "        \"\"\"\n",
        "        try:\n",
        "            embedding = await self.embeddings.aembed_query(content)\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Embedding generation failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def process_document(self, file_path: str) -> Document:\n",
        "        \"\"\"\n",
        "        Process a single document with appropriate loader\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the document file\n",
        "\n",
        "        Returns:\n",
        "            Processed Document object\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Check if document is already cached\n",
        "            if file_path in self.document_cache:\n",
        "                return self.document_cache[file_path]\n",
        "\n",
        "            ext = os.path.splitext(file_path)[1].lower()\n",
        "            if ext not in self.supported_formats:\n",
        "                raise ValueError(f\"Unsupported file format: {ext}\")\n",
        "\n",
        "            # Initialize appropriate loader\n",
        "            loader_class = self.supported_formats[ext]\n",
        "            loader = loader_class(file_path)\n",
        "\n",
        "            # Load and process document\n",
        "            raw_docs = await self._load_document(loader)\n",
        "            processed_doc = await self._process_content(raw_docs, ext)\n",
        "\n",
        "            # Generate and store embedding\n",
        "            embedding = await self._generate_embedding(processed_doc.content)\n",
        "            processed_doc.embedding = embedding\n",
        "\n",
        "            # Cache the processed document\n",
        "            self.document_cache[file_path] = processed_doc\n",
        "\n",
        "            return processed_doc\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Document processing failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def batch_process_documents(self, file_paths: List[str]) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Process multiple documents in parallel\n",
        "\n",
        "        Args:\n",
        "            file_paths: List of paths to document files\n",
        "\n",
        "        Returns:\n",
        "            List of processed Document objects\n",
        "        \"\"\"\n",
        "        try:\n",
        "            tasks = [self.process_document(path) for path in file_paths]\n",
        "            return await asyncio.gather(*tasks)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Batch processing failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    async def update_vector_store(self, documents: List[Document]):\n",
        "        \"\"\"\n",
        "        Update the vector store with new documents\n",
        "\n",
        "        Args:\n",
        "            documents: List of documents to add to vector store\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.vector_store is None:\n",
        "                # Initialize vector store\n",
        "                self.vector_store = FAISS.from_documents(\n",
        "                    [doc for doc in documents],\n",
        "                    self.embeddings\n",
        "                )\n",
        "            else:\n",
        "                # Add to existing vector store\n",
        "                self.vector_store.add_documents(documents)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Vector store update failed: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "diUTUdfLPgve"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiAgentRAG:\n",
        "    \"\"\"Advanced Multi-Agent RAG system with comprehensive document handling\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key: str, other_api_keys: Dict[str, str]):\n",
        "        self.document_manager = DocumentManager(\n",
        "            OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "        )\n",
        "        self.llm = OpenAI(openai_api_key=openai_api_key)\n",
        "        self.api_keys = other_api_keys\n",
        "        self.metrics = SystemMetrics(0, 0.0, {}, 0.0, 0.0, {})\n",
        "\n",
        "    async def initialize_knowledge_base(self, file_paths: List[str]):\n",
        "        \"\"\"Initialize system with document corpus\"\"\"\n",
        "        documents = await self.document_manager.batch_process_documents(file_paths)\n",
        "        self.vector_store = await self._create_vector_store(documents)\n",
        "        return len(documents)\n",
        "\n",
        "    async def process_query(self, query: str, domain: str = None) -> QueryResult:\n",
        "        \"\"\"Process a query with domain-specific handling\"\"\"\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Retrieve relevant documents\n",
        "            relevant_docs = await self._retrieve_documents(query)\n",
        "\n",
        "            # Get external information\n",
        "            external_info = await self._gather_external_info(query)\n",
        "\n",
        "            # Process with domain-specific logic\n",
        "            if domain:\n",
        "                processor = DomainProcessor(domain)\n",
        "                analysis = await processor.process_query(\n",
        "                    query, relevant_docs, external_info\n",
        "                )\n",
        "            else:\n",
        "                analysis = await self._general_processing(\n",
        "                    query, relevant_docs, external_info\n",
        "                )\n",
        "\n",
        "            response_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "            return QueryResult(\n",
        "                query=query,\n",
        "                response=analysis['response'],\n",
        "                sources=relevant_docs,\n",
        "                response_time=response_time,\n",
        "                confidence=analysis['confidence'],\n",
        "                analysis=analysis\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Query processing failed: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "fye5HRbGQIfF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APIManager:\n",
        "    \"\"\"Manages API keys and authentication for the Multi-Agent RAG system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.api_statuses = {}\n",
        "\n",
        "    def setup_openai(self, api_key: str) -> bool:\n",
        "        \"\"\"\n",
        "        Configure OpenAI API access\n",
        "        \"\"\"\n",
        "        try:\n",
        "            os.environ['OPENAI_API_KEY'] = api_key\n",
        "            # Verify the API key works\n",
        "            test_llm = OpenAI(api_key=api_key)\n",
        "            test_response = test_llm.invoke(\"Test\")\n",
        "            self.api_statuses['openai'] = True\n",
        "            logger.info(\"OpenAI API configured successfully\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OpenAI API setup failed: {str(e)}\")\n",
        "            self.api_statuses['openai'] = False\n",
        "            return False\n",
        "\n",
        "    def setup_external_apis(self, api_keys: dict) -> dict:\n",
        "        \"\"\"\n",
        "        Configure external APIs (Google, Scientific DB, News)\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for api_name, api_key in api_keys.items():\n",
        "            try:\n",
        "                os.environ[api_name] = api_key\n",
        "                results[api_name] = self._verify_api_key(api_name, api_key)\n",
        "                self.api_statuses[api_name] = results[api_name]\n",
        "            except Exception as e:\n",
        "                logger.error(f\"{api_name} setup failed: {str(e)}\")\n",
        "                results[api_name] = False\n",
        "                self.api_statuses[api_name] = False\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _verify_api_key(self, api_name: str, api_key: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verify that an API key is valid by making a test request\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if api_name == \"GOOGLE_API_KEY\":\n",
        "                # Test Google API\n",
        "                return self._test_google_api(api_key)\n",
        "            elif api_name == \"SCIENTIFIC_DB_KEY\":\n",
        "                # Test Scientific DB API\n",
        "                return self._test_scientific_db(api_key)\n",
        "            elif api_name == \"NEWS_API_KEY\":\n",
        "                # Test News API\n",
        "                return self._test_news_api(api_key)\n",
        "            return False\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "class EnhancedMultiAgentRAG:\n",
        "    \"\"\"Enhanced Multi-Agent RAG system with API integration\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.api_manager = APIManager()\n",
        "        self.document_manager = None\n",
        "        self.llm = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    async def initialize(self, openai_key: str, external_api_keys: dict) -> bool:\n",
        "        \"\"\"\n",
        "        Initialize the system with all necessary APIs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Setup OpenAI\n",
        "            if not self.api_manager.setup_openai(openai_key):\n",
        "                raise ValueError(\"OpenAI API setup failed\")\n",
        "\n",
        "            # Setup external APIs\n",
        "            api_results = self.api_manager.setup_external_apis(external_api_keys)\n",
        "            if not all(api_results.values()):\n",
        "                logger.warning(\"Some external APIs failed to initialize\")\n",
        "\n",
        "            # Initialize core components\n",
        "            self.llm = OpenAI(api_key=openai_key)\n",
        "            self.embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
        "            self.document_manager = DocumentManager(self.embeddings)\n",
        "\n",
        "            logger.info(\"Multi-Agent RAG system initialized successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"System initialization failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def process_query_with_documents(self,\n",
        "                                         query: str,\n",
        "                                         documents: List[str],\n",
        "                                         domain: str = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Process a query using provided documents and APIs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Process documents\n",
        "            processed_docs = await self.document_manager.batch_process_documents(documents)\n",
        "\n",
        "            # Update vector store\n",
        "            await self.document_manager.update_vector_store(processed_docs)\n",
        "\n",
        "            # Get relevant documents\n",
        "            relevant_docs = self.document_manager.vector_store.similarity_search(query)\n",
        "\n",
        "            # Get external information if APIs are available\n",
        "            external_info = await self._gather_external_info(query)\n",
        "\n",
        "            # Combine information and generate response\n",
        "            response = await self._generate_response(query, relevant_docs, external_info)\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Query processing failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Example usage\n",
        "async def main():\n",
        "    \"\"\"Demonstrate the enhanced Multi-Agent RAG system\"\"\"\n",
        "\n",
        "    # Initialize system with APIs\n",
        "    openai_key = \"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\"\n",
        "\n",
        "    external_api_keys = {\n",
        "        \"GOOGLE_API_KEY\": \"AIzaSyD2GUnxJp0X8rurOdXWGe3Mcqj3C8z_pW8\",\n",
        "        \"SCIENTIFIC_DB_KEY\": \"tel8v4ue3VMKbsyQZ4DfajDaR6dYGyBYB52MuZtm\",\n",
        "        \"NEWS_API_KEY\": \"pub_666321d0d81657c45e48ce104d6e75349825d\"\n",
        "    }\n",
        "\n",
        "    # Create and initialize system\n",
        "    rag_system = EnhancedMultiAgentRAG()\n",
        "    initialized = await rag_system.initialize(openai_key, external_api_keys)\n",
        "\n",
        "    if initialized:\n",
        "        # Example documents (you would replace these with actual file paths)\n",
        "        documents = [\n",
        "            \"quantum_paper.pdf\",\n",
        "            \"research_data.csv\",\n",
        "            \"circuit_diagram.png\"\n",
        "        ]\n",
        "\n",
        "        # Process a query\n",
        "        query = \"What are the latest developments in quantum error correction?\"\n",
        "        result = await rag_system.process_query_with_documents(\n",
        "            query=query,\n",
        "            documents=documents,\n",
        "            domain=\"quantum_computing\"\n",
        "        )\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nQuery Results:\")\n",
        "        print(f\"Response: {result['response']}\")\n",
        "        print(f\"Confidence: {result['confidence']}\")\n",
        "        print(\"\\nSources Used:\")\n",
        "        for source in result['sources']:\n",
        "            print(f\"- {source['type']}: {source['title']}\")\n",
        "\n",
        "    else:\n",
        "        print(\"System initialization failed\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "DHFtrgqYQVZ9",
        "outputId": "dd5368df-1626-4c01-999a-d2a7bd3c627c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'List' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4c2888de6906>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEnhancedMultiAgentRAG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;34m\"\"\"Enhanced Multi-Agent RAG system with API integration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-4c2888de6906>\u001b[0m in \u001b[0;36mEnhancedMultiAgentRAG\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     async def process_query_with_documents(self, \n\u001b[1;32m     95\u001b[0m                                          \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                                          \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                                          domain: str = None) -> Dict:\n\u001b[1;32m     98\u001b[0m         \"\"\"\n",
            "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FTDBL_5S8Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Core imports\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Union, Any\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# LangChain components\n",
        "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader,\n",
        "    UnstructuredPDFLoader,\n",
        "    CSVLoader,\n",
        "    UnstructuredImageLoader\n",
        ")\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Analysis tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "3bMgGmSgS8ZU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class APIConfig:\n",
        "    \"\"\"Configuration for API management\"\"\"\n",
        "    openai_key: str\n",
        "    google_key: str\n",
        "    scientific_db_key: str\n",
        "    news_key: str\n",
        "\n",
        "    @classmethod\n",
        "    def from_environment(cls):\n",
        "        \"\"\"Create configuration from environment variables\"\"\"\n",
        "        return cls(\n",
        "            openai_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "            google_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "            scientific_db_key=os.getenv(\"SCIENTIFIC_DB_KEY\"),\n",
        "            news_key=os.getenv(\"NEWS_API_KEY\")\n",
        "        )\n",
        "\n",
        "class APIManager:\n",
        "    \"\"\"Manages API authentication and verification\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.api_statuses = {}\n",
        "\n",
        "    def setup_openai(self, api_key: str) -> bool:\n",
        "        \"\"\"Configure OpenAI API access\"\"\"\n",
        "        try:\n",
        "            os.environ['OPENAI_API_KEY'] = api_key\n",
        "            test_llm = OpenAI(api_key=api_key)\n",
        "            self.api_statuses['openai'] = True\n",
        "            logger.info(\"OpenAI API configured successfully\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OpenAI API setup failed: {str(e)}\")\n",
        "            self.api_statuses['openai'] = False\n",
        "            return False\n",
        "\n",
        "    def setup_external_apis(self, api_keys: Dict[str, str]) -> Dict[str, bool]:\n",
        "        \"\"\"Configure external API access\"\"\"\n",
        "        results = {}\n",
        "        for api_name, api_key in api_keys.items():\n",
        "            try:\n",
        "                os.environ[api_name] = api_key\n",
        "                results[api_name] = True\n",
        "                self.api_statuses[api_name] = True\n",
        "                logger.info(f\"{api_name} configured successfully\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"{api_name} setup failed: {str(e)}\")\n",
        "                results[api_name] = False\n",
        "                self.api_statuses[api_name] = False\n",
        "        return results\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles document loading and processing\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings: OpenAIEmbeddings):\n",
        "        self.embeddings = embeddings\n",
        "        self.text_splitter = CharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "\n",
        "    async def process_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single file\"\"\"\n",
        "        try:\n",
        "            # Determine file type and load appropriate loader\n",
        "            if file_path.endswith('.pdf'):\n",
        "                loader = UnstructuredPDFLoader(file_path)\n",
        "            elif file_path.endswith('.csv'):\n",
        "                loader = CSVLoader(file_path)\n",
        "            elif file_path.endswith('.txt'):\n",
        "                loader = TextLoader(file_path)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file type: {file_path}\")\n",
        "\n",
        "            # Load and process document\n",
        "            docs = loader.load()\n",
        "            chunks = self.text_splitter.split_documents(docs)\n",
        "\n",
        "            # Generate embeddings\n",
        "            embeddings = await self.embeddings.aembed_documents(\n",
        "                [chunk.page_content for chunk in chunks]\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'chunks': chunks,\n",
        "                'embeddings': embeddings,\n",
        "                'metadata': {'source': file_path}\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"File processing failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "class EnhancedMultiAgentRAG:\n",
        "    \"\"\"Enhanced Multi-Agent RAG system with comprehensive document handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.api_manager = APIManager()\n",
        "        self.document_processor = None\n",
        "        self.vector_store = None\n",
        "        self.llm = None\n",
        "\n",
        "    async def initialize(self, config: APIConfig) -> bool:\n",
        "        \"\"\"Initialize the system with API configuration\"\"\"\n",
        "        try:\n",
        "            # Setup APIs\n",
        "            if not self.api_manager.setup_openai(config.openai_key):\n",
        "                raise ValueError(\"OpenAI API setup failed\")\n",
        "\n",
        "            external_apis = {\n",
        "                \"GOOGLE_API_KEY\": config.google_key,\n",
        "                \"SCIENTIFIC_DB_KEY\": config.scientific_db_key,\n",
        "                \"NEWS_API_KEY\": config.news_key\n",
        "            }\n",
        "            self.api_manager.setup_external_apis(external_apis)\n",
        "\n",
        "            # Initialize components\n",
        "            self.llm = OpenAI(api_key=config.openai_key)\n",
        "            embeddings = OpenAIEmbeddings(openai_api_key=config.openai_key)\n",
        "            self.document_processor = DocumentProcessor(embeddings)\n",
        "\n",
        "            logger.info(\"System initialized successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Initialization failed: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    async def process_query(self, query: str, documents: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Process a query using provided documents\"\"\"\n",
        "        try:\n",
        "            # Process documents\n",
        "            results = []\n",
        "            for doc_path in documents:\n",
        "                doc_result = await self.document_processor.process_file(doc_path)\n",
        "                results.append(doc_result)\n",
        "\n",
        "            # Create or update vector store\n",
        "            if self.vector_store is None:\n",
        "                self.vector_store = FAISS.from_documents(\n",
        "                    [chunk for result in results for chunk in result['chunks']],\n",
        "                    self.document_processor.embeddings\n",
        "                )\n",
        "\n",
        "            # Perform similarity search\n",
        "            relevant_docs = self.vector_store.similarity_search(query, k=3)\n",
        "\n",
        "            # Generate response\n",
        "            context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "            prompt = f\"Based on the following context, answer the query: {query}\\n\\nContext: {context}\"\n",
        "            response = self.llm.invoke(prompt)\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'response': response,\n",
        "                'sources': [doc.metadata for doc in relevant_docs],\n",
        "                'timestamp': datetime.now()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Query processing failed: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "v3WnTQ1HQm7a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's create a function to set up our test environment with sample documents\n",
        "async def setup_test_environment():\n",
        "    \"\"\"\n",
        "    Create sample documents for testing the RAG system.\n",
        "    Returns the directory path containing the sample files.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a test directory\n",
        "        test_dir = \"test_documents\"\n",
        "        os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "        # Create a sample text file\n",
        "        with open(os.path.join(test_dir, \"sample.txt\"), \"w\") as f:\n",
        "            f.write(\"\"\"\n",
        "            Quantum Computing Research Update\n",
        "\n",
        "            Recent developments in quantum error correction have shown promising results.\n",
        "            Scientists have achieved a 99.9% fidelity rate in qubit operations using the\n",
        "            new surface code implementation. This breakthrough could lead to more stable\n",
        "            quantum computers in the near future.\n",
        "            \"\"\")\n",
        "\n",
        "        # Create a sample CSV file\n",
        "        with open(os.path.join(test_dir, \"quantum_results.csv\"), \"w\") as f:\n",
        "            f.write(\"\"\"\n",
        "            date,experiment,success_rate,error_rate\n",
        "            2024-01-15,Surface Code,0.999,0.001\n",
        "            2024-01-16,Topological Quantum,0.995,0.005\n",
        "            2024-01-17,Error Correction,0.997,0.003\n",
        "            \"\"\")\n",
        "\n",
        "        logger.info(f\"Test environment created in {test_dir}\")\n",
        "        return test_dir\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create test environment: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Enhanced DocumentProcessor with better error handling\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles document loading and processing with comprehensive error handling\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings: OpenAIEmbeddings):\n",
        "        self.embeddings = embeddings\n",
        "        self.text_splitter = CharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "\n",
        "    async def process_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single file with detailed error handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Verify file exists\n",
        "            if not os.path.exists(file_path):\n",
        "                raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "            # Verify file is readable\n",
        "            if not os.access(file_path, os.R_OK):\n",
        "                raise PermissionError(f\"Cannot read file: {file_path}\")\n",
        "\n",
        "            # Determine file type and load appropriate loader\n",
        "            file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "            if file_extension == '.pdf':\n",
        "                loader = UnstructuredPDFLoader(file_path)\n",
        "                logger.info(f\"Processing PDF file: {file_path}\")\n",
        "            elif file_extension == '.csv':\n",
        "                loader = CSVLoader(file_path)\n",
        "                logger.info(f\"Processing CSV file: {file_path}\")\n",
        "            elif file_extension == '.txt':\n",
        "                loader = TextLoader(file_path)\n",
        "                logger.info(f\"Processing text file: {file_path}\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "            # Load and process document\n",
        "            logger.info(f\"Loading content from {file_path}\")\n",
        "            docs = loader.load()\n",
        "\n",
        "            if not docs:\n",
        "                raise ValueError(f\"No content loaded from {file_path}\")\n",
        "\n",
        "            logger.info(f\"Splitting content into chunks\")\n",
        "            chunks = self.text_splitter.split_documents(docs)\n",
        "\n",
        "            # Generate embeddings\n",
        "            logger.info(f\"Generating embeddings for {len(chunks)} chunks\")\n",
        "            embeddings = await self.embeddings.aembed_documents(\n",
        "                [chunk.page_content for chunk in chunks]\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'chunks': chunks,\n",
        "                'embeddings': embeddings,\n",
        "                'metadata': {\n",
        "                    'source': file_path,\n",
        "                    'chunk_count': len(chunks),\n",
        "                    'processed_at': datetime.now().isoformat()\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logger.error(f\"File not found: {file_path}\")\n",
        "            raise\n",
        "        except PermissionError as e:\n",
        "            logger.error(f\"Permission denied: {file_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Updated main function with proper setup and error handling\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Demonstrate the enhanced Multi-Agent RAG system with proper setup and error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # First, set up our test environment\n",
        "        test_dir = await setup_test_environment()\n",
        "\n",
        "        # Setup API configuration\n",
        "        api_config = APIConfig(\n",
        "            openai_key=\"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\",\n",
        "            google_key=\"AIzaSyD2GUnxJp0X8rurOdXWGe3Mcqj3C8z_pW8\",\n",
        "            scientific_db_key=\"tel8v4ue3VMKbsyQZ4DfajDaR6dYGyBYB52MuZtm\",\n",
        "            news_key=\"pub_666321d0d81657c45e48ce104d6e75349825d\"\n",
        "        )\n",
        "\n",
        "        # Initialize system\n",
        "        logger.info(\"Initializing RAG system\")\n",
        "        rag_system = EnhancedMultiAgentRAG()\n",
        "        initialized = await rag_system.initialize(api_config)\n",
        "\n",
        "        if initialized:\n",
        "            # Prepare document paths\n",
        "            documents = [\n",
        "                os.path.join(test_dir, \"sample.txt\"),\n",
        "                os.path.join(test_dir, \"quantum_results.csv\")\n",
        "            ]\n",
        "\n",
        "            # Verify all documents exist\n",
        "            for doc in documents:\n",
        "                if not os.path.exists(doc):\n",
        "                    raise FileNotFoundError(f\"Document not found: {doc}\")\n",
        "\n",
        "            logger.info(f\"Processing query with {len(documents)} documents\")\n",
        "\n",
        "            # Process query\n",
        "            query = \"What are the latest developments in quantum error correction?\"\n",
        "            result = await rag_system.process_query(query, documents)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\nQuery Results:\")\n",
        "            print(f\"Query: {result['query']}\")\n",
        "            print(f\"Response: {result['response']}\")\n",
        "            print(\"\\nSources Used:\")\n",
        "            for source in result['sources']:\n",
        "                print(f\"- {source['source']}\")\n",
        "\n",
        "            # Optional: Display processing metrics\n",
        "            print(\"\\nProcessing Metrics:\")\n",
        "            print(f\"Timestamp: {result['timestamp']}\")\n",
        "            print(f\"Number of sources used: {len(result['sources'])}\")\n",
        "\n",
        "        else:\n",
        "            print(\"System initialization failed\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"File not found error: {str(e)}\")\n",
        "    except PermissionError as e:\n",
        "        print(f\"Permission error: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "    finally:\n",
        "        # Cleanup test environment if needed\n",
        "        if 'test_dir' in locals():\n",
        "            logger.info(f\"Cleaning up test environment: {test_dir}\")\n",
        "            # Uncomment the following line to enable cleanup\n",
        "            # shutil.rmtree(test_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup async environment\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Configure logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # Run the example\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTDTti0LTd_8",
        "outputId": "64f1f5eb-fb96-4f80-e8c5-66c43484b258"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Query processing failed: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOLnwlqdTkzj"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}