{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Essay Generation for LLM-as-a-Judge Competition\n\nThis notebook implements a system for generating essays that maximize disagreement between LLM judges while maintaining high quality. The goal is to explore vulnerabilities in automated evaluation systems and contribute to the development of more robust AI-based assessment tools.\n\n---\n\n## **1. Setup and Imports**\n\nFirst, we import all necessary libraries and set up logging for tracking progress and errors.","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport pandas as pd\nimport numpy as np\nimport re\nimport random\nimport time\nfrom tqdm import tqdm\nimport logging\nfrom typing import Dict, List, Optional, Tuple\nimport json\nfrom difflib import SequenceMatcher\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Initialize logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler(),  # Log to console\n        logging.FileHandler('essay_generation.log')  # Also log to file\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Force logging to be shown immediately\nlogger.setLevel(logging.INFO)\nfor handler in logger.handlers:\n    handler.setLevel(logging.INFO)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:04:47.161428Z","iopub.execute_input":"2025-01-20T00:04:47.161741Z","iopub.status.idle":"2025-01-20T00:05:07.357570Z","shell.execute_reply.started":"2025-01-20T00:04:47.161714Z","shell.execute_reply":"2025-01-20T00:05:07.356666Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"---\n\n## **2. Competition Configuration**\n\nDefine the competition-specific parameters, including target essay length, quality thresholds, and scoring formula components.","metadata":{}},{"cell_type":"code","source":"# Competition scoring formula:\n# final_score = (avg_h × min_v × avg_e) / (avg_s × (9 - avg_q))\nCOMPETITION_CONFIG = {\n    'target_length': 100,       # Target essay length\n    'length_tolerance': 15,     # Stricter tolerance\n    'min_quality_score': 7.0,   # Target higher quality\n    'max_quality_score': 8.5,   # Not too perfect to allow disagreement\n    'target_variance': 2.5,     # High variance between judges\n    'min_english_score': 0.95,  # Very high English quality\n    'max_similarity': 0.25      # Strict similarity threshold\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:07.358643Z","iopub.execute_input":"2025-01-20T00:05:07.359286Z","iopub.status.idle":"2025-01-20T00:05:07.363293Z","shell.execute_reply.started":"2025-01-20T00:05:07.359236Z","shell.execute_reply":"2025-01-20T00:05:07.362511Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"---\n\n## **3. Model Configuration**\n\nConfigure the model and tokenizer settings for essay generation. We use `gpt2-medium` for better performance.","metadata":{}},{"cell_type":"code","source":"# Enhanced model configuration\nMODEL_CONFIG = {\n    'name': 'gpt2-medium',      # More capable model\n    'path': \"gpt2-medium\",\n    'params': {\n        'max_new_tokens': 250,  # Longer generation\n        'temperature': 1.1,     # Higher creativity\n        'top_p': 0.95,         # More diversity\n        'top_k': 100,          # More options\n        'repetition_penalty': 1.8,\n        'do_sample': True,\n        'no_repeat_ngram_size': 4,\n        'num_return_sequences': 1,\n        'pad_token_id': 50256   # GPT-2 specific\n    },\n    'tokenizer_config': {\n        'padding': True,\n        'truncation': True,\n        'max_length': 512\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:13.877342Z","iopub.execute_input":"2025-01-20T00:05:13.877672Z","iopub.status.idle":"2025-01-20T00:05:13.881838Z","shell.execute_reply.started":"2025-01-20T00:05:13.877644Z","shell.execute_reply":"2025-01-20T00:05:13.881052Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"---\n\n## **4. Style Configurations**\n\nDefine different writing styles to maximize disagreement between judges. Each style has a unique tone, structure, and approach.","metadata":{}},{"cell_type":"code","source":"# Carefully crafted styles to maximize disagreement\nSTYLE_CONFIGS = {\n    'controversial': {\n        'tone': 'logically provocative',\n        'structure': 'clear but challenging',\n        'approach': 'present unconventional perspectives',\n        'prompt_elements': [\n            'challenge established norms while maintaining logical coherence',\n            'use sophisticated rhetoric to present contentious ideas',\n            'balance controversial claims with solid reasoning'\n        ]\n    },\n    'ambiguous': {\n        'tone': 'deliberately nuanced',\n        'structure': 'multi-layered analysis',\n        'approach': 'explore multiple valid interpretations',\n        'prompt_elements': [\n            'present valid but competing viewpoints',\n            'use precise language that allows multiple readings',\n            'maintain logical consistency while allowing ambiguity'\n        ]\n    },\n    'philosophical': {\n        'tone': 'deeply analytical',\n        'structure': 'complex reasoning',\n        'approach': 'examine fundamental assumptions',\n        'prompt_elements': [\n            'question core premises while maintaining rigor',\n            'use abstract reasoning to challenge intuitions',\n            'explore edge cases and counterintuitive conclusions'\n        ]\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:23.115457Z","iopub.execute_input":"2025-01-20T00:05:23.115742Z","iopub.status.idle":"2025-01-20T00:05:23.120050Z","shell.execute_reply.started":"2025-01-20T00:05:23.115722Z","shell.execute_reply":"2025-01-20T00:05:23.119221Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"---\n\n## **5. Model Manager**\n\nThe `ModelManager` class handles model initialization and GPU memory management.","metadata":{}},{"cell_type":"code","source":"class ModelManager:\n    \"\"\"Enhanced model management with better error handling\"\"\"\n    \n    @staticmethod\n    def cleanup_models():\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            logger.info(\"GPU memory cleared\")\n    \n    @staticmethod\n    def initialize_model() -> pipeline:\n        try:\n            ModelManager.cleanup_models()\n            \n            # Initialize tokenizer\n            tokenizer = AutoTokenizer.from_pretrained(\n                MODEL_CONFIG['path'],\n                trust_remote_code=True,\n                padding_side='left',\n                **MODEL_CONFIG['tokenizer_config']\n            )\n            \n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            \n            # Load model with optimizations\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_CONFIG['path'],\n                device_map=\"auto\",\n                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n                trust_remote_code=True,\n                low_cpu_mem_usage=True\n            )\n            \n            # Create pipeline without device specification\n            generation_pipe = pipeline(\n                task=\"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                trust_remote_code=True,\n                **MODEL_CONFIG['params']\n            )\n            \n            logger.info(f\"Model {MODEL_CONFIG['name']} initialized successfully\")\n            return generation_pipe\n            \n        except Exception as e:\n            logger.error(f\"Model initialization failed: {str(e)}\")\n            raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:28.445231Z","iopub.execute_input":"2025-01-20T00:05:28.445516Z","iopub.status.idle":"2025-01-20T00:05:28.451417Z","shell.execute_reply.started":"2025-01-20T00:05:28.445485Z","shell.execute_reply":"2025-01-20T00:05:28.450675Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"---\n\n## **6. Prompt Engineering**\n\nThe `PromptEngineering` class generates detailed prompts for essay generation, tailored to specific styles.","metadata":{}},{"cell_type":"code","source":"class PromptEngineering:\n    \"\"\"Enhanced prompt engineering\"\"\"\n    \n    @staticmethod\n    def get_prompt(topic: str, style_name: str) -> str:\n        \"\"\"Generate a more detailed prompt\"\"\"\n        style_config = STYLE_CONFIGS[style_name]\n        \n        prompt = f\"\"\"Write a focused and thought-provoking essay about the topic: \"{topic}\".\n\nYour essay must be exactly 100 words long and should generate diverse opinions among readers.\n\nWriting Requirements:\n1. Use sophisticated vocabulary and complex sentence structures\n2. Present logical but potentially controversial arguments\n3. Maintain perfect grammar and coherence\n4. Create 4-5 well-constructed sentences\n5. Apply {style_config['tone']} tone throughout\n6. Follow {style_config['structure']} format\n7. {style_config['approach']}\n\nFocus on quality, clarity, and thought-provoking content.\n\nBegin your essay with \"Essay:\" followed by your 100-word response:\nEssay:\"\"\"\n        \n        return prompt.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:34.809642Z","iopub.execute_input":"2025-01-20T00:05:34.809948Z","iopub.status.idle":"2025-01-20T00:05:34.814738Z","shell.execute_reply.started":"2025-01-20T00:05:34.809924Z","shell.execute_reply":"2025-01-20T00:05:34.813738Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"---\n\n## **7. Essay Generator**\n\nThe `EssayGenerator` class handles essay generation and processing, including text cleaning and validation.","metadata":{}},{"cell_type":"code","source":"class EssayGenerator:\n    \"\"\"Handles essay generation with improved quality control\"\"\"\n    \n    def __init__(self):\n        self.model_pipe = ModelManager.initialize_model()\n        self.generation_count = 0\n    \n    def process_essay(self, generated_text: str) -> Optional[str]:\n        \"\"\"Process and clean generated essay text with detailed logging\"\"\"\n        try:\n            if not generated_text:\n                logger.warning(\"Empty generated text\")\n                return None\n\n            # Log original text length\n            logger.info(f\"Processing generated text ({len(generated_text)} chars)\")\n            \n            # Clean up the text\n            text = generated_text.strip()\n            \n            # Try to find essay start marker\n            essay_start = text.find('Essay:')\n            if essay_start != -1:\n                text = text[essay_start + 6:].strip()\n                logger.info(\"Found essay marker\")\n            else:\n                logger.warning(\"No essay marker found\")\n            \n            # Basic cleaning\n            text = re.sub(r'\\s+', ' ', text)\n            text = text.strip('\"\\'')\n            text = re.sub(r'\\n+', ' ', text)\n            \n            # Log cleaning results\n            logger.info(f\"After cleaning: {len(text.split())} words\")\n            \n            # Check minimum length\n            if len(text.split()) < 50:\n                logger.warning(f\"Essay too short: {len(text.split())} words\")\n                return None\n                \n            # Process sentences\n            sentences = [s.strip() for s in re.split('[.!?]', text) if s.strip()]\n            logger.info(f\"Found {len(sentences)} sentences\")\n            \n            if len(sentences) < 3:\n                logger.warning(\"Too few sentences\")\n                return None\n            \n            # Reconstruct with proper spacing\n            text = '. '.join(sentences) + '.'\n            \n            # Validate essay\n            metrics = QualityControl.calculate_metrics(text)\n            logger.info(\"Quality metrics calculated:\")\n            logger.info(f\"- Word count: {metrics['word_count']}\")\n            logger.info(f\"- Unique ratio: {metrics['unique_ratio']:.2f}\")\n            logger.info(f\"- Complexity score: {metrics['complexity_score']:.2f}\")\n            \n            is_valid, feedback = QualityControl.validate_essay(text, metrics)\n            \n            if is_valid:\n                logger.info(\"Essay passed validation\")\n                return text\n            else:\n                logger.warning(f\"Validation failed: {feedback}\")\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Error processing essay: {str(e)}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:40.018339Z","iopub.execute_input":"2025-01-20T00:05:40.018661Z","iopub.status.idle":"2025-01-20T00:05:40.027068Z","shell.execute_reply.started":"2025-01-20T00:05:40.018632Z","shell.execute_reply":"2025-01-20T00:05:40.025930Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"---\n\n## **8. Batch Processor**\n\nThe `BatchProcessor` class handles batch processing of topics, ensuring efficient use of GPU resources.","metadata":{}},{"cell_type":"code","source":"class BatchProcessor:\n    \"\"\"Handles batch processing with improved error handling\"\"\"\n    \n    def __init__(self, batch_size: int = 1):\n        self.batch_size = batch_size\n        self.generator = EssayGenerator()\n        self.results = []\n        self.stats = {\n            'total_processed': 0,\n            'successful_generations': 0,\n            'failed_generations': 0,\n            'style_usage': {},\n            'generated_essays': set()\n        }\n\n    def is_duplicate(self, essay: str) -> bool:\n        \"\"\"Check if essay is too similar to existing ones\"\"\"\n        if not essay:\n            return True\n            \n        # Check exact duplicates\n        if essay in self.stats['generated_essays']:\n            return True\n            \n        # Check similarity with existing essays\n        words = set(essay.lower().split())\n        for existing_essay in self.stats['generated_essays']:\n            existing_words = set(existing_essay.lower().split())\n            similarity = len(words & existing_words) / len(words | existing_words)\n            if similarity > COMPETITION_CONFIG['max_similarity']:\n                return True\n                \n        return False\n\n    def process_batch(self, topics: List[Tuple[int, str]]) -> None:\n        \"\"\"Process a batch of topics efficiently\"\"\"\n        try:\n            if self.stats['total_processed'] % 5 == 0:\n                torch.cuda.empty_cache()\n            \n            for topic_id, topic in topics:\n                logger.info(f\"\\nProcessing topic {topic_id}: {topic}\")\n                essay_generated = False\n                max_attempts = 3\n                \n                for attempt in range(max_attempts):\n                    if essay_generated:\n                        break\n                        \n                    logger.info(f\"Attempt {attempt + 1}/{max_attempts}\")\n                    \n                    for style in STYLE_CONFIGS.keys():\n                        if essay_generated:\n                            break\n                            \n                        logger.info(f\"Trying style: {style}\")\n                        try:\n                            prompt = PromptEngineering.get_prompt(topic, style)\n                            logger.info(\"Generating essay...\")\n                            \n                            outputs = self.generator.model_pipe(\n                                prompt,\n                                max_new_tokens=200,\n                                temperature=1.1,\n                                top_p=0.95,\n                                do_sample=True,\n                                num_return_sequences=1\n                            )\n                            \n                            if outputs and len(outputs) > 0:\n                                essay = self.generator.process_essay(outputs[0]['generated_text'])\n                                \n                                if essay and not self.is_duplicate(essay):\n                                    self.stats['generated_essays'].add(essay)\n                                    self.results.append({\n                                        'id': topic_id,\n                                        'essay': essay\n                                    })\n                                    self.stats['style_usage'][style] = self.stats['style_usage'].get(style, 0) + 1\n                                    self.stats['successful_generations'] += 1\n                                    essay_generated = True\n                                    logger.info(f\"Successfully generated essay for topic {topic_id}\")\n                                    break\n                                else:\n                                    logger.warning(f\"Invalid or duplicate essay generated for topic {topic_id}\")\n                            \n                        except Exception as e:\n                            logger.error(f\"Error with style {style} for topic {topic_id}: {str(e)}\")\n                            continue\n                    \n                if not essay_generated:\n                    logger.error(f\"Failed to generate valid essay for topic {topic_id} after {max_attempts} attempts\")\n                    self.stats['failed_generations'] += 1\n                    self.results.append({\n                        'id': topic_id,\n                        'essay': f\"Error generating essay for topic {topic_id}\"\n                    })\n            \n            self.stats['total_processed'] += len(topics)\n            \n        except Exception as e:\n            logger.error(f\"Batch processing error: {str(e)}\")\n            for topic_id, _ in topics:\n                if not any(r['id'] == topic_id for r in self.results):\n                    self.results.append({\n                        'id': topic_id,\n                        'essay': f\"Error generating essay for topic {topic_id}\"\n                    })\n                    self.stats['failed_generations'] += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:45.055447Z","iopub.execute_input":"2025-01-20T00:05:45.055733Z","iopub.status.idle":"2025-01-20T00:05:45.067251Z","shell.execute_reply.started":"2025-01-20T00:05:45.055710Z","shell.execute_reply":"2025-01-20T00:05:45.066131Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"---\n\n## **9. Quality Control**\n\nThe `QualityControl` class calculates and validates essay metrics to ensure they meet competition requirements.","metadata":{}},{"cell_type":"code","source":"class QualityControl:\n    \"\"\"Enhanced quality control with sophisticated metrics\"\"\"\n    \n    @staticmethod\n    def calculate_metrics(essay: str) -> Dict:\n        if not essay:\n            return None\n        \n        words = essay.split()\n        word_count = len(words)\n        unique_words = len(set(words))\n        sentences = [s.strip() for s in re.split('[.!?]', essay) if s.strip()]\n        \n        return {\n            'word_count': word_count,\n            'unique_ratio': unique_words / word_count if word_count > 0 else 0,\n            'avg_word_length': sum(len(w) for w in words) / word_count if word_count > 0 else 0,\n            'sentence_count': len(sentences),\n            'avg_sentence_length': word_count / len(sentences) if sentences else 0,\n            'complexity_score': (sum(len(w) for w in words) / word_count) * (unique_words / word_count) if word_count > 0 else 0\n        }\n    \n    @staticmethod\n    def validate_essay(essay: str, metrics: Optional[Dict] = None) -> Tuple[bool, str]:\n        if not metrics:\n            metrics = QualityControl.calculate_metrics(essay)\n        \n        target = COMPETITION_CONFIG['target_length']\n        tolerance = COMPETITION_CONFIG['length_tolerance']\n        \n        checks = [\n            (target - tolerance <= metrics['word_count'] <= target + tolerance,\n             f\"Word count {metrics['word_count']} outside target range {target}±{tolerance}\"),\n            (metrics['unique_ratio'] >= 0.65,\n             f\"Low word variety: {metrics['unique_ratio']:.2f}\"),\n            (3 <= metrics['sentence_count'] <= 5,\n             f\"Invalid sentence count: {metrics['sentence_count']}\"),\n            (20 <= metrics['avg_sentence_length'] <= 35,\n             f\"Invalid average sentence length: {metrics['avg_sentence_length']:.1f}\"),\n            (metrics['complexity_score'] >= 3.8,\n             f\"Low complexity score: {metrics['complexity_score']:.2f}\")\n        ]\n        \n        failed = [msg for condition, msg in checks if not condition]\n        return not bool(failed), '; '.join(failed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:49.386865Z","iopub.execute_input":"2025-01-20T00:05:49.387193Z","iopub.status.idle":"2025-01-20T00:05:49.394335Z","shell.execute_reply.started":"2025-01-20T00:05:49.387172Z","shell.execute_reply":"2025-01-20T00:05:49.393592Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"---\n\n## **10. Submission Creation**\n\nThe `create_submission` function generates and saves the final submission file.","metadata":{}},{"cell_type":"code","source":"def create_submission(processor: BatchProcessor):\n    \"\"\"Create and validate submission file\"\"\"\n    try:\n        submission_df = pd.DataFrame(processor.results)\n        submission_df = submission_df.sort_values('id')\n        \n        # Save locally\n        local_path = 'submission.csv'\n        submission_df.to_csv(local_path, index=False)\n        logger.info(f\"Submission saved to {local_path}\")\n        \n        # Try to save to Drive\n        try:\n            drive_path = '/content/drive/MyDrive/GenerativeAI/submission.csv'\n            submission_df.to_csv(drive_path, index=False)\n            logger.info(f\"Submission also saved to Drive at {drive_path}\")\n        except Exception as e:\n            logger.warning(f\"Could not save to Drive: {str(e)}\")\n        \n        # Analyze submission\n        analyze_submission(submission_df)\n        \n        return submission_df\n        \n    except Exception as e:\n        logger.error(f\"Error creating submission: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:54.397511Z","iopub.execute_input":"2025-01-20T00:05:54.397807Z","iopub.status.idle":"2025-01-20T00:05:54.402889Z","shell.execute_reply.started":"2025-01-20T00:05:54.397784Z","shell.execute_reply":"2025-01-20T00:05:54.402032Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"---\n\n## **11. Submission Analysis**\n\nThe `analyze_submission` function provides detailed statistics and sample essays from the submission.","metadata":{}},{"cell_type":"code","source":"def analyze_submission(df: pd.DataFrame):\n    \"\"\"Analyze submission quality\"\"\"\n    try:\n        total_essays = len(df)\n        valid_essays = df[~df['essay'].str.contains('Error', na=True)].shape[0]\n        \n        logger.info(\"\\nSubmission Analysis:\")\n        logger.info(f\"Total Essays: {total_essays}\")\n        logger.info(f\"Valid Essays: {valid_essays}\")\n        logger.info(f\"Success Rate: {(valid_essays/total_essays)*100:.1f}%\")\n        \n        if valid_essays > 0:\n            # Sample essays\n            logger.info(\"\\nSample Essays:\")\n            sample_df = df[~df['essay'].str.contains('Error', na=True)].sample(min(3, valid_essays))\n            for _, row in sample_df.iterrows():\n                logger.info(f\"\\nID: {row['id']}\")\n                logger.info(f\"Essay:\\n{row['essay']}\\n\")\n                logger.info(\"-\" * 80)\n    \n    except Exception as e:\n        logger.error(f\"Error analyzing submission: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:05:59.863227Z","iopub.execute_input":"2025-01-20T00:05:59.863497Z","iopub.status.idle":"2025-01-20T00:05:59.869566Z","shell.execute_reply.started":"2025-01-20T00:05:59.863478Z","shell.execute_reply":"2025-01-20T00:05:59.868651Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"---\n\n## **12. Main Execution**\n\nThe `main` function orchestrates the entire process, from loading data to generating essays and creating the submission.","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Main execution flow\"\"\"\n    try:\n        start_time = time.time()\n        \n        # Initial setup logging\n        logger.info(\"=\" * 80)\n        logger.info(\"Starting essay generation\")\n        logger.info(f\"GPU available: {torch.cuda.is_available()}\")\n        if torch.cuda.is_available():\n            logger.info(f\"Using device: {torch.cuda.get_device_name(0)}\")\n        logger.info(\"=\" * 80)\n        \n        # Load competition data first\n        try:\n            file_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'  # Adjust this path as needed\n            test_data = pd.read_csv(file_path)\n            logger.info(f\"Loaded {len(test_data)} topics from {file_path}\")\n        except Exception as e:\n            logger.error(f\"Error loading test data from {file_path}: {str(e)}\")\n            raise\n        \n        # Initialize processor\n        batch_size = 1  # Process one topic at a time for stability\n        processor = BatchProcessor(batch_size)\n        \n        # Process topics in batches\n        topic_batches = [\n            test_data.iloc[i:i+batch_size][['id', 'topic']].values.tolist()\n            for i in range(0, len(test_data), batch_size)\n        ]\n        \n        # Process with progress bar\n        with tqdm(total=len(topic_batches), desc=\"Processing topics\") as pbar:\n            for batch_idx, batch in enumerate(topic_batches):\n                try:\n                    processor.process_batch(batch)\n                    pbar.update(1)\n                    \n                    # Log progress every few topics\n                    if (batch_idx + 1) % 2 == 0:  # Increased frequency\n                        elapsed = time.time() - start_time\n                        rate = (batch_idx + 1) / elapsed if elapsed > 0 else 0\n                        logger.info(f\"\"\"\n                        Progress Update:\n                        - Completed: {batch_idx + 1}/{len(topic_batches)} topics\n                        - Success Rate: {(processor.stats['successful_generations']/(batch_idx+1))*100:.1f}%\n                        - Processing Rate: {rate:.2f} topics/second\n                        - Time Elapsed: {elapsed:.1f} seconds\n                        \"\"\")\n                        \n                except Exception as e:\n                    logger.error(f\"Error processing batch {batch_idx}: {str(e)}\")\n                    continue\n        \n        # Create submission if we have results\n        if processor.results:\n            submission_df = create_submission(processor)\n            logger.info(f\"\"\"\n            Generation Complete:\n            - Total Topics: {len(test_data)}\n            - Successful Generations: {processor.stats['successful_generations']}\n            - Failed Generations: {processor.stats['failed_generations']}\n            - Overall Success Rate: {(processor.stats['successful_generations']/len(test_data))*100:.1f}%\n            \"\"\")\n        else:\n            raise ValueError(\"No essays were generated successfully\")\n        \n    except Exception as e:\n        logger.error(f\"Execution failed: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:06:04.105777Z","iopub.execute_input":"2025-01-20T00:06:04.106146Z","iopub.status.idle":"2025-01-20T00:06:04.115164Z","shell.execute_reply.started":"2025-01-20T00:06:04.106119Z","shell.execute_reply":"2025-01-20T00:06:04.114055Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"---\n\n## **13. Run the Script**\n\nFinally, execute the script by calling the `main` function.","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    try:\n        # Configure logging first\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.StreamHandler(),\n                logging.FileHandler('essay_generation.log')\n            ]\n        )\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.INFO)\n        \n        # Suppress warnings\n        import warnings\n        warnings.filterwarnings('ignore')\n        \n        # GPU setup\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            device = torch.cuda.get_device_name(0)\n            memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n            logger.info(f\"Using GPU: {device} ({memory:.1f}GB)\")\n        else:\n            logger.warning(\"No GPU available, using CPU\")\n        \n        # Set random seeds\n        random.seed(42)\n        np.random.seed(42)\n        torch.manual_seed(42)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(42)\n            torch.backends.cudnn.deterministic = True\n        \n        # Run main process\n        main()\n        \n    except Exception as e:\n        logger.error(f\"Script execution failed: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T00:06:09.240113Z","iopub.execute_input":"2025-01-20T00:06:09.240418Z","iopub.status.idle":"2025-01-20T00:06:09.607576Z","shell.execute_reply.started":"2025-01-20T00:06:09.240396Z","shell.execute_reply":"2025-01-20T00:06:09.606424Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-25c947aba2c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Run main process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-48182dd907b6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/llms-you-cant-please-them-all/test.csv'\u001b[0m  \u001b[0;31m# Adjust this path as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(test_data)} topics from {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/llms-you-cant-please-them-all/test.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/llms-you-cant-please-them-all/test.csv'","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}