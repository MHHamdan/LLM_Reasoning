{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM Judge Disagreement Maximizer\n## This notebook aims to generate essays that maximize disagreement between LLM judges while maintaining quality and coherence. Using GPU acceleration for faster processing.\n\n## The goal is to:\n- Maximize disagreement between 3 LLM judges (horizontal variance)\n- Maintain high vertical variance across essays\n- Keep high English language quality (avg_e)\n- Avoid repetition (avg_s)\n- Manage average quality scores (avg_q)\n\nFinal score = (avg_h × min_v × avg_e) / (avg_s × (9 - avg_q))\n\n### We select \"GPU T4 x2\" as it offers good performance for transformer models and is well-suited for text generation tasks.","metadata":{}},{"cell_type":"markdown","source":"## Library Imports & GPU Setup\n- Imports essential libraries for ML/DL tasks\n- Sets up device detection for GPU/CPU processing","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nimport pandas as pd\nimport numpy as np\nimport re\nimport random\nimport time\nfrom tqdm import tqdm\nimport logging\nimport json\nfrom typing import Dict, List, Optional, Tuple\n\n# Setup GPU device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:57:39.886455Z","iopub.execute_input":"2025-01-19T13:57:39.886827Z","iopub.status.idle":"2025-01-19T13:57:59.854967Z","shell.execute_reply.started":"2025-01-19T13:57:39.886801Z","shell.execute_reply":"2025-01-19T13:57:59.854220Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Logging Configuration\n- Sets up structured logging for tracking the generation process\n- Helps in debugging and monitoring model performance","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:58:30.024542Z","iopub.execute_input":"2025-01-19T13:58:30.025268Z","iopub.status.idle":"2025-01-19T13:58:30.029431Z","shell.execute_reply.started":"2025-01-19T13:58:30.025233Z","shell.execute_reply":"2025-01-19T13:58:30.028501Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Model Configuration\n- Defines model parameters and generation settings\n- Optimized for generating diverse, high-quality essays\n- Includes settings to handle tokenization and text generation","metadata":{}},{"cell_type":"code","source":"# Initial Model Configuration\nMODEL_CONFIG = {\n    'name': 'phi-2',\n    'path': \"microsoft/phi-2\",\n    'params': {\n        'max_new_tokens': 150,     # Control essay length\n        'temperature': 1.2,        # High for diversity\n        'top_p': 0.98,            # High for varied sampling\n        'top_k': 100,             # More token options\n        'repetition_penalty': 1.5, # Avoid repetition\n        'do_sample': True,        # Enable sampling\n        'no_repeat_ngram_size': 3  # Additional repetition avoidance\n    },\n    'tokenizer_config': {\n        'padding': True,\n        'truncation': True,\n        'max_length': 512\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:58:39.502633Z","iopub.execute_input":"2025-01-19T13:58:39.502916Z","iopub.status.idle":"2025-01-19T13:58:39.507380Z","shell.execute_reply.started":"2025-01-19T13:58:39.502895Z","shell.execute_reply":"2025-01-19T13:58:39.506613Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Style Configurations\n- Defines different writing styles to create diverse essays that may trigger varying judgments from LLM evaluators","metadata":{}},{"cell_type":"code","source":"# Style Configurations\n\nSTYLE_CONFIGS = {\n    'contrarian': {\n        'tone': 'deliberately challenging',\n        'structure': 'unconventional logic',\n        'approach': 'subvert expectations'\n    },\n    'academic': {\n        'tone': 'heavily theoretical',\n        'structure': 'complex analysis',\n        'approach': 'scholarly abstraction'\n    },\n    'provocative': {\n        'tone': 'emotionally charged',\n        'structure': 'rhetorical challenge',\n        'approach': 'perspective shifting'\n    },\n    'analytical': {\n        'tone': 'objective and methodical',\n        'structure': 'systematic analysis',\n        'approach': 'data-driven examination'\n    },\n    'critical': {\n        'tone': 'evaluative and challenging',\n        'structure': 'point-counterpoint',\n        'approach': 'deep examination of assumptions'\n    },\n    'exploratory': {\n        'tone': 'inquiring and open-ended',\n        'structure': 'progressive discovery',\n        'approach': 'multiple perspective analysis'\n    },\n    'controversial': {\n        'tone': 'provocative yet logical',\n        'structure': 'thesis-antithesis',\n        'approach': 'challenging established views'\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:58:45.807497Z","iopub.execute_input":"2025-01-19T13:58:45.807802Z","iopub.status.idle":"2025-01-19T13:58:45.812525Z","shell.execute_reply.started":"2025-01-19T13:58:45.807780Z","shell.execute_reply":"2025-01-19T13:58:45.811592Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Model Manager Class\n- Handles model initialization and GPU memory management for efficient processing","metadata":{}},{"cell_type":"code","source":"class ModelManager:\n    \"\"\"Manages model initialization and GPU resources\"\"\"\n    \n    @staticmethod\n    def cleanup_models():\n        \"\"\"Clean GPU memory\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            logger.info(\"GPU memory cleared\")\n        for obj in ['model', 'pipe', 'tokenizer']:\n            if obj in globals():\n                del globals()[obj]\n    \n    @staticmethod\n    def verify_model(pipe) -> bool:\n        try:\n            # Test generation\n            test_output = pipe(\"Test prompt\", max_new_tokens=10)\n            return bool(test_output)\n        except Exception as e:\n            logger.error(f\"Model verification failed: {str(e)}\")\n            return False\n            \n    @staticmethod\n    def initialize_model() -> pipeline:\n        \"\"\"Initialize model with GPU optimization\"\"\"\n        ModelManager.cleanup_models()\n        \n        try:\n            logger.info(f\"Loading {MODEL_CONFIG['name']}\")\n            \n            # Initialize tokenizer\n            tokenizer = AutoTokenizer.from_pretrained(\n            MODEL_CONFIG['path'],\n            trust_remote_code=True,\n            padding_side='left', \n            **MODEL_CONFIG['tokenizer_config']\n            )\n                    \n           \n            \n            # Handle padding token\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            \n            # Load model\n            model = AutoModelForCausalLM.from_pretrained(\n                MODEL_CONFIG['path'],\n                device_map=\"auto\",\n                torch_dtype=torch.bfloat16,\n                trust_remote_code=True,\n                low_cpu_mem_usage=True\n            )\n            \n            # Create pipeline\n            pipe = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                **MODEL_CONFIG['params']  # Use generation params\n            )\n            \n            # Verify model initialization\n            if ModelManager.verify_model(pipe):\n                logger.info(\"Model initialized and verified successfully\")\n                return pipe\n            else:\n                raise RuntimeError(\"Model verification failed\")\n                \n        except Exception as e:\n            logger.error(f\"Error initializing model: {str(e)}\")\n            raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:59:15.489330Z","iopub.execute_input":"2025-01-19T13:59:15.489659Z","iopub.status.idle":"2025-01-19T13:59:15.497528Z","shell.execute_reply.started":"2025-01-19T13:59:15.489619Z","shell.execute_reply":"2025-01-19T13:59:15.496621Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Prompt Engineering Class\n- Specialized class for generating prompts that encourage diverse essay generation","metadata":{}},{"cell_type":"code","source":"# Prompt Engineering\nclass PromptEngineering:\n    @staticmethod\n    def get_prompt(topic: str, style_name: str) -> str:\n        style_config = STYLE_CONFIGS[style_name]\n        perspectives = [\n            \"Consider an unconventional viewpoint where\",\n            \"Challenge the assumption that\",\n            \"Propose a radical reframing where\",\n            \"Analyze through the lens of an extreme position where\"\n        ]\n        \n        base_prompt = f\"\"\"Write a focused 100-word essay that will generate diverse opinions.\n        Topic: \"{topic}\"\n        {random.choice(perspectives)}\n        \n        Requirements:\n        - Use {style_config['tone']} tone\n        - Follow {style_config['structure']} format\n        - Apply {style_config['approach']}\n        - Present controversial yet logical arguments\n        - Maintain grammatical correctness\n        \n        Essay:\"\"\"\n        return base_prompt.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:59:23.645365Z","iopub.execute_input":"2025-01-19T13:59:23.645700Z","iopub.status.idle":"2025-01-19T13:59:23.650185Z","shell.execute_reply.started":"2025-01-19T13:59:23.645673Z","shell.execute_reply":"2025-01-19T13:59:23.649277Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Quality Control Class\n- Advanced quality metrics and validation to ensure generated essays meet requirements while maximizing potential judge disagreement","metadata":{}},{"cell_type":"code","source":"# Quality Control\nclass QualityControl:\n    \"\"\"Comprehensive quality control for generated essays\"\"\"\n\n    THRESHOLDS = {\n        'min_words': 90,\n        'max_words': 110,\n        'min_unique_ratio': 0.7,\n        'min_sentences': 4,\n        'max_avg_sentence_length': 30\n    }\n    \n    @staticmethod\n    def calculate_metrics(essay: str) -> Dict:\n        \"\"\"Calculate detailed quality metrics\"\"\"\n        if not essay:\n            return None\n            \n        # Basic metrics\n        words = essay.split()\n        unique_words = len(set(words))\n        word_count = len(words)\n        sentences = re.split('[.!?]', essay.strip())\n        sentences = [s.strip() for s in sentences if s.strip()]\n        \n        # Advanced metrics\n        avg_word_length = sum(len(w) for w in words) / word_count if word_count > 0 else 0\n        sentence_lengths = [len(s.split()) for s in sentences]\n        sentence_length_variance = np.var(sentence_lengths) if sentence_lengths else 0\n        \n        return {\n            'word_count': word_count,\n            'unique_word_ratio': unique_words / word_count if word_count > 0 else 0,\n            'avg_word_length': avg_word_length,\n            'sentence_count': len(sentences),\n            'avg_sentence_length': word_count / len(sentences) if sentences else 0,\n            'sentence_length_variance': sentence_length_variance,\n            'complexity_score': avg_word_length * (unique_words / word_count if word_count > 0 else 0)\n        }\n    \n    @staticmethod\n    def validate_essay(essay: str, metrics: Optional[Dict] = None) -> Tuple[bool, str]:\n        \"\"\"Validate essay with detailed feedback\"\"\"\n        if not essay:\n            return False, \"Empty essay\"\n            \n        if not metrics:\n            metrics = QualityControl.calculate_metrics(essay)\n        \n        # Comprehensive quality thresholds\n        thresholds = {\n            'min_words': 80,\n            'max_words': 120,\n            'min_unique_ratio': 0.6,\n            'min_sentences': 3,\n            'max_avg_sentence_length': 25,\n            'min_complexity_score': 3.0\n        }\n        \n        # Validation checks\n        checks = [\n            (thresholds['min_words'] <= metrics['word_count'] <= thresholds['max_words'],\n             f\"Word count {metrics['word_count']} outside range {thresholds['min_words']}-{thresholds['max_words']}\"),\n            (metrics['unique_word_ratio'] >= thresholds['min_unique_ratio'],\n             f\"Insufficient word variety: {metrics['unique_word_ratio']:.2f}\"),\n            (metrics['sentence_count'] >= thresholds['min_sentences'],\n             f\"Too few sentences: {metrics['sentence_count']}\"),\n            (metrics['avg_sentence_length'] <= thresholds['max_avg_sentence_length'],\n             f\"Sentences too long: {metrics['avg_sentence_length']:.1f}\"),\n            (metrics['complexity_score'] >= thresholds['min_complexity_score'],\n             f\"Complexity score too low: {metrics['complexity_score']:.1f}\")\n        ]\n        \n        # Check all conditions\n        failed_checks = [msg for passed, msg in checks if not passed]\n        return not bool(failed_checks), '; '.join(failed_checks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:59:30.332754Z","iopub.execute_input":"2025-01-19T13:59:30.333041Z","iopub.status.idle":"2025-01-19T13:59:30.341867Z","shell.execute_reply.started":"2025-01-19T13:59:30.333017Z","shell.execute_reply":"2025-01-19T13:59:30.341067Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Essay Generator Class\n- GPU-optimized essay generation with batch processing capabilities and error handling","metadata":{}},{"cell_type":"code","source":"class EssayGenerator:\n    def __init__(self):\n        self.model_pipe = ModelManager.initialize_model()\n        self.generation_count = 0\n    \n    \n    def process_essay(self, generated_text: str) -> str:\n        \"\"\"Process and clean generated essay text\"\"\"\n        try:\n            # Find where the essay starts (after 'Essay:')\n            essay_start = generated_text.find('Essay:')\n            if essay_start == -1:\n                return None\n            \n            essay = generated_text[essay_start + 6:].strip()\n            \n            # Basic cleaning\n            essay = re.sub(r'\\s+', ' ', essay)  # Remove extra whitespace\n            essay = essay.replace('\"', '')      # Remove quotes\n            \n            # Validate essay\n            metrics = QualityControl.calculate_metrics(essay)\n            is_valid, feedback = QualityControl.validate_essay(essay, metrics)\n            \n            if is_valid:\n                return essay\n            else:\n                logger.warning(f\"Essay validation failed: {feedback}\")\n                return None\n                \n        except Exception as e:\n            logger.error(f\"Error processing essay: {str(e)}\")\n            return None\n    \n        \n    @torch.no_grad()\n    def generate_essays_batch(self, prompts: List[str], batch_size: int) -> List[str]:\n        \"\"\"Generate multiple essays efficiently\"\"\"\n        try:\n            outputs = self.model_pipe(\n                prompts,\n                batch_size=batch_size,\n                **MODEL_CONFIG['params']\n            )\n            return outputs\n        except Exception as e:\n            logger.error(f\"Batch generation error: {str(e)}\")\n            return [None] * len(prompts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:59:39.547797Z","iopub.execute_input":"2025-01-19T13:59:39.548126Z","iopub.status.idle":"2025-01-19T13:59:39.554946Z","shell.execute_reply.started":"2025-01-19T13:59:39.548073Z","shell.execute_reply":"2025-01-19T13:59:39.554006Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def create_submission(processor: BatchProcessor):\n    \"\"\"Create and save the submission CSV file\"\"\"\n    try:\n        # Convert results to DataFrame\n        submission_df = pd.DataFrame(processor.results)\n        \n        # Sort by ID to ensure correct order\n        submission_df = submission_df.sort_values('id')\n        \n        # Save to CSV\n        submission_path = 'submission.csv'\n        submission_df.to_csv(submission_path, index=False)\n        \n        logger.info(f\"Submission saved to {submission_path}\")\n        logger.info(f\"Total essays: {len(submission_df)}\")\n        \n        # Basic validation\n        if len(submission_df) == 0:\n            raise ValueError(\"No essays generated!\")\n            \n        # Check for missing essays\n        missing_essays = submission_df['essay'].isna().sum()\n        if missing_essays > 0:\n            logger.warning(f\"Warning: {missing_essays} missing essays\")\n            \n    except Exception as e:\n        logger.error(f\"Error creating submission: {str(e)}\")\n        raise\n\ndef log_progress(processor: BatchProcessor, start_time: float):\n    \"\"\"Log generation progress and statistics\"\"\"\n    elapsed = time.time() - start_time\n    rate = processor.stats['total_processed'] / elapsed if elapsed > 0 else 0\n    \n    logger.info(f\"\"\"\n    Progress Update:\n    - Processed: {processor.stats['total_processed']}\n    - Successful: {processor.stats['successful_generations']}\n    - Failed: {processor.stats['failed_generations']}\n    - Rate: {rate:.2f} essays/second\n    - Time elapsed: {elapsed:.1f}s\n    \"\"\")\n    \n    # Log style usage\n    logger.info(\"Style usage:\")\n    for style, count in processor.stats['style_usage'].items():\n        logger.info(f\"- {style}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:00:49.574803Z","iopub.execute_input":"2025-01-19T14:00:49.575212Z","iopub.status.idle":"2025-01-19T14:00:49.584574Z","shell.execute_reply.started":"2025-01-19T14:00:49.575178Z","shell.execute_reply":"2025-01-19T14:00:49.583456Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Main Execution Logic\n- Implements batch processing, GPU optimization, and comprehensive monitoring of the essay generation process","metadata":{}},{"cell_type":"code","source":"class BatchProcessor:\n    \"\"\"Handles batch processing and GPU optimization\"\"\"\n    \n    def __init__(self, batch_size: int = 5):\n        self.batch_size = batch_size\n        self.generator = EssayGenerator()\n        self.results = []\n        self.stats = {\n            'total_processed': 0,\n            'successful_generations': 0,\n            'failed_generations': 0,\n            'style_usage': {}\n        }\n    \n    def prepare_dataset(self, topics: List[Tuple[int, str]]):\n        \"\"\"Prepare dataset for efficient GPU processing\"\"\"\n        prompts = []\n        ids = []\n        for topic_id, topic in topics:\n            # Try different styles\n            for style in STYLE_CONFIGS.keys():\n                prompt = PromptEngineering.get_prompt(topic, style)\n                prompts.append(prompt)\n                ids.append(topic_id)\n        return prompts, ids\n    \n    def process_batch(self, topics: List[Tuple[int, str]]) -> None:\n        \"\"\"Process a batch of topics efficiently\"\"\"\n        if self.stats['total_processed'] % 20 == 0:\n            torch.cuda.empty_cache()\n        \n        # Prepare batch dataset\n        prompts, ids = self.prepare_dataset(topics)\n        \n        try:\n            # Generate all prompts at once\n            outputs = self.generator.model_pipe(\n                prompts,\n                batch_size=self.batch_size,\n                **MODEL_CONFIG['params']\n            )\n            \n            # Process outputs\n            for idx, output in enumerate(outputs):\n                essay = self.generator.process_essay(output[0]['generated_text'])\n                if essay:\n                    style = list(STYLE_CONFIGS.keys())[idx % len(STYLE_CONFIGS)]\n                    self.stats['style_usage'][style] = self.stats['style_usage'].get(style, 0) + 1\n                    self.stats['successful_generations'] += 1\n                    \n                    self.results.append({\n                        'id': ids[idx // len(STYLE_CONFIGS)],\n                        'essay': essay\n                    })\n                    break\n                \n            if not essay:\n                self.stats['failed_generations'] += 1\n                self.results.append({\n                    'id': ids[idx // len(STYLE_CONFIGS)],\n                    'essay': \"Error generating essay\"\n                })\n            \n            self.stats['total_processed'] += 1\n            \n        except Exception as e:\n            logger.error(f\"Batch processing error: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:00:53.272559Z","iopub.execute_input":"2025-01-19T14:00:53.272847Z","iopub.status.idle":"2025-01-19T14:00:53.280789Z","shell.execute_reply.started":"2025-01-19T14:00:53.272826Z","shell.execute_reply":"2025-01-19T14:00:53.279932Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def validate_config():\n    \"\"\"Validate MODEL_CONFIG settings\"\"\"\n    required_keys = ['name', 'path', 'params', 'tokenizer_config']\n    required_params = ['max_new_tokens', 'temperature', 'top_p', 'top_k', 'repetition_penalty']\n    \n    try:\n        # Check main keys\n        for key in required_keys:\n            if key not in MODEL_CONFIG:\n                raise ValueError(f\"Missing required key in MODEL_CONFIG: {key}\")\n        \n        # Check params\n        for param in required_params:\n            if param not in MODEL_CONFIG['params']:\n                raise ValueError(f\"Missing required parameter in MODEL_CONFIG['params']: {param}\")\n        \n        # Validate values\n        if MODEL_CONFIG['params']['max_new_tokens'] > 512:\n            logger.warning(\"max_new_tokens > 512 might cause issues with context length\")\n        \n        if MODEL_CONFIG['params']['temperature'] > 1.5:\n            logger.warning(\"Very high temperature might lead to incoherent outputs\")\n            \n        logger.info(\"MODEL_CONFIG validation passed\")\n        return True\n        \n    except Exception as e:\n        logger.error(f\"MODEL_CONFIG validation failed: {str(e)}\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:01:02.221056Z","iopub.execute_input":"2025-01-19T14:01:02.221399Z","iopub.status.idle":"2025-01-19T14:01:02.226995Z","shell.execute_reply.started":"2025-01-19T14:01:02.221374Z","shell.execute_reply":"2025-01-19T14:01:02.226074Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def main():\n    try:\n        start_time = time.time()\n        logger.info(\"Starting essay generation process...\")\n        \n        # Load data\n        test_data = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')\n        total_topics = len(test_data)\n        logger.info(f\"Loaded {total_topics} topics\")\n        \n        # Initialize processor with optimal batch size\n        batch_size = 5  # Optimized for T4 GPU\n        processor = BatchProcessor(batch_size)\n        \n        # Create batches\n        topic_batches = [\n            test_data.iloc[i:i+batch_size][['id', 'topic']].values.tolist()\n            for i in range(0, len(test_data), batch_size)\n        ]\n        \n        # Process batches\n        with tqdm(total=len(topic_batches), desc=\"Processing batches\") as pbar:\n            for batch in topic_batches:\n                processor.process_batch(batch)\n                pbar.update(1)\n                \n                # Log periodically\n                if processor.stats['total_processed'] % 20 == 0:\n                    log_progress(processor, start_time)\n        \n        # Create submission\n        create_submission(processor)\n        \n    except Exception as e:\n        logger.error(f\"Error in main execution: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:01:10.489218Z","iopub.execute_input":"2025-01-19T14:01:10.489528Z","iopub.status.idle":"2025-01-19T14:01:10.495441Z","shell.execute_reply.started":"2025-01-19T14:01:10.489504Z","shell.execute_reply":"2025-01-19T14:01:10.494512Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#  analysis of results\ndef analyze_submission():\n    \"\"\"Analyze the generated submission file\"\"\"\n    try:\n        # Read the generated submission file\n        submission_df = pd.read_csv('submission.csv')\n        logger.info(f\"Total essays generated: {len(submission_df)}\")\n        \n        # Sample a few essays\n        print(\"\\nSample Essays:\")\n        for i in range(min(3, len(submission_df))):\n            print(f\"\\nEssay {i+1}:\")\n            print(f\"ID: {submission_df.iloc[i]['id']}\")\n            print(f\"Essay:\\n{submission_df.iloc[i]['essay']}\\n\")\n            print(\"-\"*80)\n        \n        # Basic statistics\n        error_count = submission_df['essay'].str.contains('Error').sum()\n        success_rate = (len(submission_df) - error_count) / len(submission_df) * 100\n        \n        print(f\"\\nSubmission Statistics:\")\n        print(f\"Total Essays: {len(submission_df)}\")\n        print(f\"Successful Generations: {len(submission_df) - error_count}\")\n        print(f\"Failed Generations: {error_count}\")\n        print(f\"Success Rate: {success_rate:.2f}%\")\n        \n        # Save some samples to inspect\n        sample_df = submission_df.sample(min(5, len(submission_df)))\n        sample_df.to_csv('submission_samples.csv', index=False)\n        print(\"\\nSaved sample essays to 'submission_samples.csv'\")\n        \n        return submission_df\n        \n    except FileNotFoundError:\n        logger.error(\"submission.csv file not found\")\n    except Exception as e:\n        logger.error(f\"Error analyzing submission: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:01:18.833456Z","iopub.execute_input":"2025-01-19T14:01:18.833762Z","iopub.status.idle":"2025-01-19T14:01:18.840004Z","shell.execute_reply.started":"2025-01-19T14:01:18.833737Z","shell.execute_reply":"2025-01-19T14:01:18.839051Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## GPU Optimization Tips\n- Batch size of 5 is optimized for T4 GPU memory\n- Using torch.no_grad() for inference\n- Regular GPU memory cleanup\n- Monitoring GPU memory usage","metadata":{}},{"cell_type":"code","source":"@staticmethod\ndef cleanup_gpu_memory():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        memory_allocated = torch.cuda.memory_allocated(0)/1e9\n        logger.info(f\"GPU memory after cleanup: {memory_allocated:.2f}GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:01:30.283459Z","iopub.execute_input":"2025-01-19T14:01:30.283784Z","iopub.status.idle":"2025-01-19T14:01:30.288072Z","shell.execute_reply.started":"2025-01-19T14:01:30.283758Z","shell.execute_reply":"2025-01-19T14:01:30.287173Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Validate configuration\n    if not validate_config():\n        raise ValueError(\"Invalid MODEL_CONFIG\")\n        \n    # Set GPU optimization flags\n    torch.backends.cudnn.benchmark = True\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        logger.info(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n        logger.info(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB\")\n    \n    main()\n    submission_df = analyze_submission()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:24:32.116260Z","iopub.execute_input":"2025-01-19T07:24:32.116554Z","iopub.status.idle":"2025-01-19T07:26:29.182140Z","shell.execute_reply.started":"2025-01-19T07:24:32.116527Z","shell.execute_reply":"2025-01-19T07:26:29.181417Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7a7914cc2914d11b3c2bb6dad13def8"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nProcessing batches: 100%|██████████| 1/1 [01:48<00:00, 108.50s/it]","output_type":"stream"},{"name":"stdout","text":"\nSample Essays:\n\nEssay 1:\nID: 1097671\nEssay:\n[Your focus on both positive... and negative side plan goes here....] **Solution 2 with comments - Topic, Writing Format & Organization Structure Part II/ III* 10 points each***(50 extra point)(20 additional bonus) 200 word minimum 1500 words = 1756point (for full sample see http://www.) If your assessment is approved to be an assignment or portfolio refer only this specific problem along within 700 maximum 5 000 character. Example Solution can look like : <code><section style=fontsize=150%;><b>Idea 1:{0}\\nI believe {1}</p></i>. Repeat it thrice.<br/> Idea number two was…………………………………….. Ideal writing formats are\n\n--------------------------------------------------------------------------------\n\nSubmission Statistics:\nTotal Essays: 1\nSuccessful Generations: 1\nFailed Generations: 0\nSuccess Rate: 100.00%\n\nSaved sample essays to 'submission_samples.csv'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30}]}