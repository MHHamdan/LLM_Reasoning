{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8393355c763444e2a4ad11a8820cdd33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_a221212890c14fa793058a128bd53d7e"
          }
        },
        "1d0b1aff34b74019b70f90b66c9588bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faaec22102604c1caee9ff31b6ff2387",
            "placeholder": "​",
            "style": "IPY_MODEL_978fc56b415744fea2e111b6211db7dd",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "455f299aaed74753b5c83505617e3609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_42bf7d1f74ea4227912d92bb25985278",
            "placeholder": "​",
            "style": "IPY_MODEL_4ac58c7807e94ac9ad799f288b553ff7",
            "value": ""
          }
        },
        "6f8e774158684727b57b9ae52f8bc484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7215aedf9b7e4329895819b015ba9b5b",
            "style": "IPY_MODEL_10c476023a3b470c9fbb9bd1945f79f8",
            "value": true
          }
        },
        "07b09fb116fd4b25b48388c546139f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3bcc0c8c9eae40aa87b61e342fa59fd7",
            "style": "IPY_MODEL_915ff8e296c747c7b42118a047585381",
            "tooltip": ""
          }
        },
        "d1da4b4bc0f042ab91e5fd31e389a684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0ab316adc645898ecb8c566d0fdbce",
            "placeholder": "​",
            "style": "IPY_MODEL_9d53afc496a048fdbb257776200d46a8",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "a221212890c14fa793058a128bd53d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "faaec22102604c1caee9ff31b6ff2387": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978fc56b415744fea2e111b6211db7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bf7d1f74ea4227912d92bb25985278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ac58c7807e94ac9ad799f288b553ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7215aedf9b7e4329895819b015ba9b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c476023a3b470c9fbb9bd1945f79f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bcc0c8c9eae40aa87b61e342fa59fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915ff8e296c747c7b42118a047585381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "6d0ab316adc645898ecb8c566d0fdbce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d53afc496a048fdbb257776200d46a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8818243bc84046e296f49ab7f67c099e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c10e5d789194617ade9616dea7d4eef",
            "placeholder": "​",
            "style": "IPY_MODEL_f9a70c6253fc490bbbde46995adafbca",
            "value": "Connecting..."
          }
        },
        "0c10e5d789194617ade9616dea7d4eef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a70c6253fc490bbbde46995adafbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# step 1: install required libraries\n",
        "# before we begin, install the necessary libraries if they are not already installed.\n"
      ],
      "metadata": {
        "id": "Z7NNlJoHFIa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q transformers torch\n",
        "#!pip install transformers torch textblob\n"
      ],
      "metadata": {
        "id": "gtHDtL6yFNyC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 2: import required modules"
      ],
      "metadata": {
        "id": "UVU6E5B4LD79"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "nypVSVRNEzGr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from textblob import TextBlob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 3: load pretrained model and tokenizer\n"
      ],
      "metadata": {
        "id": "EBQixQ8EK-gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we use a pretrained gpt-2 model for our rlhf experiment.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "xvh9242XFRjp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define pad token explicitly to avoid warnings\n",
        "\n",
        "def generate_completion(prompt, max_length=50):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = policy_model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50, top_p=0.95)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "sample_prompt = \"Once upon a time,\"\n",
        "sample_completion = generate_completion(sample_prompt)\n",
        "print(\"Generated Completion:\", sample_completion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3jhN3BZF05O",
        "outputId": "25132ec8-9e5d-4872-f3d2-9e347448e21b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Completion: Once upon a time, I've lost my faith in the power of God, and if I do not believe my faith and my work are well balanced, who will know what purpose this office is for me, or whether I will ever be able to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 4: generate sample completions\n"
      ],
      "metadata": {
        "id": "wYcjtMzJK3zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define pad token explicitly to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# let's define a function to generate text completions from our model.\n",
        "# def generate_completion(prompt, max_length=50):\n",
        "#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "#     attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "#     output = policy_model.generate(\n",
        "#         input_ids,\n",
        "#         attention_mask=attention_mask,\n",
        "#         max_length=max_length,\n",
        "#         do_sample=True,\n",
        "#         top_k=50,\n",
        "#         top_p=0.95\n",
        "#     )\n",
        "#     return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# # example usage\n",
        "# sample_prompt = \"once upon a time,\"\n",
        "# sample_completion = generate_completion(sample_prompt)\n",
        "# #print(\"generated completion:\", sample_completion)\n",
        "# import textwrap\n",
        "\n",
        "# print(\"Generated Completion:\")\n",
        "# print(textwrap.fill(sample_completion.strip(), width=60))  # Simulate whiteboard width\n",
        "\n",
        "def generate_multiple_completions(prompt, num_samples=3, max_length=50):\n",
        "    completions = []\n",
        "    for _ in range(num_samples):\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "        output = policy_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95\n",
        "        )\n",
        "        completions.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "    return completions\n"
      ],
      "metadata": {
        "id": "upHeT2xgGaTA"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 5: implementing a simple reward function\n"
      ],
      "metadata": {
        "id": "QfSDcNOrKneB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # now, let's define a simple reward function that scores generated text based on certain criteria.\n",
        "# def reward_function(text):\n",
        "#     \"\"\"\n",
        "#     a simple reward function that assigns a score based on politeness.\n",
        "#     returns +1 if the text contains polite phrases, -1 otherwise.\n",
        "#     \"\"\"\n",
        "#     polite_phrases = [\"thank you\", \"please\", \"kindly\", \"appreciate\"]\n",
        "#     if any(phrase in text.lower() for phrase in polite_phrases):\n",
        "#         return 1.0\n",
        "#     return -1.0\n",
        "\n",
        "# # example usage\n",
        "# reward_score = reward_function(sample_completion)\n",
        "# print(\"reward score:\", reward_score)\n",
        "\n",
        "\n",
        "def reward_function(text):\n",
        "    \"\"\"\n",
        "    an improved reward function that assigns a score based on politeness and sentiment.\n",
        "    \"\"\"\n",
        "    polite_phrases = [\"thank you\", \"please\", \"kindly\", \"appreciate\", \"grateful\", \"respect\"]\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "\n",
        "    if any(phrase in text.lower() for phrase in polite_phrases):\n",
        "        return 2.0  # increased reward for polite phrases\n",
        "    elif sentiment > 0.2:\n",
        "        return 1.0  # slightly positive sentiment gets a moderate reward\n",
        "    return -2.0  # increased penalty for non-positive responses"
      ],
      "metadata": {
        "id": "sw2nnmErGz7k"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 6: computing log probabilities for the generated text\n"
      ],
      "metadata": {
        "id": "gNHtWtrPKY-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's define a function to compute log probabilities of the generated tokens.\n",
        "def compute_log_probs(model, input_text):\n",
        "    \"\"\"\n",
        "    computes log probabilities of the generated text under the given model.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    log_probs = -outputs.loss.item()\n",
        "    return log_probs\n",
        "\n",
        "# example usage\n",
        "log_prob_score = compute_log_probs(policy_model, sample_completion)\n",
        "print(\"log probability score:\", log_prob_score)\n"
      ],
      "metadata": {
        "id": "ObU3IVFiIlOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd187edd-6766-43df-a66d-e5dddfcc8f3d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log probability score: -2.65922474861145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 7: computing the advantage estimate\n"
      ],
      "metadata": {
        "id": "mfeCot-4NKfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the advantage function helps determine how much better (or worse) an action was compared to the expected value.\n",
        "def compute_advantage(reward, baseline=0.0):\n",
        "    \"\"\"\n",
        "    computes the advantage estimate given the reward and an optional baseline.\n",
        "    \"\"\"\n",
        "    return reward - baseline\n",
        "\n",
        "# # example usage\n",
        "# advantage = compute_advantage(reward_score)\n",
        "# print(\"advantage estimate:\", advantage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgxcoOJfKPb9",
        "outputId": "b9af91cd-8073-486e-9523-7f0234a6f54c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "advantage estimate: -1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 8: computing the policy gradient loss\n"
      ],
      "metadata": {
        "id": "sKOhhadWNrpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's compute the policy gradient loss using the advantage estimate.\n",
        "# now, let's compute the policy gradient loss using the advantage estimate.\n",
        "def compute_policy_gradient_loss(log_prob, advantage):\n",
        "    \"\"\"\n",
        "    computes the policy gradient loss given log probabilities and advantage.\n",
        "    \"\"\"\n",
        "    return torch.tensor(-log_prob * advantage, requires_grad=True)\n",
        "\n",
        "\n",
        "# example usage\n",
        "policy_gradient_loss = compute_policy_gradient_loss(log_prob_score, advantage)\n",
        "print(\"policy gradient loss:\", policy_gradient_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy5RTzPeNLUt",
        "outputId": "79be0594-84de-4e6e-f3a9-2bdb473cbfdd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy gradient loss: tensor(-2.6592, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 9: updating the model using gradient descent\n"
      ],
      "metadata": {
        "id": "6KONO_7nS9T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's update the model parameters using the computed loss.\n",
        "def update_model(model, loss, learning_rate=1e-5):\n",
        "    \"\"\"\n",
        "    updates the model parameters using gradient descent.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# example usage\n",
        "update_model(policy_model, policy_gradient_loss)\n",
        "print(\"model updated successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KufRafINum2",
        "outputId": "50897b3a-385d-4c5e-9128-93fc2657d8d6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model updated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 10: evaluating the updated model\n"
      ],
      "metadata": {
        "id": "BcWhjW7jUb1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # now, let's generate a new sample completion after training to see if the model has improved.\n",
        "# def evaluate_model(prompt):\n",
        "#     \"\"\"\n",
        "#     generates a new completion after model updates to evaluate changes.\n",
        "#     \"\"\"\n",
        "#     new_completion = generate_completion(prompt)\n",
        "#     new_reward = reward_function(new_completion)\n",
        "#     print(\"new generated completion:\", new_completion)\n",
        "#     print(\"new reward score:\", new_reward)\n",
        "#     return new_completion, new_reward\n",
        "\n",
        "# # example usage\n",
        "# evaluate_model(\"once upon a time,\")\n",
        "\n",
        "# def train_model(model, prompt, iterations=30):\n",
        "#     for i in range(iterations):\n",
        "#         print(f\"iteration {i+1}:\")\n",
        "#         completion = generate_completion(prompt)\n",
        "#         reward = reward_function(completion)\n",
        "#         log_prob = compute_log_probs(model, completion)\n",
        "#         advantage = compute_advantage(reward)\n",
        "#         loss = compute_policy_gradient_loss(log_prob, advantage)\n",
        "#         update_model(model, loss)\n",
        "#         print(\"updated model.\")\n",
        "\n",
        "# # example usage\n",
        "# train_model(policy_model, \"once upon a time,\")\n",
        "\n",
        "\n",
        "def train_model(model, prompts, iterations=30, num_samples=3):\n",
        "    \"\"\"\n",
        "    trains the model on multiple prompts and multiple generated responses per prompt.\n",
        "    \"\"\"\n",
        "    for i in range(iterations):\n",
        "        print(f\"iteration {i+1}:\")\n",
        "        total_loss = 0\n",
        "        for prompt in prompts:\n",
        "            completions = generate_multiple_completions(prompt, num_samples)\n",
        "            rewards = [reward_function(c) for c in completions]\n",
        "            avg_reward = sum(rewards) / len(rewards)\n",
        "            log_probs = [compute_log_probs(model, c) for c in completions]\n",
        "            avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "            advantage = compute_advantage(avg_reward)\n",
        "            loss = compute_policy_gradient_loss(avg_log_prob, advantage)\n",
        "            update_model(model, loss)\n",
        "            total_loss += loss.item()\n",
        "        print(f\"updated model. average loss: {total_loss / len(prompts)}\")\n",
        "\n",
        "# example usage\n",
        "training_prompts = [\n",
        "    \"once upon a time,\",\n",
        "    \"hello, how are you doing today?\",\n",
        "    \"thank you for your help!\"\n",
        "]\n",
        "train_model(policy_model, training_prompts, iterations=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oSWkFgxUD-w",
        "outputId": "4fd6388d-872c-477c-a344-e4a16ab68ce2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -0.47951141993204754\n",
            "iteration 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.7511004408200581\n",
            "iteration 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.13246959447860718\n",
            "iteration 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 2.080908457438151\n",
            "iteration 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -1.7102519671122234\n",
            "iteration 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.7292830149332683\n",
            "iteration 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.6331895987192789\n",
            "iteration 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.09235954284667969\n",
            "iteration 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 2.5033911069234214\n",
            "iteration 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -0.251630703608195\n",
            "iteration 11:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.12434267997741699\n",
            "iteration 12:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.08274402221043904\n",
            "iteration 13:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.4080325762430828\n",
            "iteration 14:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.7650775114695231\n",
            "iteration 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -2.271008332570394\n",
            "iteration 16:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.4671982129414876\n",
            "iteration 17:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.750105619430542\n",
            "iteration 18:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.608530064423879\n",
            "iteration 19:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.8191476265589397\n",
            "iteration 20:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.7198676268259684\n",
            "iteration 21:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.6296799182891846\n",
            "iteration 22:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -0.1741150220235189\n",
            "iteration 23:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.7161304155985515\n",
            "iteration 24:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.3216264247894287\n",
            "iteration 25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.17237520217895508\n",
            "iteration 26:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.4448275367418926\n",
            "iteration 27:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -0.017626603444417317\n",
            "iteration 28:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 0.7708389759063721\n",
            "iteration 29:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: 1.60145370165507\n",
            "iteration 30:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated model. average loss: -1.0149595737457275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 11: evaluating the updated model\n"
      ],
      "metadata": {
        "id": "KguPHcX4VZld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(prompt):\n",
        "    \"\"\"\n",
        "    generates a new completion after model updates to evaluate changes.\n",
        "    \"\"\"\n",
        "    new_completion = generate_completion(prompt)\n",
        "    new_reward = reward_function(new_completion)\n",
        "    print(\"new generated completion:\", new_completion)\n",
        "    print(\"new reward score:\", new_reward)\n",
        "    return new_completion, new_reward\n",
        "\n",
        "# example usage\n",
        "evaluate_model(\"once upon a time,\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXYeg9FXUdqP",
        "outputId": "c9928599-fc39-4928-c927-8fd83d99a5f2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new generated completion: once upon a time, the world could not possibly have suffered as catastrophically a situation as the one in Egypt, which at the time the British considered to be the worst-kept secret. The United States had been doing its utmost to convince Germany\n",
            "new reward score: -2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('once upon a time, the world could not possibly have suffered as catastrophically a situation as the one in Egypt, which at the time the British considered to be the worst-kept secret. The United States had been doing its utmost to convince Germany',\n",
              " -2.0)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting all together"
      ],
      "metadata": {
        "id": "d60aMjyE8KYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: install required libraries\n",
        "# before we begin, install the necessary libraries if they are not already installed.\n",
        "#!pip install transformers torch textblob\n",
        "\n",
        "# step 2: import required modules\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from textblob import TextBlob\n",
        "\n",
        "# step 3: load pretrained model and tokenizer\n",
        "# we use a pretrained gpt-2 model for our rlhf experiment.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# define pad token explicitly to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# step 4: generate multiple sample completions\n",
        "def generate_multiple_completions(prompt, num_samples=3, max_length=50):\n",
        "    completions = []\n",
        "    for _ in range(num_samples):\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "        output = policy_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=1.2  # encourage exploration\n",
        "        )\n",
        "        completions.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "    return completions\n",
        "\n",
        "# step 5: refining the reward function with noise injection\n",
        "def reward_function(text):\n",
        "    \"\"\"\n",
        "    a refined reward function that assigns a progressive score based on politeness and sentiment intensity,\n",
        "    with random noise to encourage policy updates.\n",
        "    \"\"\"\n",
        "    polite_phrases = [\"thank you\", \"please\", \"kindly\", \"appreciate\", \"grateful\", \"respect\", \"much obliged\", \"thanks\"]\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "\n",
        "    # progressive scoring\n",
        "    if any(phrase in text.lower() for phrase in polite_phrases):\n",
        "        reward = 2.0  # strong positive reward for explicit politeness\n",
        "    elif sentiment > 0.5:\n",
        "        reward = 1.5  # strong positive sentiment\n",
        "    elif sentiment > 0.2:\n",
        "        reward = 1.0  # moderate positive sentiment\n",
        "    elif sentiment > 0.0:\n",
        "        reward = 0.5  # slight positive sentiment\n",
        "    elif sentiment < -0.5:\n",
        "        reward = -2.0  # strong penalty for highly negative sentiment\n",
        "    else:\n",
        "        reward = -1.0  # default penalty for neutral or slightly negative sentiment\n",
        "\n",
        "    # add small noise to ensure variation in rewards\n",
        "    return reward + random.uniform(-0.2, 0.2)\n",
        "\n",
        "# step 6: computing log probabilities for the generated text\n",
        "def compute_log_probs(model, input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    log_probs = -outputs.loss.item()\n",
        "    return log_probs\n",
        "\n",
        "# step 7: improving advantage computation to prevent zero values\n",
        "def compute_advantage(reward, reward_baseline=0.0, epsilon=1e-6):\n",
        "    advantage = (reward - reward_baseline) / (abs(reward_baseline) + epsilon)\n",
        "    return max(advantage, 1e-3)  # prevent zero advantage values\n",
        "\n",
        "# step 8: computing the policy gradient loss\n",
        "def compute_policy_gradient_loss(log_prob, advantage):\n",
        "    return torch.tensor(-log_prob * advantage, requires_grad=True)\n",
        "\n",
        "# step 9: updating the model using gradient descent\n",
        "def update_model(model, loss, learning_rate=1e-5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# step 10: training loop with debugging outputs\n",
        "def train_model(model, prompts, iterations=30, num_samples=3):\n",
        "    \"\"\"\n",
        "    trains the model on multiple prompts and multiple generated responses per prompt using progressive rewards and normalized advantage.\n",
        "    \"\"\"\n",
        "    for i in range(iterations):\n",
        "        print(f\"iteration {i+1}:\")\n",
        "        total_loss = 0\n",
        "        reward_baseline = 0  # baseline initialized per iteration\n",
        "        for prompt in prompts:\n",
        "            completions = generate_multiple_completions(prompt, num_samples)\n",
        "            rewards = [reward_function(c) for c in completions]\n",
        "            reward_baseline = sum(rewards) / len(rewards)  # update baseline dynamically\n",
        "            log_probs = [compute_log_probs(model, c) for c in completions]\n",
        "            avg_reward = sum(rewards) / len(rewards)\n",
        "            avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "            advantage = compute_advantage(avg_reward, reward_baseline)\n",
        "            loss = compute_policy_gradient_loss(avg_log_prob, advantage)\n",
        "            update_model(model, loss)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # debugging outputs\n",
        "            print(f\"prompt: {prompt}\")\n",
        "            print(f\"generated completions: {completions}\")\n",
        "            print(f\"rewards: {rewards}, baseline: {reward_baseline}\")\n",
        "            print(f\"log probabilities: {log_probs}\")\n",
        "            print(f\"computed advantage: {advantage}\")\n",
        "            print(f\"computed loss: {loss.item()}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        print(f\"updated model. average loss: {total_loss / len(prompts)}\")\n",
        "\n",
        "# example usage\n",
        "training_prompts = [\n",
        "    \"once upon a time,\",\n",
        "    \"hello, how are you doing today?\",\n",
        "    \"thank you for your help!\",\n",
        "    \"i really appreciate your kindness.\",\n",
        "    \"could you please assist me with this?\"\n",
        "]\n",
        "train_model(policy_model, training_prompts, iterations=30)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_PQRSmJzwH4",
        "outputId": "ac561cfe-a5d1-467d-f832-5aef13c4f594"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.1->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, but at least one of this is happening.\"\\n\\nThe study was published today (April 5th) in the journal Cell and will be followed by a later release that includes more on the link to these researchers and some of', 'once upon a time, you will be able to see him as one who always does his homework and always remembers what he likes.\"\\n\\nAnd that\\'s in what he means on a personal level, too: \"I\\'m very much a father figure', 'once upon a time, when there are only four people for many who are in danger for each other and yet all are capable of caring for one another. But there are still more to a relationship that does not depend upon some sort of force or power']\n",
            "rewards: [0.563179194320097, 0.8661526295122776, 1.0495142163934765], baseline: 0.826282013408617\n",
            "log probabilities: [-2.9378597736358643, -2.8032310009002686, -3.1920664310455322]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029777190648019314\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nWyatt: I am doing much better. In two weeks, I have got 10 games on [The Ticket] in the American market and it is in the playoffs.\\n\\nHow close are', \"hello, how are you doing today?\\n\\nIt's great that you have a better view in the front-right corner and I guess we could have even gotten some good shots back then. My only regret will be that I didn't make some\", \"hello, how are you doing today?\\n\\nI was sitting down with my girlfriend at 2 in the morning with my 3rd grade teacher a couple of weeks ago. When he asked if I didn't like the color of our school uniform, I\"]\n",
            "rewards: [1.1982109146891262, 1.0936721279197859, -1.0857712387755882], baseline: 0.40203726794444133\n",
            "log probabilities: [-2.8342525959014893, -2.849701166152954, -2.3485472202301025]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026775002479553223\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! I will be putting all updates coming from here in this blog into my social media.\\n\\nPlease feel free to suggest improvements here or even send me suggestions via this link. In the meantime – your feedback and suggestions will', 'thank you for your help! There is an existing donation at the bottom of the page that is completely needed by everyone. Thank you!\\n\\n-Jax\\n\\nThe game is under review, and there is no money for a full review.', 'thank you for your help! Thank you for your help!']\n",
            "rewards: [1.8532548829968505, 2.0076895982106233, 2.036582570782603], baseline: 1.9658423506633589\n",
            "log probabilities: [-2.9178011417388916, -2.7823801040649414, -1.8464999198913574]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002515560481697321\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. It\\'s true you\\'d love to talk. I hope we can work something out for tomorrow.\"\\n\\nHannah grinned and nodded in her ear, which seemed a little sad but obviously happy with the conversation. Her new', 'i really appreciate your kindness. Please stop wasting away my time.\"', \"i really appreciate your kindness. I'll take care of that for us.\\n\\n\\nBest regards\\n\\n\\nJohn\\n\\nWe got the video of her crying at this link: http://www.mediafire.com/file/5yv3\"]\n",
            "rewards: [2.051125785909168, 1.9711837166880606, 1.888832152761043], baseline: 1.9703805517860904\n",
            "log probabilities: [-3.2478532791137695, -4.342706203460693, -2.7456769943237305]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003445412265136838\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Your company is so valuable! I love working on this project! Thank you very much!\"\\n\\nThe following images (in alphabetical order) are of the project:\\n\\nhttp://julia.s', \"could you please assist me with this?\\n\\nYa: Ah~ I need some light now...\\n\\nXxXxxx xxxxxxxxxx xxxxxxxx\\n\\nYa: I really can't believe you did this, Yay...\", \"could you please assist me with this? And I've never had it to match up before, does that sound interesting? Can I ask you a simple question which might seem contradictory? I have a problem with it as it seems the best answer would be\"]\n",
            "rewards: [1.961176227504424, 1.8731883707436103, 2.01054781245655], baseline: 1.948304136901528\n",
            "log probabilities: [-2.5830562114715576, -2.7717232704162598, -3.13425874710083]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028296795208007097\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0028891743160784243\n",
            "iteration 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, and no more.\\n\\nThis is a great book I came to love - as does a superb collection of more accurate history, and an excellent look at the origins of Britain. It also makes a fine introduction to the role', 'once upon a time, it will be very difficult for the party of our nation to defeat a hostile power and restore liberty and security, while the country continues to suffer, with alarming regularity.\"\\n\\nThe first attack on our liberty and its security', 'once upon a time, which only happened when I was seventeen.\\n\\n\\nWhen I was thirteen, the first of the family arrived at school at a time when the city was very weak, and so we stayed off-the-shelf. So']\n",
            "rewards: [1.3887747475597587, -1.050525376801087, -0.9536096163345691], baseline: -0.20512008185863242\n",
            "log probabilities: [-3.057638645172119, -3.068385124206543, -2.8257393836975098]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029839209746569395\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nW: Actually a bit.\\n\\nGR: Yeah.\\n\\nGW: Is there anything about your past that's stuck to you at all today?\\n\\nW: Not for the money,\", 'hello, how are you doing today?\\n\\nMy name is Jack. I was a student in high school when my parents divorced. I just went to College in Illinois with my mother and I moved to Nashville with my dad so I was really close', 'hello, how are you doing today? We live in a country of laws. And no more laws, no more laws. Because in this country, everything is different - there is this freedom to be different and I find a place in American society for']\n",
            "rewards: [-0.9948154478390491, 0.5092000298064029, -1.1443668979645256], baseline: -0.543327438665724\n",
            "log probabilities: [-2.4588966369628906, -2.5465030670166016, -3.027876853942871]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026777589228004217\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! If you ever have a question about making a donation or even if you've done something else in the last 15 years and are interested I will gladly assist if you would like us to be able to. Thank you.\\n\", \"thank you for your help! The program's free, yet very active, members will be able to join in to donate their money at the next one or two events in 2014. There will also be a link to Patreon. I could probably do that\", \"thank you for your help! For many, many years you still have to work long hours and spend endless hours to get to the top. Well now that we have some experience, it's time to step up and do something for these guys.\\n\"]\n",
            "rewards: [2.0940576270842888, 1.8935249018448683, 2.179215999175556], baseline: 2.0555995093682378\n",
            "log probabilities: [-2.531367301940918, -3.1384122371673584, -2.7156548500061035]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002795144682750106\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. Thank you.\"\\n\\nShe shook her head and nodded once. \"You and I have been dating in all of my dreams.\"\\n\\nShe took a step toward her, then slowly turned to face me, who was', \"i really appreciate your kindness.\\n\\nI hope the letter contains the details. But do make sure to ask for my personal information as well. And you should probably also try a copy of your medical data (such as if you're trying to buy\", \"i really appreciate your kindness.\\n\\nLiz: We are sorry if this is hard to swallow but our deepest apologies. If you feel sorry for us, please feel free to feel free to send the following letters. In case you haven't already\"]\n",
            "rewards: [2.0842097186054263, 1.946404686963877, 2.063618823299902], baseline: 2.0314110762897353\n",
            "log probabilities: [-2.751812696456909, -3.194190263748169, -2.710049629211426]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028853509575128555\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? If anyone can, please take care of that in the field!\"\\n\\nYang took out two rings from her pocket with a \"H\" shaped ring as she pressed the buttons to the metal ring which was engraved', 'could you please assist me with this?', 'could you please assist me with this? And when do you plan to go to Syria on this?\" \"Why would a family from an Arab country join a criminal group.\" A middle-aged, blond-haired man named Omar told BuzzFeed News that he']\n",
            "rewards: [2.155346068118855, 1.931428428280567, 1.9287471763021675], baseline: 2.00517389090053\n",
            "log probabilities: [-3.0645718574523926, -3.1072323322296143, -3.1352813243865967]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003102361923083663\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002888907492160797\n",
            "iteration 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, or the time before the beginning of that which is to come. For that which begins only a few months ago will never cease after that time; yet at that time the true progress of creation is far from progressing toward it.', 'once upon a time, when you have not, will the Lord be gracious to you? For many have been troubled by the things about which men have been distressed, but now they know nothing that the Lord will.\\n\\n6 When men were perplex', 'once upon a time, the other ones that went away were taken as well, if not from those with whom it could have been resolved without the necessity of having any other motive and by a means other than that with which he possessed a desire or the']\n",
            "rewards: [0.586974106837935, -1.0410733459592623, -0.8869388578954971], baseline: -0.4470126990056081\n",
            "log probabilities: [-3.0377678871154785, -3.0548624992370605, -3.5867490768432617]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003226459724828601\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nA: Really great. We've gone around here and gone across it really hard. It was about getting together as a couple in town. We've seen the world outside. There was something beautiful going\", \"hello, how are you doing today?\\n\\n\\nLincoln.\\n\\n\\nNo.\\n\\n\\nWell, well. When I took over the office from Bill Keller and he took the office off -- after all that you're the head of that office,\", 'hello, how are you doing today? \" Well here is that, so it is no shock that if he went to sleep one morning, I have heard the whole story about how he died.\" And that was probably the way it went. \" It']\n",
            "rewards: [1.1765355528559966, -1.1420312792923997, 0.5205956984945144], baseline: 0.18503332401937045\n",
            "log probabilities: [-3.2289528846740723, -3.0469555854797363, -3.3899502754211426]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003221952822059393\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! We will now be using the data we gathered for the last 2 years at the top level of the DDSO – and you can choose to use this data to give us any feedback or suggestions to improve this site and', \"thank you for your help!\\n\\n\\n*If I could use your money to purchase a new laptop from you, I would. However, I don't have a lot of money to spend to buy a new laptop. Your support will do you much\", 'thank you for your help! (Please also check my other posts)']\n",
            "rewards: [2.110900588685351, 1.8714557485377545, 1.8130553347702911], baseline: 1.9318038906644655\n",
            "log probabilities: [-2.9695494174957275, -2.194770097732544, -3.05539608001709]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002739905146881938\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. We are all of us here trying to protect people but I cannot do it with all of us! This is our community and there are so many of you who still care so much, we can be the last ones with', \"i really appreciate your kindness. Is it really possible for some people on my side of this issue? I can't tell you what else I think of it. I'm the person in this room right now and I feel as if every decision I make\", 'i really appreciate your kindness. I have been having a couple of good nights off work, and I am glad to finally be back for Christmas.\\n\\nThanks for coming to my house. I love you and I hope to see you in a few']\n",
            "rewards: [1.8005579365154643, 2.0194727156408523, 1.9220070591922145], baseline: 1.9140125704495103\n",
            "log probabilities: [-3.233456611633301, -2.9989168643951416, -2.509824275970459]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029140659607946873\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? It\\'ll be really helpful, because the one who made that is so wonderful, they do an amazing job of it,\" wrote the owner.\"Just one note to confirm with you,\" the man then added. \"', \"could you please assist me with this? (laugh) You're my guest. Just tell me my name if I have to say it myself. Okay. I'm looking through these three items! Okay, I thought I would use them in a couple\", 'could you please assist me with this?\\n\\nJANE: Thanks,\\n\\nJAVE: I\\'ll help you, if any of your sisters were like, \"That\\'s nice!\" It\\'s just the way I feel, and I see these']\n",
            "rewards: [1.8372596569468036, 2.053107826482071, 1.821052008674338], baseline: 1.9038064973677375\n",
            "log probabilities: [-3.415451765060425, -3.2413368225097656, -2.933971881866455]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003196920268237591\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.003059860784560442\n",
            "iteration 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, and have you been working in the business as long as ever?\\n\\nIf your idea for a brand new home has nothing more to do with how you feel, then do what I do. Your words are important. And', 'once upon a time, when women started being more willing and willing to have the conversations they once did with men and had had, to feel welcome without being judged for it, and to not be pushed down like that, and that I want to understand', 'once upon a time, it will become a more efficient and more reliable system that saves all energy, makes the food available for everyone, and that is what I was talking about. My next step for this is that we go forward to see if we']\n",
            "rewards: [0.8438854844772853, 0.8581519548868584, 0.8553217765108893], baseline: 0.8524530719583443\n",
            "log probabilities: [-2.981999397277832, -3.2118537425994873, -3.1053571701049805]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030997367575764656\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nI am well and truly enjoying the results of this great tournament! Our community continues to respond to our players all with a variety of questions about their favourite player. We have made many great suggestions as to', 'hello, how are you doing today? (hugs, happy birthday!) Thanks, Santa!\\n\\nThanks a bunch, thank you guys for coming.\\n\\nThanks for getting me all busy! How are you doing today? Have you changed a', \"hello, how are you doing today?\\n\\nThe day before Thanksgiving, the couple is living together in Atlanta.\\n\\nHer husband, William, has a full-time job, and a brother has a job that's not related to politics.\"]\n",
            "rewards: [1.418370452853645, 2.0798220040496394, 2.095968605206864], baseline: 1.8647203540367159\n",
            "log probabilities: [-2.9794857501983643, -2.656424045562744, -2.626970052719116]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002754293382167816\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nYou should be able to view screenshots by tapping here if you don't want to download the original and save for a later viewing.\\n\\nDon't forget to add the latest Update as this patch can be updated\", 'thank you for your help! Thank you for sharing your enthusiasm. Thank you for spreading the word for your work. Thank you for making that change possible.\\n\\n[Read more on The Dangers to All of Us here.]\\n\\nHere', 'thank you for your help!']\n",
            "rewards: [2.0434264245594633, 2.01573629221547, 1.8625818444503885], baseline: 1.973914853741774\n",
            "log probabilities: [-2.7801148891448975, -2.484928607940674, -2.5483052730560303]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026044496335089207\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. Maybe I could be a better dad. But since you are such a lovely, loving person and a pretty good parent I'd never understand this attitude of a kid. You want to leave me alone? No. Because then\", \"i really appreciate your kindness. Also, we really appreciate your cooperation in making new versions of my game to be more efficient and useful! :) Thanks. I'm glad we finally have an option for any problems, bugfixes, performance tweaks, etc (\", \"i really appreciate your kindness.\\n\\n\\nThat said, do you think we could really pull them together without it being like a massive game between factions that will never be resolved? It's one that will make the show more complex, which I hope will\"]\n",
            "rewards: [1.8425597117538508, 1.9079826370210775, 1.8284506682060884], baseline: 1.859664338993672\n",
            "log probabilities: [-3.3095149993896484, -3.463463306427002, -3.4259235858917236]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0033996340353041887\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Your name is Harry Weasley and you were born on May 11, 1973. Your parents were both killed in an automobile accident at the age of 22 in Birmingham, Alabama. The parents of your sister are still unidentified', 'could you please assist me with this? I need your help if I can\\'t find you in the town.\"\\n\\nThe only one who could answer her was Ye Zichen, but he still had little time to spare to let her down.\\n', \"could you please assist me with this?\\n\\n\\nThere's a number of reasons why you would choose your own ship. It's not a perfect representation or the best choice but it's the way it looks. I've been asked plenty of questions but\"]\n",
            "rewards: [1.8778573882815928, 2.109916321724601, 2.0892248138578164], baseline: 2.0256661746213367\n",
            "log probabilities: [-2.47979998588562, -2.484349012374878, -2.8399441242218018]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026013643946498632\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0028918956406414507\n",
            "iteration 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, when that is an impossibility.', 'once upon a time, they had no idea why that was, let alone what the future will hold; when all would not be lost, they had no idea, let alone what the future will hold. Their lives began to be taken from them by', 'once upon a time, he took a step back from me and said nothing. Then his voice grew quiet and he said, \"It goes without saying that it never goes without my guidance, and I don\\'t know why I cannot continue. A year']\n",
            "rewards: [-0.9024512909333573, -0.8018415410099788, -0.8761257541354326], baseline: -0.8601395286929229\n",
            "log probabilities: [-3.4803104400634766, -2.6346113681793213, -2.7475757598876953]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002954165916889906\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? It really wasn't like that at all. I can't seem to be seeing those lights up. We have pretty good schedules. What is it? Our job is always working, at least in our first few\", 'hello, how are you doing today? Didn\\'t you go outside the building in advance?\" \"Yeah. That\\'s when I realized I didn\\'t know who the woman was,\" she continued. \"We were very quiet inside the building. It\\'s', \"hello, how are you doing today?\\n\\nWell, I'm just about getting out of here, you know? And I'll tell you.\\n\\nI'm actually a great guy now. And here's to a guy like me. You\"]\n",
            "rewards: [0.5768378867202575, -0.9030036592604949, 1.4513951703783976], baseline: 0.37507646594605343\n",
            "log probabilities: [-3.147933006286621, -2.7394282817840576, -2.4441030025482178]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002777154790237546\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! Thank you for my advice:', 'thank you for your help! :D\\n\\n\\n-Alliance and Horde in general', 'thank you for your help!\\n\\n-Lori -\\n\\nThank you for your help!']\n",
            "rewards: [2.010403510911217, 2.0874915580832587, 1.9006748775486006], baseline: 1.9995233155143586\n",
            "log probabilities: [-3.0570714473724365, -3.4519565105438232, -2.136436939239502]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00288182171061635\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. Thank you for supporting Rodeo, you have always been a special person and this time, you have my most humbled wish to serve a friend that I feel I have never known would be such a blessing for someone', \"i really appreciate your kindness. It also made for an excellent video game! The animation is great! There aren't many Nintendo games who I've seen that use the Nintendo Superstars model. What is more, in many of the Mario games this was\", 'i really appreciate your kindness. Thank you.\"\\n\\n[The first person to react in public was The New Yorker\\'s Ellen Nakashima.]\\n\\nTrump\\'s initial response, which prompted both Twitter users and those of Trump\\'s transition team to question']\n",
            "rewards: [2.097717130571581, 1.848828180183786, 1.8334330992443637], baseline: 1.92665946999991\n",
            "log probabilities: [-3.287294864654541, -3.5034854412078857, -2.9108786582946777]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032338863238692284\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this?\\n\\n\\nIt does exist. We can't give up on us unless we win with our lives. And if we don't, we will never survive.\\n\\n\\nBut that, and the fact that it makes\", 'could you please assist me with this? :D\\n\\nJenny B.\\n\\nN.F.J. Aye Aye !\\n\\nThe First Man (Sophie Johnson-Johnson, Aye)\\n\\nBENJ', 'could you please assist me with this?\\n\\nI think if I could have one question, it would be to help him.\\n\\nI wonder… do we have any way to reach this guy for help?\\n\\nI think…\\n\\n']\n",
            "rewards: [2.037299010950013, 1.9285710303539978, 1.983487059547754], baseline: 1.983119033617255\n",
            "log probabilities: [-2.7810208797454834, -3.2638862133026123, -2.8114254474639893]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002952110953629017\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029598279390484093\n",
            "iteration 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, you will feel like nothing special.\"', 'once upon a time, that this can be done in all the other ways, without any more effort in one\\'s own mind, I think, that this, that which I call \"the one\" are the true things in the universe and we are', 'once upon a time, or a day before it became a part of the game.']\n",
            "rewards: [0.8353885876530616, 1.095559612444685, -1.174556544838625], baseline: 0.2521305517530405\n",
            "log probabilities: [-3.481353759765625, -3.2558815479278564, -3.1482770442962646]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032951708417385817\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nI think I am more focused today.\\n\\nOkay. I am thinking about work today because we are all having lunch.\\n\\nThat could be the time I spend working for you at the office', \"hello, how are you doing today?\\n\\n\\nJ.D.: It was fantastic. It feels like I really was running, and you have to put that pressure on him as the person you're in a relationship with. You have to keep him\", 'hello, how are you doing today?\\n\\nWe were supposed to give this speech. I just want to say thank you for letting me be able to speak because I did. I will always remember it. My friends from the university and the community']\n",
            "rewards: [1.1555817409802063, 1.0333096455083493, 1.9638493559730896], baseline: 1.3842469141538818\n",
            "log probabilities: [-2.7195324897766113, -2.784485101699829, -2.682990789413452]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027290028519928455\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nI can't think of anything better to say goodbye to such a great band. The fact that so many of you were involved in so much of this creative process gave me so much pride, and so many love\", 'thank you for your help! Please visit our forums to join or join our community and share information related to this campaign. In the meantime you can follow other games and see the results if you are interested. Thanks for playing! The developers do not endorse', \"thank you for your help! We're hoping we can provide this information. Please contact the department's communications office or e-mail them any time with information on changes in terms of services and conditions in your home. If you would like the information to\"]\n",
            "rewards: [2.0696376720487155, 1.9770552709746168, 1.8101615672950295], baseline: 1.9522848367727874\n",
            "log probabilities: [-2.4010603427886963, -2.8127565383911133, -2.9380321502685547]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027172830887138844\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. But here you have my apologies I do not appreciate it.\\n\\nI had asked for your help before being given time away due to my wife's serious illness. I asked where she was to find her family and I\", 'i really appreciate your kindness. Maybe it\\'s your friend.\"\\n\\nMallory said he didn\\'t remember much about what happened during their conversation. Maybe she was trying to distract or to avoid it. Maybe she had been hit from behind while she', 'i really appreciate your kindness. My dear friend!\\n\\nMy love -\\n\\n\\nThank you so much from day one, my dear friend. It was a bit difficult having to deal with being called your daughter. You told me you were an accountant']\n",
            "rewards: [2.1338121337286435, 1.963471742919951, 1.935005897831605], baseline: 2.0107632581600665\n",
            "log probabilities: [-3.2465028762817383, -3.0505597591400146, -3.08939528465271]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003128819400444627\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? No No Please No No No, please try to read it without me No No, try to think of your own ideas No No No Please No No, please try to imagine a way for me to do that', 'could you please assist me with this? It\\'s really a lot and it\\'s not just that we need this, we need to get it done to get it done for all the people of the country. Do I have any support?\"\\n\\nG', 'could you please assist me with this? [29:28:17.846] SexualChocolate: oooooo [29:28:24.367] ChocolateRambo: I saw some shit last night [29:28:27']\n",
            "rewards: [2.198795755633242, 2.1867240233474754, 2.043871940328883], baseline: 2.1431305731031998\n",
            "log probabilities: [-2.5608251094818115, -2.6699726581573486, -2.249999761581421]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002493599196895957\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002872775075957179\n",
            "iteration 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, and he was, by the authority of the whole Church, elected Archbishop and Minister to the State.\" (Catechism of the Catholic Church, vol. 1, ed. T. A. JOHNSON, Philadelphia', 'once upon a time, he said:\\n\\n\"I\\'d like a picture of the day from my childhood.\\n\\n\\nOne day, I was on my way from school to the train. He took the picture – and I just remember it!', 'once upon a time, was a very big deal by the time the film was released, since it was set to be a movie about some serious people, and it was such a huge undertaking for them, so when they came forward with a specific date']\n",
            "rewards: [0.6422768634669092, -1.0748176613957385, 0.40460958085727833], baseline: -0.009310405690517018\n",
            "log probabilities: [-2.4180736541748047, -2.8749043941497803, -2.928041934967041]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027403400745242834\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nI know you did yesterday. Today you got to leave, too. That would seem to mean you needed to go into bed, too. I mean, it was my birthday on Wednesday and they told', 'hello, how are you doing today? I don\\'t know why I\\'m going to show it. But it\\'s a fun show!\" (Emma Smith)\\n\\nI love you, Maggie, even though this was probably the craziest ever of', \"hello, how are you doing today?\\n\\nYou're in a hospital, right?\\n\\nAre you ok?\\n\\nAre you all going to fall asleep?\\n\\nHow's it going?\\n\\nYou think you can do things like\"]\n",
            "rewards: [-0.8542242593610685, 1.1002292685417243, 1.105341844761824], baseline: 0.4504489513141599\n",
            "log probabilities: [-3.076730489730835, -2.935995101928711, -2.099442958831787]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027040562126785517\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nThere are plenty of options for you if you want it all! For example, the \"Permanent Tattoo\" series is awesome. You can use my tassel or custom tassel with my new', 'thank you for your help! [email protected]', \"thank you for your help!\\n\\n\\nDon't want to join the club to talk politics or hate the movement and just join us as we do in one place.\\n\\nIf you think it'll help, please stop by and check the membership page\"]\n",
            "rewards: [1.8427738160958702, 1.9211559091697428, 1.9348331258725036], baseline: 1.8995876170460388\n",
            "log probabilities: [-2.8535423278808594, -2.6611621379852295, -2.9651763439178467]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002826626878231764\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\nI also wish you a success, I'd like to thank you as well. Thank you for your time.\\n\\nLove,\\n\\nCarole\\n\\nIt's been almost three weeks since you gave up\", 'i really appreciate your kindness.\\n\\nI appreciate it.', \"i really appreciate your kindness. I know there can be bad things that people do, but I don't think there can be good, if I could offer them a smile. I hope they are able to be patient with themselves. They would do a\"]\n",
            "rewards: [2.1114637596960844, 2.162964170732374, 2.064359315603295], baseline: 2.112929082010585\n",
            "log probabilities: [-2.3859148025512695, -3.338928461074829, -2.9558825492858887]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002893575234338641\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?):', \"could you please assist me with this?\\n\\nMEGANY: Please.\\n\\nSHIMA: Oh, I'm here right now. Please. Can I see you? I'm really trying to.\\n\\nMEGANY: Okay\", \"could you please assist me with this? A) The other night one of my friends invited me over for dinner (who would've thought she'd attend dinner with her parents?). He was a regular at the table with me (and I'm glad you\"]\n",
            "rewards: [1.937610250315223, 2.0456391493267567, 2.0626500405488435], baseline: 2.015299813396941\n",
            "log probabilities: [-3.3813579082489014, -2.355138063430786, -2.9541072845458984]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028968676924705505\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002812293218448758\n",
            "iteration 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, though he was not at all eager for it. He said they might all make it, that they might get along, that there might be a compromise; and this and the whole thing was as easy as they could have taken', 'once upon a time, but they have it made clear from no other means, that this evil is real and can be eliminated.\\n\\nThus as it was by an angel, a devil is created.\\n\\nThus it was by a father in', 'once upon a time, people started showing up in the lobby of a local store and giving them free sandwiches; it was about being able to grab free bread for free as well as be there for free when there were people out for that. But they']\n",
            "rewards: [0.8149739142578689, -1.154428685701713, 0.8788429978790628], baseline: 0.17979607547840626\n",
            "log probabilities: [-2.894437551498413, -3.1539032459259033, -3.294004440307617]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031141149811446667\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Are you getting better at it?\"\\n\\nAfter giving Taylor some warning, Taylor nodded, \"I am.\"\\n\\nThey left, leaving a trail of silence throughout the city. Taylor followed the others while leaving', \"hello, how are you doing today? I told him that they're on their way back. All I'm doing at this time is taking on their little brother. I thought he would be happy to meet us in person so that we could be reunited\", 'hello, how are you doing today?\\n\\nWiley: I\\'m working really really hard. I\\'m always happy.\\n\\nThe whole \"You Don\\'t Have to Do This\" thing about getting to the top and being able to stand in']\n",
            "rewards: [1.1139858894776065, 1.1179508994775782, 1.0733384624970286], baseline: 1.101758417150738\n",
            "log probabilities: [-2.84538197517395, -2.900348663330078, -2.456775188446045]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002734168665483594\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! 🙂\\n\\n\\nClick Here for the Details on these great things!', 'thank you for your help!\\n\\nI am here for you and hope to see you again for the next few years to build something that really connects you.', 'thank you for your help!\\n\\nShare this: Email\\n\\nPrint\\n\\nFacebook\\n\\nTwitter\\n\\n\\nReddit\\n\\nLinkedIn\\n\\nTumblr\\n\\nGoogle\\n\\nTumblr\\n\\nPinterest']\n",
            "rewards: [1.9415013060289477, 1.9860347672850078, 1.8340190418233469], baseline: 1.9205183717124343\n",
            "log probabilities: [-3.335911750793457, -2.659968137741089, -1.0049556493759155]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002333611948415637\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I know I will never make a man sick because they don\\'t understand. All things considered, it is only logical that some more friends know what you mean.\\n\\n\\n\"My next friend has shown all kinds of nice', 'i really appreciate your kindness. :)\\n\\n\\nI am sorry... but I really cannot wait for you to go to visit me in your honor! I love to eat here, but you might also like a glass of sake. What do you mean?', 'i really appreciate your kindness. Please keep it in mind when you see it!\" I had a few friends, two of whom were actually a nice person, with whom I spent a lot of hours at that conference over the past year. We spent an']\n",
            "rewards: [2.0146964385282073, 2.134080836153907, 1.8601273148996214], baseline: 2.0029681965272452\n",
            "log probabilities: [-3.5377602577209473, -3.12785005569458, -3.195042848587036]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032868843991309404\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? Because I don't want to have to do that to myself.'\\n\\nShe says her first reaction was: 'What does a good lawyer have to deal with these allegations?'\\n\\nThe court heard how she\", 'could you please assist me with this? I\\'ve been trying to get an information about the source of the \"slam dunk?\" which he says happened on November 9th with the help of several men named Matt Larkin…that\\'s all I can', 'could you please assist me with this? You are a wonderful and talented woman,\" her response read with an emotionless chuckle. It was an odd moment of silence. The young mother of two young children knew it, but how did this make her think']\n",
            "rewards: [1.9021844881602024, 1.8627136455621502, 1.8702457388505593], baseline: 1.8783812908576374\n",
            "log probabilities: [-2.585843324661255, -3.3228821754455566, -3.2591941356658936]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030559732113033533\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029049506410956383\n",
            "iteration 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, I may have a greater desire to win, or at best I just give up the struggle completely.\"\\n\\nWith that, the crowd erupts like a torrent in a stream of people looking over a fire. It is unclear', 'once upon a time, I remember getting in my car and realizing I had seen this poster and felt like I was in the back seat. I drove down to the car in a different van.\\n\\n\\n\"That guy\\'s driving on a blue paint', \"once upon a time, it had to be at the heart of the situation. At the end of October 1941, a meeting took place on board a German submarine. Both sides were very happy with each other's performance in the battle on Sorens\"]\n",
            "rewards: [1.5876467749649157, -0.9196334318793677, 1.1809799294553134], baseline: 0.6163310908469538\n",
            "log probabilities: [-3.3664679527282715, -3.0622386932373047, -2.8453164100646973]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030913411173969507\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? A: Um... I was on day one. The only time I was home was on the 5th of January (4th November - 6th August). At my bedtime - I got up late -', 'hello, how are you doing today?\\n\\nA: Great. First of all you said I can have you next month? Second is I am having all your stuff on the plane this month you want me to start taking a photo.\\n\\n', 'hello, how are you doing today? Do you think she has a job for her family now?\"\\n\\n\"I see. We still have to go to church each day, at least. My husband will work a couple hours tomorrow. You\\'ve']\n",
            "rewards: [-0.9177634660960665, 1.1876881591692103, -1.1304551867083605], baseline: -0.2868434978784056\n",
            "log probabilities: [-3.027484893798828, -3.348421335220337, -2.9027438163757324]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003092883387580514\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! There isn't much more I'm going to do. Until then, I want you to go to your next job in New York and find out what it was like working with Mike's in NYC. Thank you.\\n\", 'thank you for your help! For your assistance or to help us get new designs put together!', 'thank you for your help!']\n",
            "rewards: [1.974134325243553, 1.864822863160518, 2.0499434362045315], baseline: 1.9629668748695341\n",
            "log probabilities: [-2.5766472816467285, -3.6669111251831055, -2.5483052730560303]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002930621150881052\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\n(Please follow @brucehavres on Facebook to send me messages about this).\\n\\nBrent is looking forward to sharing his experiences by joining me next month on @brucehavres to', 'i really appreciate your kindness.\\n\\nAs you would do any of us who are really struggling to find work and help the young, the hungry and homeless in need in the face of economic oppression, you may see me a little more or if you', 'i really appreciate your kindness.\\n\\nQ.\\n\\nWhen, exactly, is the last time you spoke to him?\\n\\nA.\\n\\nOn February 16, 2012, Michael Gittlis of North Dallas attended Kool-Aid Tea']\n",
            "rewards: [1.8083608921846375, 1.9464703140599529, 1.8193295204335118], baseline: 1.8580535755593675\n",
            "log probabilities: [-3.1578798294067383, -3.378342866897583, -2.8025503158569336]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031129242852330208\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? (Pause) Thank you. (He smiles)\\n\\nWe have a great time talking in the car.\\n\\n(He smiles back. She goes back to his phone) Hello, my name is Hannah', 'could you please assist me with this? What is it like as a kid in Japan?\" The two walked in front of the computer, but were only able to speak from in their cellphones.\\n\\n\"This is a pretty good question. I', 'could you please assist me with this? I think I\\'ve heard a lot. Thanks!\"\\n\\n\"Yeah, we\\'ll see you soon…\" Yueyang said casually, as he followed through the elevator into Yueyang\\'s office. There was more room']\n",
            "rewards: [1.9830266598837638, 2.0270018997130643, 2.1058484903792736], baseline: 2.038625683325367\n",
            "log probabilities: [-2.638620376586914, -2.8467183113098145, -2.9419169425964355]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028090851847082376\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.003007371025159955\n",
            "iteration 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, we had the option of not doing or not taking part in this campaign anymore, and even now with me and my family here at home being left to our own devices, this election would not come to an end. What I', 'once upon a time, with every step of her going in, and every step of hers leading up to this moment; with every step that is going in, and every step on which she should walk in or come to or come out.\\n\\n', 'once upon a time, a different world was born to her—to another people—to another, a different time.\" He said, sitting down and looking into the mirror. She didn\\'t ask him what it was again. Instead she shook her head']\n",
            "rewards: [0.8285772895489205, -1.0979841335238816, -0.9221074996913282], baseline: -0.39717144788876313\n",
            "log probabilities: [-2.9794745445251465, -2.8947184085845947, -3.0424609184265137]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029722179751843214\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nNUTLETON: Pretty okay. Just let me make it better. Just, I don't think it's gonna be quite like that today but it'll probably be the same tonight.\\n\\n\", 'hello, how are you doing today? It\\'s gonna start off with getting on the phone, so you should definitely check with your dad. The guy that is the other part, I guess it\\'s your dad. That\\'s why she does it.\"', \"hello, how are you doing today?\\n\\nI'm off to a good start. I've got nothing new to work on. After you finish the series you'll be working with me in New Orleans for two years. If you want to see\"]\n",
            "rewards: [0.8089335112026291, -0.882939938569445, 0.9859693319793267], baseline: 0.30398763487083696\n",
            "log probabilities: [-2.8891727924346924, -3.1711325645446777, -2.4648914337158203]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002841732231900096\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nHere\\'s a quick rundown of the changes which were made. You can find the changes by clicking the checkbox next to \"General\" in the center. You should have something like this on your first-look', 'thank you for your help!\\n\\nHere is a video of you (or any friend you are!) joining in for a friendly chat.\\n\\nIf you have any questions on this or any similar posts or anything about Bitcoin, just drop me a', 'thank you for your help!\\n\\nIn conclusion\\n\\nIn one sentence: The idea that \"the word of God must be as high a high-minded, high-spirited as possible,\" or, \"the Word must possess all its essential']\n",
            "rewards: [2.1418981945679434, 1.8360974782973196, 1.9803607300503352], baseline: 1.986118800971866\n",
            "log probabilities: [-2.4429256916046143, -2.624021530151367, -3.096280574798584]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002721075899899006\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. She\\'d definitely love to be married. It\\'s a dream come true for me.\"\\n\\nIn response to one of her own being contacted during their honeymoon, Iggy had his personal information deleted. But we don', \"i really appreciate your kindness.\\n\\nA very few things that need to be corrected:\\n\\nYou're welcome. There will be a good chance that they won't be using anything else for lunch that day.\\n\\nYou can see what we\", \"i really appreciate your kindness. I think she deserves special treatment from other young women around the world — not some bunch of losers from some college campus. If you don't want to take responsibility for what you do, I can give you this:\\n\"]\n",
            "rewards: [1.8110611947880875, 1.8747569371032629, 2.03263203515659], baseline: 1.906150055682647\n",
            "log probabilities: [-3.286334991455078, -2.944951057434082, -3.085644483566284]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031056434381753206\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Because I cannot please make anything of my time without your permission. It makes me want to make something of myself. Please help me. We cannot make anything from this place. We will all go back later.', 'could you please assist me with this? Thanks!\\n\\n\\nThank you so much! Thank you. I could tell, from my hands around the base, that I was in fact getting hit in the mouth again by an earthquake. I thought, I', \"could you please assist me with this? I want the best for you!\\n\\nAnd then he leaves and leaves. I'm going in this way too, I am an idiot I never understand. This place is all the more incredible because it feels\"]\n",
            "rewards: [1.8391072409901772, 1.9562589884606858, 1.9096749850310928], baseline: 1.9016804048273188\n",
            "log probabilities: [-2.8662009239196777, -2.9635353088378906, -3.2601418495178223]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00302995927631855\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029341257642954586\n",
            "iteration 11:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, for an even greater number of the righteous, for there are many who were righteous, and did right deeds and made sacrifices: and therefore, the children of the children of God go on to fulfill the duties which they received,', 'once upon a time, we saw all of the good things to do to avoid suffering. So now we look at what good we could have accomplished in trying to stop hunger, which, while not ideal on its own, can indeed make things better.', \"once upon a time, but you can feel it, it's in your hair, you have you here and now for all this time. You don't mean you have to look up at the ceiling. And if you don't, or something isn\"]\n",
            "rewards: [1.0113322710470594, 1.0269229546998726, -1.1999615304311186], baseline: 0.2794312317719377\n",
            "log probabilities: [-3.1328389644622803, -3.1362459659576416, -3.028489351272583]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030991914682090282\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? You're pretty awesome, and are having some fantastic time. [Laughs.] I think today was our 50th straight victory. But what makes you special to us in the past are people that have been with us\", \"hello, how are you doing today? You and I could have left this out tonight. It's not something you've worked with me to help solve, because I won't even be there until we talk.\\n\\nAdvertisement\\n\\nYou have your\", 'hello, how are you doing today? I\\'ve been at this since we did \"The Last Call\", which had very little budget for any kind of advertising at all. But all the advertising was coming from places like TV channels or radio shows, where']\n",
            "rewards: [0.9195496192099284, -1.0680551064379225, 0.44871925358966885], baseline: 0.1000712554538916\n",
            "log probabilities: [-3.1235883235931396, -3.037937641143799, -3.2057242393493652]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003122416790574789\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\n\\n-Budu\\n\\nEdited by lanced, 2 February 2014 - 08:02 PM.', 'thank you for your help! This piece of equipment should not be used on you if you already have it and may not fit into your vehicle if you have trouble locating your kit when it was purchased.', \"thank you for your help! I hope these things change and don't have my name used again. Thanks :) This project, like I've said before in the past, would require a completely separate thread, so I am not sure how easy and beneficial\"]\n",
            "rewards: [2.184455053200889, 2.062313943491162, 1.8159942460254028], baseline: 2.020921080905818\n",
            "log probabilities: [-2.8372793197631836, -2.968416929244995, -3.2611706256866455]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003022288903594017\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. Do I have any further questions for you, if any-I haven't gotten back to you yet but should if you would like to do whatever would be in your interests. Please-we're looking forward to seeing this\", 'i really appreciate your kindness. He\\'s so wonderful!\" \"Nu! Wait. What\\'s wrong?\" \"Oh man…he\\'s going back to his shop and selling.\" And then, of course, something happened. I\\'ve become pretty used to', 'i really appreciate your kindness.']\n",
            "rewards: [1.9666753482996036, 1.8994017584267588, 1.8216384913882393], baseline: 1.8959051993715341\n",
            "log probabilities: [-3.1177003383636475, -3.3757636547088623, -4.8637542724609375]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0037857394199818373\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this?\\n\\nKawakami: I don't understand as the reason why you have to explain.\\n\\nNagana: Please make sure to give this person your best attention, Nagana. I will\", \"could you please assist me with this? Thanks in advance!)\\n\\nThe fact that I have chosen to do research is really encouraging because it implies that the real question – why haven't the stories ever even come out on screen? – will be very\", \"could you please assist me with this? I will gladly lend you, as I want everything to work out for you! Oh please, if I could, you'd also get up to speed with something. Don't worry, I'll come see you\"]\n",
            "rewards: [2.1404075534641094, 1.9284359264377797, 2.0080392769418682], baseline: 2.025627585614586\n",
            "log probabilities: [-2.7923495769500732, -3.146760940551758, -2.8786609172821045]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029392570722848177\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0031937787309288977\n",
            "iteration 12:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, a time I am forever grateful to have known.\"\\n\\n\\nI met and talked to people over the last seven years, of those men and women I met and talked to over the years. Sometimes they even took the time to', 'once upon a time, that he might set himself against some creature: not out of his own desire for glory that had he no use for it, but out of fear and that of others and some malice. Such things, however, he did not', 'once upon a time, when all other things had been created for a thousand years.\" (Siphon of St. Augustine, III., vol. 3, 3 :2) \"We are to see God as the great builder of all things,']\n",
            "rewards: [2.1612217593002705, 0.9404439624002863, 1.119453192939678], baseline: 1.4070396382134114\n",
            "log probabilities: [-2.671653985977173, -3.2161202430725098, -2.963660717010498]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002950478345155716\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\n\\nMELBY: (chuckleful) Yeah, nice to meet you. The fact that you're here today is an indication of all the times that I've been in a fight. Just the\", \"hello, how are you doing today? I'm very excited to watch the action and feel like I was taken up. I'm very happy with my experience. The game is well played, the graphics nice, and the action action engaging. This means\", \"hello, how are you doing today?\\n\\nWhen they did come to, I gave them good news. My brother will be back later this month, so be sure to check back as soon as that happens. You're invited.\\n\\nAnd\"]\n",
            "rewards: [-1.1641732459420975, 0.911019877778761, 1.1828238927331467], baseline: 0.3098901748566034\n",
            "log probabilities: [-2.659212350845337, -3.0239570140838623, -2.651970624923706]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027783799450844526\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nI thought the following blogposts and emails would help with this:\\n\\nYou see a lot of people asking if I've found a problem or if it's the first time I'm posting something to this page\", \"thank you for your help!\\n\\n\\nFor a very good reason: in 2013, for the first time this year, the New Jersey Legislature voted down, once again, a law allowing the state to impose fines on municipalities that didn't comply with state\", 'thank you for your help! [link]\\n\\nWhat we need in this project... we need your help.']\n",
            "rewards: [2.035642007181535, 2.1358752715499505, 2.01406954735787], baseline: 2.0618622753631186\n",
            "log probabilities: [-2.7008140087127686, -2.594078540802002, -2.8137764930725098]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002702889731153846\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\n-Alex\\n\\nThe video I made for my site is also great. I really appreciate the outpouring of support from both the fans and the people to come and show them that you were the \"perfect\"', \"i really appreciate your kindness. And so the next week I'm getting home from work and the morning paper is being published in the Times, which I love but I hate, because it's sort of self-help. We've had a lot of\", \"i really appreciate your kindness.\\n\\nBunny in tow and good natured,\\n\\nGlad to see you back out here...you're really welcome\\n\\nYou always loved my mother too, right?\\n\\nAnd she used to like\"]\n",
            "rewards: [2.0492963442437278, 1.805446225902413, 1.9467569181438789], baseline: 1.9338331627633398\n",
            "log probabilities: [-3.1368486881256104, -2.9783740043640137, -3.240124225616455]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00311844889074564\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Because I have made some mistakes. I do not regret these mistakes as there is nothing to lose. Even if there is not, that does not matter. I need your help in creating my world from scratch and', 'could you please assist me with this? If not I shall have you, and shall then send you to where you were living with my daughter and with a man of her father\\'s.\" \"The lady has no children in her marriage, but you cannot', 'could you please assist me with this? Thank you. The first person I ask you to assist is my dear, wonderful-little, lovely, fine, and great Aunt Jemima.\"\\n\\n\"There are a number of ways to save the family']\n",
            "rewards: [2.077745730697494, 2.165173603500837, 1.9540387295539414], baseline: 2.065652687917424\n",
            "log probabilities: [-2.8629305362701416, -3.245974063873291, -2.9202282428741455]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030097109265625477\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029119815677404403\n",
            "iteration 13:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, but what if it wasn't? There will now be a real reckoning and a reckoning that we can all share.\\n\\nHere's what your future does in three minutes, and when that reality hits you, go and share\", 'once upon a time,\\n\\nAs we may go on from here,\\n\\nAll the gods have not come, and your days\\n\\nAre to have been\\n\\nFaced to go on, or fade, and\\n\\nYou will say', 'once upon a time, at once it took the entire human race for the last seven centuries to attain enlightenment (cf. Mt. 25:16). That said the great achievement of the age was never a mere matter of waiting until an inevitable end to']\n",
            "rewards: [0.6447623877023309, -0.9298534407905122, 0.5313738815438928], baseline: 0.08209427615190379\n",
            "log probabilities: [-3.0879406929016113, -3.039712905883789, -3.47196888923645]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031998741906136274\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? This is the fifth game the Cardinals have missed out on with a win since Aug. 4. How many are you thinking about playing over their next 8-13 games? Is there something I'm missing out on\", 'hello, how are you doing today? And what do you have planned to give us tomorrow?\\n\\nYou are planning to attend your first and foremost school — with help, money and time. Let me reassure you that your expectations and expectations for this', \"hello, how are you doing today?\\n\\nMolly:\\n\\nSo we are. As if the next big problem that it's not a problem of the economic system is all that there is to it. I think most people, the American\"]\n",
            "rewards: [0.32198435271894665, 0.871679414941216, 0.37036027699430957], baseline: 0.5213413482181574\n",
            "log probabilities: [-2.9522736072540283, -3.0508999824523926, -3.07356333732605]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00302557903341949\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nYou can subscribe to our emails about new news, releases, free apps, podcasts or any other app you have added to our list. If you're already subscribed and logged into Facebook it makes perfect sense to follow\", 'thank you for your help!\\n\\nIn addition, many of these documents also have a link to the PDF of this document. Be sure to follow our YouTube channel for links to more available information about our product.\\n\\nFinally, as long as', \"thank you for your help!\\n\\n\\nI'd like to send my thanks to any of you who have already purchased the game and were involved with this. I know I could have been involved too but I'd like to offer your assistance.\\n\\n\"]\n",
            "rewards: [2.1656070287332034, 1.9782523717436113, 2.109763537388195], baseline: 2.0845409792883367\n",
            "log probabilities: [-2.6349098682403564, -2.634408712387085, -2.4909164905548096]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0025867449585348368\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. There will be great things planned out and there will be some people involved.\\n\\nMORNING LANDER (from The Post): If you have this much trust to yourself then don't feel that you have to\", 'i really appreciate your kindness.\\n\\n\"My brother was also there, but he\\'d been out of town lately.\" He said. \"And he\\'s got to wait through my problems to find me.\"\\n\\n\"But how can he get out', \"i really appreciate your kindness.\\n\\nMy thoughts to you are\\n\\nDon't forget my name's Rachel and Sarah, and they are a real love, love my little sister.\\n\\nLove,\\n\\nMelanie May\\n\\nDear Rachel\"]\n",
            "rewards: [1.8763122232774476, 2.132093109108442, 1.9820903977190063], baseline: 1.9968319100349652\n",
            "log probabilities: [-3.41522216796875, -2.9343748092651367, -3.0821897983551025]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031439289450645447\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? [Sigh] You don't have to worry about me now... just tell them what happens to you at this level. That would keep me safe from everyone else... That would give them the time that they\", 'could you please assist me with this? If so, please tell me I am in danger and my body needs an amputation as per the conditions of this article and I don\\'t need to do anything.\"\\n\\n\"You may have a hard time', 'could you please assist me with this? I do have several answers for this.\"\\n\\n\"Don\\'t touch,\" the guard said, and her sword swung open, allowing a quick swing to go as she landed on top of what he\\'d grabbed.']\n",
            "rewards: [2.1493165218871924, 2.0446104570411703, 2.1547222547606326], baseline: 2.116216411229665\n",
            "log probabilities: [-2.8600332736968994, -2.8475217819213867, -3.186443328857422]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002964666113257408\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029841586481779816\n",
            "iteration 14:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, it has always been a hard sell. I always knew that things were changing—which was why I kept coming back. I never could believe that I wouldn't get out there and try, and what could possibly go wrong?\", 'once upon a time,\\n\\nHow, at last, do you feel?\"\\n\\nAnd then,\\n\\nIt had been long since the first person.\\n\\nShe looked back over her shoulder and smiled at him as she continued:\\n\\n', 'once upon a time, like a new moon, and the dawn becomes one hour after sunset, the Earth in a state of total darkness, and for thirty minutes after sunset the sky fills like two cottons in a house.\\n\\n\\nAnd when']\n",
            "rewards: [-0.9978316033420063, 0.467615434817677, 0.44463782680471275], baseline: -0.028526113906538846\n",
            "log probabilities: [-2.7779033184051514, -2.669607400894165, -3.2693722248077393]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029056277126073837\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? Well, it really is hard for a good friend of mine. He needs you very badly, even more than you do now.\\n\\nI'm on some sort of break because I'm not back until late\", \"hello, how are you doing today? It was great yesterday morning, so thank you so much and I will be back. And that's all. It was awesome. My next step I will be staying in Canada and there's going to be more\", 'hello, how are you doing today?\\n\\nOh, I am so excited!\\n\\nI mean, you can be up on the team like always.\\n\\nYeah, that really suits us.']\n",
            "rewards: [-0.943543127299054, 1.8466147017791867, 0.4269578921431215], baseline: 0.4433431555410848\n",
            "log probabilities: [-2.8813812732696533, -2.7734713554382324, -2.64453387260437]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027664622757583857\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nWe apologize that there are bugs in this project. Our community is pretty tight around this, so please leave suggestions that maybe it's better, please tell us anything you want to add.\", \"thank you for your help! A week and a half later, when I'm done playing, it works better than expected.\", \"thank you for your help!\\n\\nWhat is 'Culture Shock' ?\\n\\nAs with CULTURAL HEALTH CENTERS' EMERGENCY CHEMICAL MEDICS CARE (which is why many people don't feel very comfortable even\"]\n",
            "rewards: [1.8462010653972973, 1.825798000195174, 1.9694298279384124], baseline: 1.880476297843628\n",
            "log probabilities: [-2.8007516860961914, -2.836796998977661, -2.9896695613861084]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00287573947571218\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. We are very grateful to everyone who shares in your appreciation.\"\\n\\nThe New Zealand Herald', \"i really appreciate your kindness. He was such a fine man and I felt so blessed, he had such kindness! Thank you so much, I look so honored! I'll never be the same again… and I'm grateful for the kindness, you\", 'i really appreciate your kindness.\\n\\n\\nI think this would have helped the people and people like us. I am very grateful we had friends and family with us in the past few years and thank you to everyone that supported and supported us for such a']\n",
            "rewards: [2.002157674798822, 2.0318635791127955, 2.122798167041362], baseline: 2.0522731403176597\n",
            "log probabilities: [-3.097038507461548, -2.9756457805633545, -3.0472545623779297]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030399796087294817\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Or is your character set in such a different world?\"\\n\\n\"Yeah.\" It was a strange question. \"Do you believe there is any chance of us going to go down that road?\"\\n\\nHe', 'could you please assist me with this?\\n\\nAs a Christian\\n\\nI understand there is no sin, there is only sinfulness. But you need help to see clearly where I stand and where I was hurt.\\n\\nAs an American\\n', 'could you please assist me with this??\"\\n\\n\"I\\'ll tell it to the rest of the band of adventurers who are about to leave that castle. We have several orders for them to leave, and if they\\'re caught in the fight for']\n",
            "rewards: [2.043895547432822, 2.127835208165463, 1.9918725225433411], baseline: 2.0545344260472085\n",
            "log probabilities: [-2.5928494930267334, -2.7728888988494873, -2.8715198040008545]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002745752688497305\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0028667123522609474\n",
            "iteration 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, when every aspect of American culture and people were changed, is just a fantasy. It\\'s not a reality. It\\'s a myth.\"\\n\\nTrump will certainly take time to adjust this sentiment to the way his public image is', 'once upon a time, though sometimes at once, there be certain moments during which the game would not even play out in such a way; a game about winning a game, but a story about the characters. In the final cutscene and opening of', 'once upon a time, but that\\'s the one that gave them the real power: to shape the world. But for these to do it and be successful, they must overcome, and in this sense he was more valuable and more important than ever,\"']\n",
            "rewards: [0.5336919626672377, -0.832489248781871, 0.8444197404564249], baseline: 0.18187415144726385\n",
            "log probabilities: [-2.944032669067383, -3.392198324203491, -3.3700075149536133]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003235412761569023\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nNEXT DUST!\\n\\n\\nThe new map we\\'re putting together is really very cool. It\\'s about a new \"City\" (from DUST), where you get something new to explore on', \"hello, how are you doing today?\\n\\nJYNNY: I'm really looking forward to the summer. So you guys, there are so many challenges this week in terms of getting through the offseason. It's been really tough. This\", \"hello, how are you doing today? I mean a lot of you will need a few things, and I'll be there for you, I know about you. It's a nice surprise, you know? But I hope you enjoy having that meeting\"]\n",
            "rewards: [0.3095307032389679, 0.5923140443601709, 0.4728710133963229], baseline: 0.45823858699848724\n",
            "log probabilities: [-3.256086587905884, -2.543975591659546, -2.810973882675171]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028703452553600073\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! I did make this for a blog post and will post further updates when they are complete, but just a quick mention. I love it because I see all my fellow travelers as potential guests as opposed to having to make plans', \"thank you for your help! I'd love to find out how many of them there are and if any will be available, it will be in writing within the next 24 months!\\n\\nIf you have an idea about why there would be such a\", 'thank you for your help! Please send your info to @kirkstahl (takes you right on Twitter) . Thanks for stopping by!']\n",
            "rewards: [1.8043964795618361, 2.0443049466576153, 2.057503213879801], baseline: 1.9687348800330842\n",
            "log probabilities: [-3.1655516624450684, -2.505197525024414, -3.383932113647461]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030182271730154753\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. It\\'s great. And now everyone who doesn\\'t see me will go around begging.\" I smiled and smiled a great deal. My face was like a sea of red because my body stiffened, I had to get off', 'i really appreciate your kindness. It took me quite a while as well. Then I started receiving a note saying \"Dear Pim in your place.\" So the reply I received got less than 15 words on my blog post. And this means that a', 'i really appreciate your kindness. And if the guy wants to know what kind of woman you would have had in an industry where only men get to write the majority of the shit you write, he\\'ll get your ass to go, too. \"This']\n",
            "rewards: [1.9117770208344915, 2.0560949419683126, 1.8846315240485807], baseline: 1.9508344956171282\n",
            "log probabilities: [-3.4416749477386475, -3.5967049598693848, -3.231377124786377]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0034232523757964373\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this?): http://goo.gl/YHqN7Y [1] On September 23 2010 08:52:08 GMT+13, Triton said:\\n\\nWhat's up Triton\", 'could you please assist me with this? I\\'ve seen someone else do the same.\"\\n\\nHarry put his head down and frowned, causing his wand to fly out of the wand compartment into the air. \"Okay, well... you\\'re almost there', \"could you please assist me with this? (Pause) Okay. Yes, I just figured they would be all right now. They're all right here, all right, but this is why I'm not getting out of there. I can get to\"]\n",
            "rewards: [2.0207760871248217, 2.13125421688751, 1.9103723741875367], baseline: 2.0208008927332894\n",
            "log probabilities: [-2.982496500015259, -2.732698440551758, -2.8328142166137695]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028493364807218313\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.003079314809292555\n",
            "iteration 16:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, to be the object of their power?\" A young man, at the close of his youth, who, so he was taught, was already making a great and proud speech; yet we know he is the first that does not', 'once upon a time, it could seem, and they all know it, to make it their own. On the other hand, their own actions might become very unproductive and be seen only as a reflection of people. They might use any other motive', 'once upon a time, when the world would be better off without it. It has become the most popular book in every English university and every college.\" - John Milton\\n\\n\"It teaches the history of how we think about, where we are,']\n",
            "rewards: [0.9169098804412842, 0.3103210090496097, 1.0600691706367704], baseline: 0.7624333533758881\n",
            "log probabilities: [-3.447983980178833, -3.3849029541015625, -2.924264907836914]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032523840200155973\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nA: Well, I am feeling better and so I really want to, uh… get things going in the week and see how this progresses… I'll give you a little sneak preview next year:\", \"hello, how are you doing today? Thanks so much, everybody.\\n\\nJH: Awesome. Okay. Great job, Dave's been really productive, as well as your own colleagues.\\n\\nWG: I want to say I really\", \"hello, how are you doing today?\\n\\nYou are just amazing! Everyone is happy because they know you've helped them! You're so cool! Your work is more enjoyable to me. I love it! (I'm also in work and\"]\n",
            "rewards: [0.637278830364617, 1.9503528866591113, 1.5866453526659745], baseline: 1.3914256898965676\n",
            "log probabilities: [-2.877987861633301, -2.8656067848205566, -3.0098907947540283]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002917828503996134\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nThank you so much, but now I gotta tell you something we don't have enough time. As soon as it comes to that, everyone has to start preparing this new chapter. You know that, right?\", \"thank you for your help!\\n\\n\\nAnd don't forget you need to keep your email, phone and browser secure as hell: don't be an idiot or anyone but me, right?!\\n\\n\\nThanks again!\", 'thank you for your help!\\n\\nMy name is Zech and I am writing this to get a small update from my wife (also writing this) about her new laptop at work. She has an 8.6Ghz iMac with 4']\n",
            "rewards: [2.1948267109591564, 2.1139517496600533, 1.9601670697963962], baseline: 2.0896485101385354\n",
            "log probabilities: [-2.560537576675415, -2.871674060821533, -2.793727397918701]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002741979667916894\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. So I would tell you that it is important that you help and care for that lady, as much as anybody should have to, as I do. Thank you!\"\\n\\n\\nShe and other family members of the three were', 'i really appreciate your kindness. (pause.) You know, all those bad boy guys at my school? And you were actually like... that\\'s probably how you saw what had happened. \"I don\\'t think so,\" I say. There were some', 'i really appreciate your kindness. Thank you.']\n",
            "rewards: [2.0277023062862223, 1.9724738771650032, 1.9712972587734245], baseline: 1.9904911474082168\n",
            "log probabilities: [-3.2176353931427, -3.4584736824035645, -3.683239221572876]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003453116165474057\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? The answer is no. What we have to do now is make him look like a normal boy, because it\\'s impossible.\"\\n\\nThe boy sighed loudly.\\n\\n\"Okay,\" he said. \"I', \"could you please assist me with this? (sigh) I'll give it to you if I can. (Pause for effect. Don't cry, just hold still) Don't cry, if I can save your life and make you into the\", \"could you please assist me with this? (Pause) Please help! Help! Please.\\n\\nTighten your shoes. Move along. Just like we mentioned before we need to hold my hand to make the scene even more seamless. Don't\"]\n",
            "rewards: [1.9445651926658516, 2.0811953831269445, 1.8811121936424042], baseline: 1.9689575898117335\n",
            "log probabilities: [-2.4682929515838623, -2.5954911708831787, -3.1491167545318604]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002737633651122451\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0030205884017050267\n",
            "iteration 17:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, you may have heard about a person who found one of these \"reveals\" to be false.\" As the Supreme Court of Oklahoma made its ruling, it said he should be denied that status on the ground of an obvious', 'once upon a time, even after these times. To them I say that the most holy thing you, the most holy life for them, they can be for all time for you all. What good can it then to be happy to live your whole', 'once upon a time, it is always in the best interests of the people, even at one or two points in time, to stand firm together against all forces to prevent further harm.\\n\\nIn this sense, the \"coup\" has long']\n",
            "rewards: [-1.0359004742730253, 1.0394594686430414, 0.5304490288858883], baseline: 0.1780026744186348\n",
            "log probabilities: [-3.300305128097534, -3.6951887607574463, -2.8394112586975098]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032783017959445715\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? The only thing we need to say is thank you. We think in terms of getting people around here because we really think there's a huge potential for growth for here. We can't say no, because it\", 'hello, how are you doing today? I am going to go into some details soon. You are feeling alright, do you feel that way? Do you feel as if your body is moving, at least in a certain way, this is strange.', 'hello, how are you doing today?\\n\\n\"I\\'m trying to focus,\" she says. \"I\\'m doing this while it\\'s happening. We\\'re having a discussion.\"\\n\\nYou are standing on a train and you look up, the']\n",
            "rewards: [2.117708167774537, -1.1477818257852754, -0.8804422834258578], baseline: 0.029828019521134525\n",
            "log probabilities: [-2.851994514465332, -2.696397066116333, -2.547091007232666]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026984941214323044\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nMore', \"thank you for your help!\\n\\n\\nIf you love or want the video or any questions, check your local Youtube channels\\n\\n\\nDownload the official content - The Best Online Games Collection! The only games available now. Here's the list. If it\", 'thank you for your help!\\n\\nIf you liked this post, please use my patreon:']\n",
            "rewards: [1.8140669412363561, 1.9589207843319383, 1.8234026449486125], baseline: 1.8654634568389692\n",
            "log probabilities: [-2.393367290496826, -3.702463150024414, -2.280562400817871]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00279213092289865\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. [Charming laugh] A little bit, for sure. I\\'m also wondering if you\\'ve had any other contact related stuff with them either?\\n\\nTatsuya (speaking) \"That was pretty close, isn', 'i really appreciate your kindness. Thank you.\"\\n\\nAfter returning from the hospital, Shazier said he knew a man in the church for just over seven years, who told him he had a daughter and was struggling with anxiety. After the meeting', 'i really appreciate your kindness. For now your life is on the line and if there were a problem with it I might have to come back for this, but I\\'m pretty sure the same person I once called \"anonymous\" made her stay at']\n",
            "rewards: [1.9938077222943722, 2.089078643587525, 2.0166907417324595], baseline: 2.0331923692047855\n",
            "log probabilities: [-3.3945350646972656, -3.0147016048431396, -3.414102077484131]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003274446353316307\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?\\n\\n\\nWhat is the best way for you to take down this group, you ask me?\\n\\n\\nAny ideas would be much appreciated.\\n\\nWhat can you do please?\\n\\nWhat am I doing', \"could you please assist me with this? If you have not replied yet, please don't come in. Let us do this to her for a moment. And please think as you do, that she is happy, calm and pure. Be gentle with\", \"could you please assist me with this? If you're not sure of something I should know, or something is wrong with the code, and perhaps you just really can't help with it?\\n\\nHere is what we can try on the console.\"]\n",
            "rewards: [2.170302442337438, 1.9792411526918237, 2.1658747338599933], baseline: 2.1051394429630848\n",
            "log probabilities: [-2.4563214778900146, -3.083702802658081, -2.8388357162475586]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027929532807320356\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002967265294864774\n",
            "iteration 18:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, in such great strength, to fight such a war and have such victories as is impossible to meet.\\n\\nThe enemy is already at his heels!\\n\\nIt is not for me to say what that number was and what', 'once upon a time, people will have an idea of what is right; and what is not? Some can argue that the only good thing is that the person has the knowledge of what they want the result is not what he ought or can do in', 'once upon a time, in the present period of history.\" [See the second edition of our paper on Russian Military Power, p. 12 (New York: Robert and Company Printing Co. and other publishers), August 1995]\\n\\n[I am']\n",
            "rewards: [-1.0045572967382723, 1.0209985236959627, -1.1377747330031915], baseline: -0.3737778353485004\n",
            "log probabilities: [-3.066654682159424, -3.079735279083252, -3.292508602142334]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031462996266782284\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? [laughter] Oh. So I don\\'t get distracted by the car,\" he said. \"I like going about my business. I like the view and not just for money,\" he said.\\n\\nK', 'hello, how are you doing today?\\n\\n\\nWhat a great day!\\n\\n\\nWe spent the holidays doing our usual activities of cooking for you. The only way to get a nice holiday gift in your life is for everyone at the table to show', 'hello, how are you doing today?\\n\\nHear a brief explanation on how to get started in the office.']\n",
            "rewards: [-0.8395485860719389, 1.0865933645503243, -1.1039707486992276], baseline: -0.28564199007361407\n",
            "log probabilities: [-2.7103514671325684, -3.0222644805908203, -2.7906551361083984]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028410903178155422\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! The last straw was my friend and mentor. When his name was mentioned for the first time, I asked to be heard on the phone as her name would play an important part in his future career. We agreed to keep', \"thank you for your help!\\n\\nIf you want to contribute to the project without having to pay fees, or if you like working hard and contribute to the campaign, follow the instructions on what's required, how much you need, who gives whom\", 'thank you for your help!\\n\\nThanks for your help,\\n\\nKara!']\n",
            "rewards: [2.1740308223932723, 2.0153685023432084, 2.1244209009777295], baseline: 2.1046067419047367\n",
            "log probabilities: [-2.925382137298584, -2.7023258209228516, -2.215028762817383]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00261424551717937\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. Thank you very much,\" the officer stated as his voice got closer as he got out his gun and pointed it towards the man who was sitting on the other hand.\\n\\n\"What\\'s wrong?\" the two looked up', \"i really appreciate your kindness. You just made me feel like I have something to live for.\\n\\nHannity: Do you have a sense of humor like this one you wrote in the comic on how you don't even realize that your mother\", \"i really appreciate your kindness. It also seems like something you've experienced with other girls who feel like the need has been on your sleeve and you're still waiting to become your own dad.\\n\\nThank you so much!\"]\n",
            "rewards: [2.0324905861243954, 1.9599473360887487, 2.158224236266028], baseline: 2.050220719493057\n",
            "log probabilities: [-2.7700178623199463, -3.0308709144592285, -3.274289608001709]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030250593554228544\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Should I ask something like this or I ask something like this?\" The male female didn\\'t answer, so he didn\\'t. She didn\\'t need a condom. He said he didn\\'t believe a child could die', 'could you please assist me with this?\\n\\nPlease feel free to message me if your request is needed. Thanks in advance\\n\\nPlease note that these responses are subject to change without written notice.\\n\\nDo you think this survey or this link', 'could you please assist me with this?\\n\\n-Greetings the Commander\\n\\nCaster: This, indeed, is the only possible way for your mission. You are the very best at it. I have some questions for you, all of']\n",
            "rewards: [2.1530673001872334, 1.911980971198165, 2.084185564982422], baseline: 2.049744612122607\n",
            "log probabilities: [-3.068197727203369, -2.558628559112549, -3.0249266624450684]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028839176520705223\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029021224938333033\n",
            "iteration 19:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, we would like to make that case.\"\\n\\nWe ask for the following answers:\\n\\nCan you comment on this case?\\n\\nAre there people that would find you credible? I can\\'t respond to you personally but', \"once upon a time, not only would we be getting a better chance at catching the elite NFL players who can help put one of this sport's elite players on the map, but we'd soon be playing an ever-more competitive and exciting sport on\", \"once upon a time, and I just wanted the right story. The main character's first adventure is just fantastic - the setting works great for me, there's plenty to do around the house, the stories come across a little fresh and exciting every once\"]\n",
            "rewards: [0.4670571168929526, 0.9771534292585594, 0.8181619869042194], baseline: 0.7541241776852438\n",
            "log probabilities: [-2.8731729984283447, -2.9939746856689453, -3.3111684322357178]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030594386626034975\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? [Laughter.]\\n\\nCHANGE: Well, just, I was on Sunday night, so we're not making any secret about what's going on now. So I guess in any sort of good way\", \"hello, how are you doing today?\\n\\nBryce: Well, we're getting back to the hotel.\\n\\nWyatt: Is it on time? And, what's it like?\\n\\nBryce: Pretty much. My\", 'hello, how are you doing today?\\n\\nYou may know that our friends over here at Red Lion are having a couple of drinks at TLC on a regular basis, but you will also know just how hard the food has to be for these']\n",
            "rewards: [0.554515761732169, 0.4753440108892055, -0.8085640093544004], baseline: 0.07376525442232469\n",
            "log probabilities: [-2.7930142879486084, -2.2053494453430176, -3.0811760425567627]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026931799948215485\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! Thanks for your patience. Keep them coming!\\n\\n\\nWhat's up with that the whole idea of having the same character do a battle? Yeah, a battle for those with bad karma just might have worked... And if\", 'thank you for your help!\\n\\nPlease use the link above if you can, it helps bring some more insight into what a common problem a programmer creates. It really would be great if that helps your practice a little.\\n\\nHappy coding :)', 'thank you for your help!\\n\\nIt seems the rest of the team and you are here to help in the process. As a result, now is a great time. I hope you will join the team. Thanks again for your cooperation in my']\n",
            "rewards: [1.9904909083590165, 2.166408924686962, 1.872727385815387], baseline: 2.009875739620455\n",
            "log probabilities: [-3.454343795776367, -2.9221248626708984, -2.5101850032806396]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029622178990393877\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\nJIM MARINEZ: Thank you.\\n\\nMARRIE MATTERSON: Thank you for allowing me on with my talk. Thank you for standing alongside me, and now our next question is about', 'i really appreciate your kindness. This morning and today he was called by your great man and I would always like to thank you. I am so blessed for your generosity...\\n\\nThank you. This morning and today you are my little cousin and have', \"i really appreciate your kindness.\\n\\nAnd don't do this to those that try to prevent you. They may have done, but that's how I look at the world nowadays, if those people are to ever come back. They might have to\"]\n",
            "rewards: [1.9900642891900322, 1.8093490108789367, 2.0452734546729205], baseline: 1.9482289182472965\n",
            "log probabilities: [-2.8616485595703125, -3.349587917327881, -3.2410054206848145]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031507473904639482\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Thanks a lot :)\\n\\nMichelin_1339 : Just want to clear my head! No real way of knowing with that information? Thank you\\n\\n\\nMy first reaction after this was when I saw', 'could you please assist me with this?\\n\\nThanks\\n\\nBjørn Ajørn\\n\\nDjørn Djørn Djørn\\n\\ndjørn Djørn The Netherlands\\n\\nD', \"could you please assist me with this?\\n\\nA) He's clearly been suffering from severe depression as he desperately waits for some day (this might come from some other factor). So far his thoughts are being directed to his father as this is the\"]\n",
            "rewards: [1.963594444494005, 2.1234822672639506, 1.8169116695758416], baseline: 1.9679961271112656\n",
            "log probabilities: [-3.5233540534973145, -2.3668315410614014, -3.524367332458496]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031381843145936728\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.003000753652304411\n",
            "iteration 20:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, there would be no more. The only other place people ever had was in the cave in Tarkir.\\n\\nI walked on a small ledge beside the waterfall, staring up at it when I reached some of the other', 'once upon a time, but as a result of what had happened.\"\\n\\n\"Do those things change in a few years?\" The voice spoke from around him. \"They do. That\\'s what we are dealing with now.\"\\n\\nHe hesitated', 'once upon a time, I knew they would die in their own ways. But once upon a time there was a man like me,\" he remembers. And now there are them. So, like so many others in Chicago who have witnessed similar murders in']\n",
            "rewards: [-1.0498205255252333, -0.8642071908576263, 0.9494271373725013], baseline: -0.3215335263367861\n",
            "log probabilities: [-3.085659980773926, -2.739004373550415, -3.0329031944274902]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002952522598206997\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? You\\'re the best teacher to ever come from a new family member in any profession, as soon as you graduate...it will be in your blood to lead the family\\'s life as a student...\"\\n\\n\"', 'hello, how are you doing today? It\\'s not like you\\'re tired of reading and trying out new things anymore, but…it feels so different that there\\'s something special happening in my world.\"\\n\\nOn working on his writing: \"Honestly', \"hello, how are you doing today?\\n\\nI feel I have moved over from the older stuff, like having one hand in all the conversations and having one head at the table.\\n\\nIf you're not interested in this thing, then why\"]\n",
            "rewards: [1.431220000682121, 0.35273268386820567, 0.528977940212493], baseline: 0.7709768749209399\n",
            "log probabilities: [-3.1662590503692627, -2.9440035820007324, -2.951378107070923]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030205468647181988\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! It might cost you some time, and I want the best possible product. So we'll be adding all of those for our customers who purchase from etsy and then on to different styles, including new textures and designs for\", 'thank you for your help!\\n\\n\\nSo many amazing and helpful folks have contacted me through this thread in the forums, trying my best to put this together. I just started the project last night, and am super thankful that everyone who shares this has', 'thank you for your help!\\n\\n(Please note that this program doesn\\'t work with the \"Rigged\" version of any of the above plugins and plugins. It doesn\\'t do what the users think it will!)']\n",
            "rewards: [1.949466469271813, 1.9366179037931948, 1.975150244074136], baseline: 1.9537448723797146\n",
            "log probabilities: [-3.3760812282562256, -2.937483549118042, -2.853626012802124]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030557303689420223\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I know it\\'s the most amazing feeling of your kind,\" said one of his former classmates, Alex Sorensen, 46. He says his father is an insurance salesman and a social worker after his mother went uninsured', 'i really appreciate your kindness. We will thank you on the day of your wedding night,\" he wrote, thanking \"all those whom you have devoted the years to your service.\"\\n\\nA picture of the couple was also shown to him on Friday as', \"i really appreciate your kindness. It will help in the long run. I'll continue to do so.\"]\n",
            "rewards: [2.1306743332220175, 1.9429170929820845, 2.1220209016275446], baseline: 2.0652041092772158\n",
            "log probabilities: [-3.3486106395721436, -3.0468077659606934, -2.9548630714416504]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003116760402917862\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? It's something I need now since I really don't have my own computer. This may require some time since it doesn't need anything fancy… So here is my message:Please be patient until I give something\", 'could you please assist me with this? I\\'ve got a really big one right now…\"\\n\\n「Nn」\\n\\nShit\\n\\nAuras breathed out a deeply sigh. Shichirou was able to guess that this was only', 'could you please assist me with this? I am unable to handle your demands for your money.\\n\\nPicking up, her hand moved to where she was holding the money, and the black bag rolled into position before her eyes again. In a']\n",
            "rewards: [1.9211256617535428, 1.944261437561764, 1.9234512017723804], baseline: 1.9296127670292291\n",
            "log probabilities: [-3.270927667617798, -2.890357255935669, -3.0374972820281982]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030662608332931995\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.003042364213615656\n",
            "iteration 21:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, but he has yet to receive an explanation for how, until now he says \"In my absence, you are very weak.\" On a good, peaceful day: \"I cannot do much without that.\" For a man will often', 'once upon a time, he never spoke to me or said anything else about it. I just had the chance to look back at it and think about it. Now to take back an aspect of the game that I know will really be appreciated in the', 'once upon a time, the gods decided to create those demons the gods knew would destroy humanity. Now there were still demons, but they were not so well trained or used. Instead the spirits of humans are called, \"the dark beings\". They dwell']\n",
            "rewards: [0.5829790309479731, 1.8500841554112095, -0.8172798978492071], baseline: 0.5385944295033251\n",
            "log probabilities: [-3.54129958152771, -2.953338384628296, -3.4390997886657715]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003311245935037732\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nGORDON: We have had a busy week. That's why we went all over the United States to start this new chapter in our life.\\n\\nAdvertisement - Continue Reading Below\\n\\nG\", \"hello, how are you doing today?\\n\\nROGER GRACE: Good. So if we get off a few, it's almost a perfect storm, and when it gets out to a great extent, we really don't like it,\", \"hello, how are you doing today?\\n\\nAdvertisement - Continue Reading Below\\n\\nI'm on day 2 of my internship at the University of Maryland, but I've been working on writing an article about working in the hospitality industry.\\n\\nWhat\"]\n",
            "rewards: [0.5977242884946453, 0.8126544797078025, -0.9879904653669418], baseline: 0.14079610094516867\n",
            "log probabilities: [-2.2082436084747314, -2.939445972442627, -2.1386497020721436]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0024287798441946507\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! I hope this works out for your use. If you use any of these colors again, I will try and fix up your colors as best as I can for you at your home studio.\\n\\n\\nThis is so much', 'thank you for your help!\\n\\nShare this: Reddit\\n\\nFacebook\\n\\nEmail', 'thank you for your help!\\n\\nYou can read the whole video on Giga.\\n\\nClick here to donate through Patreon or through some other means.\\n\\nHave you been inspired? Have more questions? Take a look at our forums.']\n",
            "rewards: [2.0403266238227338, 2.080825320351053, 2.0139671770051164], baseline: 2.0450397070596344\n",
            "log probabilities: [-2.587111711502075, -1.5571413040161133, -2.4933464527130127]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002212533261626959\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I can honestly tell that a lot of this story is being told without a lot of due care, attention and judgement, and that isn\\'t quite right.\"\\n\\nHe added that he also feels that the case is not', 'i really appreciate your kindness. But she has also spent considerable time fighting for more. Is that fair? I just wish they were better at dealing with me.\"\\n\\nThis was one of those cases where an outsider could be trusted and yet was still', 'i really appreciate your kindness. My name is Laila Kainen and I come from an Irish immigrant family who hail from Kays. As they said, Ireland is a great place…\\n\\nBut in addition to my education background, I also']\n",
            "rewards: [1.9173646702962408, 1.9567938384387846, 1.903408304867171], baseline: 1.9258556045340656\n",
            "log probabilities: [-3.019533395767212, -3.378833770751953, -3.311460256576538]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032366090454161167\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Why don\\'t you send a letter of advice? Thanks!\"\\n\\nA man in his sixties walked in and brought the letter. \"A friend came here today from Moscow!\"\\n\\nBrought in by', 'could you please assist me with this??\"\\n\\nAfter a brief look behind me, he walked away. As soon as he got into the bushes and was walking, I felt something crunch as he pushed me backwards.\\n\\n\"What the hell', 'could you please assist me with this? Thanks!\"\\n\\n\"I do! I can help you!\" said Elsa.\\n\\n\"Oh yes, ma\\'am! Please, come here!\" Elsa gave way to the table; then looked to the']\n",
            "rewards: [1.9638026747274344, 2.1356837923564562, 2.1416839878469416], baseline: 2.0803901516436105\n",
            "log probabilities: [-2.9319944381713867, -2.857384204864502, -2.617300033569336]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028022262267768383\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002798278862610459\n",
            "iteration 22:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, and the whole matter took the air and its course were laid down into your hearts.\"\\n\\nA little while later, there was a great storm over India with all kinds of mischief that could not be stopped by the government in', 'once upon a time, the great man himself to the end had a man like Mr. Wilson. Mr. Wilson had worked for an iron firm, he has worked for a paper-manufacturer, he had spent fifteen years, or fifteen years at', \"once upon a time, and I would know what she thought of me. But I've decided to make up for lost time with one. I could just sit back a moment, and maybe even put on a short talk for a bit, or something\"]\n",
            "rewards: [0.6372251891229374, 0.4375116869727437, -0.8409293312302699], baseline: 0.07793584828847037\n",
            "log probabilities: [-3.3494760990142822, -3.265031099319458, -3.101562261581421]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032386898528784513\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nA: Oh, they're in a lot of mood. Our day starts, because the kids, if they come to our office to pick them up, we do them the same. I can't\", 'hello, how are you doing today? Did you stay up that night. Were you in bed this night, or were you awake late the evening? Or did you go to get that phone from the front door? The answer, I guess, is', \"hello, how are you doing today? Are things OK? If you didn't think this would be important for you, you should know by now: it is not. You are not an idiot and you want your thoughts, feelings and desires reflected to\"]\n",
            "rewards: [-1.0358258689942594, -1.198670455307602, 0.6148020676795213], baseline: -0.5398980855407801\n",
            "log probabilities: [-2.9434144496917725, -2.7489752769470215, -2.9575655460357666]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028833183459937572\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nThe story started when the group arrived at The Great Wall. After a brief tour, the group arrived by train, with a couple of people to walk away and have lunch together. It was a pleasant, un', 'thank you for your help! It was wonderful to write to me about how a simple, low budget design can pay off. Thanks again!\\n\\nPlease visit my website at:\\n\\nhttp://www.thedesignateofdesign.org/.', \"thank you for your help!\\n\\nThanks for visiting and keep doing what you're doing - this website was not designed by me and we use cookies & other technologies to protect and improve our information. You're free to control your settings below, but\"]\n",
            "rewards: [1.9160436464094788, 1.8405154013984695, 2.1983652741128017], baseline: 1.9849747739735832\n",
            "log probabilities: [-3.0212624073028564, -2.8625264167785645, -2.44618558883667]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027766581624746323\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. (Applause) So I\\'m starting to feel sorry for the many things that I\\'ve missed this week, my family, my town, my business.\"\\n\\n\"I hope it all works out in a nice', 'i really appreciate your kindness. And there will be a lot of fun at the show! I had some fun and thought up some nice ideas for my first character.\" \"I did love working on his first film. That\\'s one thing I enjoy writing', 'i really appreciate your kindness.\\n\\nIn case you missed some of my earlier responses, you can find them here.\\n\\nSo, my name is David Smith. I am an English Literature professor at MIT where I specialize in literary philosophy. So']\n",
            "rewards: [2.068832440101651, 1.8743940484793449, 1.9768714294356848], baseline: 1.9733659726722272\n",
            "log probabilities: [-2.8832526206970215, -3.506216049194336, -2.667306423187256]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003018924966454506\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? A) To do things,\\n\\nB) To do things, A) To do things, B) To do things\\n\\nAs usual, the request must be at the right address of the user to', 'could you please assist me with this?\\'\\'\\n\\n\"Yes. I cannot, Sir.\"\\n\\nI had tried to think of anything - how could you want someone who could read you like this?!\\n\\n\"The world does know all we', \"could you please assist me with this? Thank you!)\\n\\nWe hope you're all good. We've seen enough movies before with little to no payoff. And if you were lucky, you might just learn something new. One, if there's\"]\n",
            "rewards: [2.194633968078508, 2.156174118975812, 2.0563717496398857], baseline: 2.135726612231402\n",
            "log probabilities: [-2.4818475246429443, -3.1234495639801025, -2.987067699432373]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028641216922551394\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029563426040112974\n",
            "iteration 23:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, the state was a bastion of freedom—a refuge to the majority within—the heart of civilization—the one and only hope for justice and peace for all. As a result, a political force was born into society from', 'once upon a time, and with great vigour.\"\\n\\n\"I was not that kind of person!\" exclaimed the soldier in the carriage whose name is remembered in the letters of the chief. \"There is a general\\'s report that a strong army', \"once upon a time, she'd tell him they didn't like this new book, but they always did. So she had it in a few days, when it was ready to come out with a major update.\\n\\nThis isn't really your\"]\n",
            "rewards: [-0.8469864296881188, 1.6957033236056112, 0.5852479128541265], baseline: 0.47798826892387297\n",
            "log probabilities: [-3.135580062866211, -3.0859763622283936, -2.972858428955078]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003064804943278432\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nWATERTANNE MCCASTER, AARON MCEVERS\\n\\nYou were very surprised when your sister called for the fire. You're talking about one big one day that will\", \"hello, how are you doing today? Thanks!\\n\\nI got a question from the student about what kind of clothes you use, and he started doing his shopping and it was a bit surprising. I'm on the second floor here because we all\", 'hello, how are you doing today?\\n\\nKATHIE MATTHEWS-SEBASTIAN: This morning. Tonight. As I was, because my dad worked a big job. So, my dad put two and two together.']\n",
            "rewards: [0.5155521224411549, 2.1183201354180277, -1.0152740708351478], baseline: 0.5395327290080116\n",
            "log probabilities: [-3.3513410091400146, -3.127291679382324, -2.7946786880493164]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030911038629710674\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nHelp spread the word! Share Tweet 616 total shares total shares', 'thank you for your help!', 'thank you for your help! Please send questions about the program to support@spacepopedia.com or leave your review in iTunes. You can also follow us on Facebook or Twitter. You can even find us on Pinterest and StumbleUpon']\n",
            "rewards: [2.140063425194746, 2.179670925694788, 1.927559056371126], baseline: 2.0824311357535534\n",
            "log probabilities: [-1.3375897407531738, -2.5483052730560303, -2.343554735183716]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002076483331620693\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. There is some irony to the picture, but I just cannot help but feel for it.\" (Pascal Hidalgo via YouTube)', \"i really appreciate your kindness. And that you've provided such wonderful, honest responses. Thank you for taking time to let me know.\\n\\nYou've also offered an incredible job of caring for the people in your life. How do you explain that\", \"i really appreciate your kindness. I just hope it makes any other people around this group a little more appreciative. But even if I don't like that name, my sincere wishes to you have gone out with a very sincere, sincere congratulations. Don\"]\n",
            "rewards: [1.888398869273409, 1.913008355578629, 2.1138639065875213], baseline: 1.9717570438131864\n",
            "log probabilities: [-3.495284080505371, -2.872633934020996, -3.240186929702759]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00320270168595016\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? I will not rest until such time as there is more of my soul to consume.\" ―Shane\\'s mother to her infant son\\n\\n\"I told you, that I was not prepared to have such a', 'could you please assist me with this? This is my dream\".\\n\\nI was asked by a friend about an item for which you cannot sell.\\n\\n\"They make very very good knives. Please go find it. I wish you would show', \"could you please assist me with this? This could actually be what it was designed to be: a lot less expensive of a new concept than anything in your company's line of tools or development gear.\\n\\nWe're definitely in the process of adding\"]\n",
            "rewards: [2.103663834635896, 1.9230896541935183, 2.0011193473040723], baseline: 2.009290945377829\n",
            "log probabilities: [-2.908445119857788, -3.2974374294281006, -3.30403733253479]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031699733808636665\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029210134409368036\n",
            "iteration 24:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, of course. But I'd like to go through the steps, too, in case anyone might want to keep trying.\\n\\n[...]\\n\\nThat night, while we're in the dark, you meet Mimi and\", 'once upon a time, the greatest great battle and conqueror which is the world. The battle is the last battle of all times\" ― Dagon (Warlords of Draenor)\\n\\nLord Dagon was the son of Dax', 'once upon a time, she had gone back into the house where she lived with her mother-in-law when she was twelve years old. One evening she found a young man that she had followed from her home, who told her what had happened']\n",
            "rewards: [-0.860244853116396, 1.4249064219071577, 0.3223176993037797], baseline: 0.2956597560315138\n",
            "log probabilities: [-2.914626359939575, -2.9210855960845947, -2.33109974861145]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002722270553931594\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nA few points from my earlier question in this interview;\\n\\nWhat are you doing today, how are you doing today in terms of spending money, and what you're doing for yourself? How can\", \"hello, how are you doing today?\\n\\nTrevor: My name is Tony and we're really happy to be coming home, so this is not something I have planned to attend. I'm so thankful to see all of you here over\", \"hello, how are you doing today?\\n\\nWarmup-Harmony: We've got five-mile workouts, plus I'll be riding the bike, with I'm in the mood to learn new things. This is cool. It\"]\n",
            "rewards: [-1.046916122502785, 1.6844365338178617, 0.9505587603733452], baseline: 0.5293597238961406\n",
            "log probabilities: [-2.5366880893707275, -2.6246137619018555, -3.053732395172119]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027383447159081697\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\n[00:12:25] <+johndrei> kahhh [00:12:31] <theoarspawns> yes this is not true! [00:12', 'thank you for your help!\\n\\nWhat a great way to begin! In the future I will continue with the regular training! There will be lots more on this blog soon, and it will hopefully turn out to be great.\\n\\nPlease join', 'thank you for your help! If you can see how your code was compiled successfully or any other issues that are currently experiencing it, email me. :)\\n\\nContributing\\n\\nA quick example was for the Bamboo example. I am using B']\n",
            "rewards: [1.813469883965007, 1.8239984429035265, 2.0307985092532608], baseline: 1.8894222787072648\n",
            "log probabilities: [-2.6178641319274902, -2.671747922897339, -3.2569119930267334]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0028488412499427795\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I know that will be a lot of times, but my wife is super upset. I wish her well!\"\\n\\n\"Thank you for your gift. I love you very much.\"\\n\\nThey left my husband after', \"i really appreciate your kindness. As she approaches the man I'm making fun of for being nice, I am just trying to keep from getting drawn into it. He starts pulling at my jacket, trying to hold me down to one side. She's\", \"i really appreciate your kindness. I hope that you find me interesting and that I understand your purpose in my situation. So I hope that you find the same spirit. If my time is going to be needed to understand why I've been having so much\"]\n",
            "rewards: [1.9065144772992468, 1.8458613552245644, 1.8859950467212545], baseline: 1.8794569597483555\n",
            "log probabilities: [-2.7252328395843506, -3.2061209678649902, -3.053041696548462]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029947985894978046\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? I'll let you know about it if you feel like it.\\n\\nChapter 14\\n\\nIt was an exciting day in life. Every man was here at work. Every woman was here at home. Every\", 'could you please assist me with this? I hope I can use this as inspiration.\"\\n\\nHer hand grabbed a few cards from her wallet, but her eyes narrowed.\\n\\n\"Please give me the key for my house keys so I can do', \"could you please assist me with this?\\n\\n-Youth in any form of detention, I don't want any problems here with that man. He seems to be very kind and caring with his family and his friends.\\n\\n-If we\"]\n",
            "rewards: [2.122195476654013, 1.9739876176633164, 1.8023970578044726], baseline: 1.9661933840406007\n",
            "log probabilities: [-2.475942373275757, -2.7851314544677734, -2.960740804672241]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002740604802966118\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0028089719824492933\n",
            "iteration 25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, to be sure, even though the people of all lands were so rich and so cultured, were in common all over the world and many peoples had many good advantages. Even in the former times of this world the people of all', \"once upon a time, and what does your experience with it, and how well-wishers will react to it, tell you a little bit about your new position. You might also want to try it: you're in New Zealand, we have\", 'once upon a time, but also before a period which was then and now of long, and yet to have the good of man long; and if we do not at all follow the things which shall be stated there, by every thing which was in']\n",
            "rewards: [0.9647380735228122, 0.3568725739145594, 0.6364186462067298], baseline: 0.6526764312147004\n",
            "log probabilities: [-3.226748466491699, -2.9880146980285645, -3.5771100521087646]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032639577984809875\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? What can be done in the future?\\n\\nA. If we're going to have a major transformation, it could include all the things that have been done with our past, as opposed to things that are\", 'hello, how are you doing today? Do you think there is anything we can do?\"\\n\\nTucker said that as a senator, he believed that, while women had to be treated fairly and well, men had to treat their wives fairly and', 'hello, how are you doing today? \"No one was ever quite sure but we are doing pretty good. We\\'re seeing more than we are taking notes, getting a good look at the film and writing up the short in that regard. We\\'re']\n",
            "rewards: [-0.8023412441068192, 1.4947317924622816, 0.9163556652846756], baseline: 0.536248737880046\n",
            "log probabilities: [-2.61161208152771, -2.502204656600952, -3.106698989868164]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027401719707995653\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nFor other places you should check:\\n\\nhttp://nike.com/welcome', 'thank you for your help!\\n\\nAdvertisements', 'thank you for your help!\\n\\n\\nYou might enjoy this guide:\\n\\n\\n[url=https://pbs.twimg.com/media/K2N_M2wvLXa5IQ/prologue.']\n",
            "rewards: [2.133069508069172, 1.8314058067914223, 2.0404618299695354], baseline: 2.001645714943377\n",
            "log probabilities: [-2.574929714202881, -2.291238307952881, -2.674203634262085]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0025134573224931955\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. You\\'re right...\"', \"i really appreciate your kindness.\\n\\nDonations for both items, even if they're in exchange of donations, always come from the highest caliber and can get you free shipping anywhere in the US.\\n\\nFor you to understand that as our customer\", 'i really appreciate your kindness. I appreciate that!\"\\n\\n[The lady in the red costume stands in front of the house. \"Is that true?\"\\n\\nGee, who had never told Gek\\'s parents that he wasn\\'t actually alive']\n",
            "rewards: [1.9112305182196587, 1.9090187721828422, 1.9800769884020397], baseline: 1.9334420929348468\n",
            "log probabilities: [-4.458986282348633, -3.3744046688079834, -3.298123359680176]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0037105048540979624\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? If not, please let me know.\\n\\nHi,\\n\\n\\nI hope we can do anything for it. This site doesn't work so its my pleasure to provide it.\\n\\n\\nAlso if you do\", 'could you please assist me with this? Please help.\\n\\nI believe that the Lord has asked that she may make herself known before she goes further, but to all that ever will come out that we know this she is innocent. In these darkest', \"could you please assist me with this? That's why I need a team, is that that?\\n\\nThis one's for the fans! Check it out below!\\n\\nYou can see the full preview after the break.\\n\\nShare this\"]\n",
            "rewards: [2.173359495530937, 1.9596622475955459, 1.9408293287515506], baseline: 2.0246170239593444\n",
            "log probabilities: [-2.7167975902557373, -3.5326738357543945, -2.6439690589904785]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029644800815731287\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.003038514405488968\n",
            "iteration 26:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, or even an instant. The fact that we cannot see, imagine, even touch this entity with a small but measurable \"sense\" means that they cannot comprehend reality any more accurately, that he (or she) is only the', 'once upon a time, at an ideal location, at one of three different places or at the exact same time, he or she will meet you.\"\\n\\nIf a driver had their name that day, he or she would appear in the driver\\'s', \"once upon a time, they had to take the initiative that they needed to have good relations with each other's allies on different continents. The problem was that the people wanted to know how many war vessels had been found all over these islands while others were\"]\n",
            "rewards: [0.45930898368092765, 0.9144367527049888, 0.8199746572900344], baseline: 0.731240131225317\n",
            "log probabilities: [-3.4509494304656982, -2.9337873458862305, -3.2617299556732178]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003215488977730274\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nI'm busy going about the house, but we're having breakfast here.\\n\\nI love going on the market for the last year, and that is great.\\n\\nWell, we can get\", 'hello, how are you doing today? The whole day, every time I went into the restaurant, when a waitress came in to help me out, I was so relieved because that was the first time. Now I got over all the tension. I', 'hello, how are you doing today? Can you help me with that? Are you doing so much better? Can I help you with that? Thank you again, and we have so much more to be talking about. You made it happen. And']\n",
            "rewards: [1.0559062730266424, 0.8917917247576503, 2.0414186378538255], baseline: 1.329705545212706\n",
            "log probabilities: [-2.7818870544433594, -2.7882513999938965, -2.4483771324157715]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026728385128080845\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nThis post has been posted under a special thank you form for the following. If you are feeling a bit desperate to complete the first 20,000 posts, you might make a slight donation to our Patreon: http', 'thank you for your help!\\n\\nThank you for your support!\\n\\nThank you for your support!\\n\\nThank you for your support!\\n\\nThank you for your support!\\n\\nThank you for your support!\\n\\nThank you', 'thank you for your help!']\n",
            "rewards: [2.0635269905175506, 1.8891839063534013, 1.8873346260849382], baseline: 1.9466818409852966\n",
            "log probabilities: [-2.7611844539642334, -0.5209858417510986, -2.5483052730560303]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0019434918649494648\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\nDirk Culp: This is an important issue, because one has to think about this every day. There needs to be a clear goal to this world and a framework for how to achieve these ends. One', 'i really appreciate your kindness. It makes sense he\\'d be interested in trying out the app as well.\"\\n\\nDiana did a thorough investigation. While there has been one minor issue regarding iPhone 5C owners, the phone itself was extremely underpowered', \"i really appreciate your kindness. Also I'd like to express my sincere gratitude for being so responsive and caring.\"]\n",
            "rewards: [1.9034755665212246, 1.8952540499221835, 1.9112383857741206], baseline: 1.903322667405843\n",
            "log probabilities: [-3.1445720195770264, -3.4663891792297363, -3.151390552520752]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0032541172113269567\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? I'm sorry? You're only helping out with half the project and what!? ... ... It's okay! Just tell me the name of the girl on the screen... That girl is beautiful and that you need\", 'could you please assist me with this? I am unable to help you. I really hope she dies. Thank you, my dear. We hope that she will become a very good wife.\\n\\nHer next words were a little more solemn than usual', 'could you please assist me with this? Please?\"\\n\\nZhou Xinzi raised the head, giving a big sigh.\\n\\nHe did not see the red dragon with this much talent in his hands. What exactly was behind his hair? What']\n",
            "rewards: [1.9814947269052905, 1.9322138118061447, 1.8790246293559392], baseline: 1.930911056022458\n",
            "log probabilities: [-3.297886371612549, -2.7610859870910645, -3.1766750812530518]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030785491690039635\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0028328971471637487\n",
            "iteration 27:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, the universe must undergo changes with the addition and improvement of materials by way of solar system systems. This means that a planet's mass has to be shifted so that a star with 100 million suns would emit a little of matter\", 'once upon a time, I can see a large number of individuals, sometimes children or families, who are coming to see what the world can look like in 20 years\\' time.\"\\n\\n\\'Tensions between the Israelis and Trump remain highest at home', 'once upon a time, where, in the middle of the night, and in spite of all odds, and for not only a desire for a higher reward for more pleasure, but also a desire for the highest perfection. These qualities could be compared with']\n",
            "rewards: [-0.8168454365220511, 0.9400946275437955, 0.4304241182902583], baseline: 0.18455776977066754\n",
            "log probabilities: [-3.377519369125366, -3.0809316635131836, -2.9537980556488037]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0031374164391309023\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Thanks.\"\\n\\n\"I see,\" Hermione said, looking as confused as ever. It had begun as normal, and now even the blonde girl found herself getting her heart stopped as she stared down at the door', \"hello, how are you doing today? How is your school, and your school is getting better and I'll never want to lose anything again but you know that right now?\\n\\nJH: Yeah, and you know that I can look at\", 'hello, how are you doing today?\\n\\nThis comes straight from my favorite \"best of\" lists. This year\\'s lists contain good news and bad news from things that are already hard to measure in real-time. The \"worst of\"']\n",
            "rewards: [2.067621337202405, 0.8298371072130877, 0.3648956388058836], baseline: 1.0874513610737921\n",
            "log probabilities: [-3.0121335983276367, -2.958756685256958, -2.8174068927764893]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029294323176145554\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!', 'thank you for your help! If you spot something please report it to the issue tracker on Github to report issues you find, also, try to ask a number of people directly with your questions and feedback.\\n\\nThank you for your time', \"thank you for your help!\\n\\nAnd remember—don't be the same guy next to me. I'm too busy fighting off the bullies in my office—there are more men out there doing the right thing than ever before—and my bosses\"]\n",
            "rewards: [2.0306544677701215, 1.813448748519088, 1.9509045911612366], baseline: 1.9316692691501487\n",
            "log probabilities: [-2.5483052730560303, -2.7462639808654785, -2.8232827186584473]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002705950755625963\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\nKirsty: Okay, so, this morning, I made a huge promise to Sarah and that it be 100 percent free at the store in exchange for the rest of your time with us. I do this', \"i really appreciate your kindness. You're just a really great guy and I really appreciate everyone's understanding. So thank you so much for reading my messages and my story. Please consider this an e-mail- I have no idea what I'm gonna\", 'i really appreciate your kindness.\\n\\nYour life, my life.\\n\\nI appreciate your kindness.\\n\\nBut, I can say this again: I am grateful for your generosity. I would never want to hear you say I\\'m \"f']\n",
            "rewards: [2.0629331166739515, 1.8519262583071074, 1.9827587622741487], baseline: 1.9658727124184026\n",
            "log probabilities: [-3.152620553970337, -2.8142330646514893, -2.5072648525238037]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002824706258252263\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?', 'could you please assist me with this? Should I take your attention instead?\"\\n\\n\\n\"I do.\" Her voice got hoarse after a moment.\\n\\n\"We can ask her out.\" Yang said before finally leaving.\\n\\n\\nThe first time', 'could you please assist me with this? Is it alright if I tell you this? I\\'ve made sure my brother\\'s been asleep for the longest time but sometimes it\\'s hard to have fun.\" The expression on Yang\\'s face gave me a very strange']\n",
            "rewards: [1.884488770816605, 1.893308546324012, 2.093753136965439], baseline: 1.9571834847020186\n",
            "log probabilities: [-3.1072323322296143, -3.029034376144409, -2.9614064693450928]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030325576663017273\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0029260126873850824\n",
            "iteration 28:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, where is it now that I must see a clear and well-defined line for all to see?\"\\n\\nAfter a moment of deliberation, \"you\\'ve not been reading the whole book?\"\\n\\n\"Well, not', \"once upon a time, the whole story unfolded over a period of just months or a year before my daughter's arrival. He would make our entire experience very brief if only for that reason alone. This was my time.\\n\\nHe worked under an\", 'once upon a time, the best thing that you can think of to do would be \"do what the Lord says, and do nothing for the Lord your God\"; and do what has been in the past \"made possible by the Spirit.\" That one']\n",
            "rewards: [0.48726030738173, 0.6863522677923019, 1.107121043915144], baseline: 0.760244539696392\n",
            "log probabilities: [-2.8192403316497803, -3.488952875137329, -2.7220571041107178]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003010083455592394\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\n\\nLAS VEGAS -- Here we stand in a big, big hall, this is the convention center, the great hall, you have to move forward with your vision if you're going to bring\", 'hello, how are you doing today? I had to give a quick \"thank you\" on the way in the first place. As far as I know, we had a really cool party just this morning… I mean, it\\'s like you got', \"hello, how are you doing today?\\n\\nI know this is hard but I can say this: You got to get more out of yourself every day to succeed, be productive in life. I think if we're having this conversation on social media\"]\n",
            "rewards: [0.6474985085361189, 2.011504921699332, 0.5280593390684463], baseline: 1.0623542564346324\n",
            "log probabilities: [-2.8049960136413574, -2.781212568283081, -2.7601847648620605]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00278213107958436\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nSo you see, the game still feels a bit too fast. And that's with the release of The Sims, another game that took on that feeling of speed by turning up the playing rate of your family.\", 'thank you for your help!\\n\\nYou may have read about how an experiment on this topic will take place: http://gist.github.com/vjvf-sgt/b8fdd0e3db46dc', 'thank you for your help!']\n",
            "rewards: [1.8396937687214532, 1.935400239318916, 2.086675357493943], baseline: 1.953923121844771\n",
            "log probabilities: [-3.2058498859405518, -3.1446259021759033, -2.5483052730560303]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002966260304674506\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. But now you've already raised $50,000 and already the video of them throwing a party of their own. What do you all think of me giving money to a party which is happening? Is this for charity?\", 'i really appreciate your kindness. You\\'re taking a risk.\" She sighed, \"I\\'m hoping that will work out.\"\\n\\nHe nodded, \"No problem, you could really work with the boy. I want to see how many of your friends', 'i really appreciate your kindness. I\\'m going to do everything to protect the privacy of our kids,\" said Bill Clinton, his wife, later adding: \"The best friends, the best values, are with us all. Your support and prayers are with']\n",
            "rewards: [1.8027786668746795, 1.9627402739193684, 2.108865981669343], baseline: 1.958128307487797\n",
            "log probabilities: [-3.2854108810424805, -2.9035356044769287, -2.9681742191314697]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030523736495524645\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this?\\n\\n[14:38:58 AM] Chris Kluwe: and i promise if i have to ask any questions about the current state of the race and which people i agree with, it isn't because\", 'could you please assist me with this?\\n\\n\"Do you know where I live? All of our apartments in London are connected and we are in fact a city of beautiful houses, and we have always been able to live in the city through the', 'could you please assist me with this? How can I help you?\"\\n\\nGemini answered with an exaggerated smile, before disappearing. However, as an illusion, the light emitted by the ghost that had entered the room was truly completely different to']\n",
            "rewards: [1.8898523684806934, 2.0213483890820156, 1.8365642992584763], baseline: 1.9159216856070618\n",
            "log probabilities: [-2.770500898361206, -2.8288674354553223, -2.936923027038574]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002845430513843894\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002931255800649524\n",
            "iteration 29:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, of it's existence, has become an enigma unto himself, and to have been discovered. His whole life the same, that he had seen the same, and felt it, before he found it himself: And so he\", 'once upon a time, that they had only to pay his brother for this of their own accord.\\n\\nSo this story went on and on for about a week.\\n\\nAnd on the 9th day of January, the following week, the', 'once upon a time, with the rise of the Industrial Revolution and the rise of the West Coast as a whole, has been a relatively small number of people in the United States who actually participated in both events – perhaps about 100% of Americans.\\n']\n",
            "rewards: [0.5417523696256767, 0.40205978491985406, -0.924487366003638], baseline: 0.00644159618063093\n",
            "log probabilities: [-3.255443572998047, -2.6876420974731445, -2.812782049179077]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002918622689321637\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? How are you going to go about your life today? How would you think about your children tomorrow?\"\\n\\nHe sighed, his heart suddenly breaking out in alarm.\\n\\nThe fact that he needed so much', \"hello, how are you doing today? And then you take a long walk through the town and you think 'this is wonderful'. I think a lot of people in that area get carried away by the fact that it's one big urban landscape, and\", \"hello, how are you doing today?\\n\\nWell, I haven't slept for a minute. I'm back to my usual routine. I'm taking three meals a day of what used to be a big day of running for a long period of\"]\n",
            "rewards: [0.688328763227211, 0.9064945295177914, -1.107732003949467], baseline: 0.16236376293184515\n",
            "log probabilities: [-2.501314640045166, -2.741403102874756, -2.426706314086914]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002556474646553397\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\n-Alexia', 'thank you for your help!\\n\\nPlease send a ticket at:\\n\\nMoodline Contact@moodline.com\\n\\nPlease use the #moodline hashtag of your choice\\n\\nor use the #Moodline hashtag of', 'thank you for your help!\\n\\n\"And all my brothers are now dead. So we are both out of my world...\"\\n\\n\"Then why isn\\'t he around right now?\"\\n\\nYuzuru suddenly opened his eyes: \"I']\n",
            "rewards: [1.9899197478505135, 2.0995006458097385, 1.945712450729805], baseline: 2.011710948130019\n",
            "log probabilities: [-2.9635009765625, -2.1666221618652344, -2.9606611728668213]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026969281025230885\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. It is such a wonderful privilege to be honored in the memory of the men and women who are most responsible for the great work and sacrifice that is the human life.\"\\n\\nIn 2009, Clinton traveled to Israel to share', \"i really appreciate your kindness. She's amazing!\\n\\n\\nGood grief, do not be such a coward you think, because for your sins you will be pardoned.\", 'i really appreciate your kindness.\\n\\nPlease respect all members and friends for respecting their different needs and cultures.\\n\\nPlease consider joining us to spread the word of our shared goal by sharing images and other information to make this site better.\\n\\n']\n",
            "rewards: [2.0249995996524652, 2.170670348353888, 2.0770744458491026], baseline: 2.0909147979518186\n",
            "log probabilities: [-2.790501594543457, -3.482084274291992, -3.2162883281707764]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.003162957960739732\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Why do some people have allergies?\"\\n\\nAs his son watched her, she smiled at him and looked him straight in the eye, trying to calm him down. His response to that was a few simple words', \"could you please assist me with this? (I've just used my cell) Thank you and Best wishes to our next chapter! As you wish us, please visit us in the store!\\n\\nAfter a moment, I have noticed that no longer\", 'could you please assist me with this?\")\\n\\nS: (sounds like there\\'s some way to handle it, and I\\'ll get this sorted as it goes)\\n\\nM: No need to, just ask your old friend about the']\n",
            "rewards: [1.8843432544509915, 1.9341384346542845, 1.8076750909927424], baseline: 1.875385593366006\n",
            "log probabilities: [-2.6956138610839844, -3.5655770301818848, -2.9399750232696533]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0030670552514493465\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0028804077301174402\n",
            "iteration 30:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, as of our time, when the world is no longer divided among nations, but is interdependent in its common interest by the social contract, namely its protection by mutual interests or by private property. That is, what we think', 'once upon a time, we could not believe our ears. As you might expect, we were all very surprised when you first came here today. But you are right. We have already received your letter of introduction, but this time only in English and', 'once upon a time, and I wanted to come back and try, it\\'s been like that for five years.\\n\\n\"For a lot of people, that\\'s a huge change from when I first started. If that makes you more comfortable in']\n",
            "rewards: [-1.1048213653509271, 0.34132532374490054, 0.8183837880168062], baseline: 0.018295915470259883\n",
            "log probabilities: [-3.3408355712890625, -2.9327402114868164, -2.516989231109619]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0029301883187144995\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? You are absolutely doing well. We're working on things in practice but I have to say I'm really pleased with your work for today. We really have some good players on our team, and I've seen\", \"hello, how are you doing today?\\n\\nI can't speak to any details about what was on my mind but there was some kind of anxiety, some anxiety that had come up. It's something that you have never seen before. I guess\", 'hello, how are you doing today? Have you learned anything about the game, and what would you suggest anyone should be looking out for when planning your next event, including your pets?\"\\n\\nTo keep up with the all these news, visit our']\n",
            "rewards: [1.945089370465498, 1.5462623085201388, -0.8578855550758423], baseline: 0.8778220413032648\n",
            "log probabilities: [-2.8454322814941406, -2.559126138687134, -2.907104730606079]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0027705542743206024\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nTo download a script to perform this calculation click here.\\n\\nClick here to view the spreadsheet table for download.\\n\\n(Click here for an Excel template that includes the Excel program for Windows. If you', 'thank you for your help!\\n\\n– Janssens [official homepage]\\n\\n[Thanks to /u/N0tN0_s for correcting these translations]', 'thank you for your help!']\n",
            "rewards: [1.8706733857893205, 2.095840671189849, 2.0668589826765724], baseline: 2.011124346551914\n",
            "log probabilities: [-2.6223177909851074, -2.91343355178833, -2.5483052730560303]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.0026946854777634144\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\nAs someone that gets most of his credit due to his dedication to the cause of the planet, I have been involved in many missions to support the cause on various levels of the Earth. I've never had anyone\", \"i really appreciate your kindness.\\n\\nBut there's really never a day you can't make dinner for your loved one that day anyway - it doesn't matter. We'll just leave it at that for now.\\n\\nThe second best thing:\", \"i really appreciate your kindness. Do you have any special wishes for her?\\n\\n[Mimicked by some of her fans on Twitter.] [Mimicked by some of her fans on Twitter.] You sure don't mind our little joke,\"]\n",
            "rewards: [1.9620913401244005, 2.1898843641796546, 2.1777629515707706], baseline: 2.1099128852916085\n",
            "log probabilities: [-3.0134124755859375, -2.9588654041290283, -2.598257064819336]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.002856845036149025\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?[/b] I\\'ll give you my time in creating it. I will make use of all of your generosity for the betterment of all.\"\\n\\nAfter his final performance with the Red Sox, Johnson said', 'could you please assist me with this?', 'could you please assist me with this? Thank you.\" \"Why don\\'t you go down to my room?\" *shrug* \"Yeah, um, it\\'ll be cool to go, I guess. Do you have a room nearby.\" >I']\n",
            "rewards: [2.1296931902060314, 1.836295977507978, 1.8192256504948843], baseline: 1.9284049394029645\n",
            "log probabilities: [-2.950019598007202, -3.1072323322296143, -2.8005764484405518]\n",
            "computed advantage: 0.001\n",
            "computed loss: 0.00295260944403708\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.002840976510196924\n",
            "new generated completion: once upon a time, he might take it upon himself to put the burden on his brother's shoulders. That was a lesson that the king had failed to take into account, as well as a lesson that he was capable of taking up the matter for\n",
            "new reward score: -1.0995761423599568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"once upon a time, he might take it upon himself to put the burden on his brother's shoulders. That was a lesson that the king had failed to take into account, as well as a lesson that he was capable of taking up the matter for\",\n",
              " -1.0995761423599568)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 11: evaluating the updated model\n",
        "def evaluate_model(prompt):\n",
        "    new_completion = generate_multiple_completions(prompt, num_samples=1)[0]\n",
        "    new_reward = reward_function(new_completion)\n",
        "    print(\"new generated completion:\", new_completion)\n",
        "    print(\"new reward score:\", new_reward)\n",
        "    return new_completion, new_reward\n",
        "\n",
        "# example usage\n",
        "evaluate_model(\"once upon a time,\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q06BPeuu5plX",
        "outputId": "d420e425-b946-4a75-cbfb-468ef49757fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new generated completion: once upon a time, I didn't know that I existed.\" When his father asked if he was still alive he replied, \"Maybe someday. But now, I can't look at it anymore.\" Even though he has no interest in his life,\n",
            "new reward score: 0.48848573721266714\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('once upon a time, I didn\\'t know that I existed.\" When his father asked if he was still alive he replied, \"Maybe someday. But now, I can\\'t look at it anymore.\" Even though he has no interest in his life,',\n",
              " 0.48848573721266714)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refining the reward system further\n",
        "I will:\n",
        "\n",
        "introduce a negative word penalty\n",
        "– penalize words like \"no\", \"can't\", \"won't\", \"not\", \"bad\", which contribute to negative sentiment.\n",
        "increase reward gap for polite vs. non-polite responses\n",
        "– to encourage stronger preference learning.\n",
        "strengthen the reinforcement learning signal\n",
        " – by adjusting the advantage calculation.\n",
        "i'll update the notebook now. then, run the updated version and check the evaluation output again.\n",
        "\n",
        "\n",
        "\n",
        "# # Introduce a penalty:\n",
        "\n",
        " for negative words like \"no\", \"can't\", \"not\", which contribute to negative sentiment.\n",
        "increase the reward gap to further encourage positive completions.\n",
        "amplify the reinforcement signal by tweaking the advantage function."
      ],
      "metadata": {
        "id": "CygxZmh9BVTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: install required libraries\n",
        "# before we begin, install the necessary libraries if they are not already installed.\n",
        "#!pip install transformers torch textblob\n",
        "\n",
        "# step 2: import required modules\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from textblob import TextBlob\n",
        "\n",
        "# step 3: load pretrained model and tokenizer\n",
        "# we use a pretrained gpt-2 model for our rlhf experiment.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# define pad token explicitly to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# step 4: generate multiple sample completions\n",
        "def generate_multiple_completions(prompt, num_samples=3, max_length=50):\n",
        "    completions = []\n",
        "    for _ in range(num_samples):\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "        output = policy_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=1.2  # encourage exploration\n",
        "        )\n",
        "        completions.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "    return completions\n",
        "\n",
        "# step 5: refining the reward function with stronger penalization\n",
        "def reward_function(text):\n",
        "    \"\"\"\n",
        "    a refined reward function that assigns a progressive score based on politeness and sentiment intensity,\n",
        "    with a stronger penalty for negative words.\n",
        "    \"\"\"\n",
        "    polite_phrases = [\"thank you\", \"please\", \"kindly\", \"appreciate\", \"grateful\", \"respect\", \"much obliged\", \"thanks\"]\n",
        "    negative_words = [\"no\", \"not\", \"never\", \"can't\", \"won't\", \"bad\", \"sad\", \"problem\", \"worse\"]\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "\n",
        "    # progressive scoring\n",
        "    if any(phrase in text.lower() for phrase in polite_phrases):\n",
        "        reward = 3.0  # increased reward for strong politeness\n",
        "    elif sentiment > 0.5:\n",
        "        reward = 2.0  # strong positive sentiment\n",
        "    elif sentiment > 0.2:\n",
        "        reward = 1.5  # moderate positive sentiment\n",
        "    elif sentiment > 0.0:\n",
        "        reward = 1.0  # slight positive sentiment\n",
        "    elif sentiment < -0.5:\n",
        "        reward = -3.0  # increased penalty for highly negative sentiment\n",
        "    else:\n",
        "        reward = -1.5  # default penalty for neutral or slightly negative sentiment\n",
        "\n",
        "    # additional penalty for explicit negative words\n",
        "    if any(word in text.lower() for word in negative_words):\n",
        "        reward -= 1.0\n",
        "\n",
        "    # add small noise to ensure variation in rewards\n",
        "    return reward + random.uniform(-0.2, 0.2)\n",
        "\n",
        "# step 6: computing log probabilities for the generated text\n",
        "def compute_log_probs(model, input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    log_probs = -outputs.loss.item()\n",
        "    return log_probs\n",
        "\n",
        "# step 7: improving advantage computation to amplify differences\n",
        "def compute_advantage(reward, reward_baseline=0.0, epsilon=1e-6):\n",
        "    advantage = (reward - reward_baseline) / (abs(reward_baseline) + epsilon)\n",
        "    return max(advantage, 1e-2)  # prevent zero advantage values\n",
        "\n",
        "# step 8: computing the policy gradient loss\n",
        "def compute_policy_gradient_loss(log_prob, advantage):\n",
        "    return torch.tensor(-log_prob * advantage, requires_grad=True)\n",
        "\n",
        "# step 9: updating the model using gradient descent\n",
        "def update_model(model, loss, learning_rate=1e-5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# step 10: training loop with debugging outputs\n",
        "def train_model(model, prompts, iterations=30, num_samples=3):\n",
        "    \"\"\"\n",
        "    trains the model on multiple prompts and multiple generated responses per prompt using progressive rewards and normalized advantage.\n",
        "    \"\"\"\n",
        "    for i in range(iterations):\n",
        "        print(f\"iteration {i+1}:\")\n",
        "        total_loss = 0\n",
        "        reward_baseline = 0  # baseline initialized per iteration\n",
        "        for prompt in prompts:\n",
        "            completions = generate_multiple_completions(prompt, num_samples)\n",
        "            rewards = [reward_function(c) for c in completions]\n",
        "            reward_baseline = sum(rewards) / len(rewards)  # update baseline dynamically\n",
        "            log_probs = [compute_log_probs(model, c) for c in completions]\n",
        "            avg_reward = sum(rewards) / len(rewards)\n",
        "            avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "            advantage = compute_advantage(avg_reward, reward_baseline)\n",
        "            loss = compute_policy_gradient_loss(avg_log_prob, advantage)\n",
        "            update_model(model, loss)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # debugging outputs\n",
        "            print(f\"prompt: {prompt}\")\n",
        "            print(f\"generated completions: {completions}\")\n",
        "            print(f\"rewards: {rewards}, baseline: {reward_baseline}\")\n",
        "            print(f\"log probabilities: {log_probs}\")\n",
        "            print(f\"computed advantage: {advantage}\")\n",
        "            print(f\"computed loss: {loss.item()}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        print(f\"updated model. average loss: {total_loss / len(prompts)}\")\n",
        "\n",
        "# example usage\n",
        "training_prompts = [\n",
        "    \"once upon a time,\",\n",
        "    \"hello, how are you doing today?\",\n",
        "    \"thank you for your help!\",\n",
        "    \"i really appreciate your kindness.\",\n",
        "    \"could you please assist me with this?\"\n",
        "]\n",
        "train_model(policy_model, training_prompts, iterations=30)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1ZjMA_WAiQB",
        "outputId": "d9598bbf-e219-4092-c7cd-3e7d695996c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, it was an opportunity to have some fun, a good time and a decent food, to let everyone know they were safe and secure.\"\\n\\nOn how she got to where she is today and how that will happen: \"', 'once upon a time, when human civilization seemed dead to their minds. And where were all those brave souls, for all the terrible crimes and crimes of civilization, before they all perished and found themselves among us?\\n\\nAnd where are these dark souls', \"once upon a time, we can learn about our own. And I think some will ask:\\n\\nWhat about what is really good and how do we know we're good?\\n\\nWhat does that have to do with who we are and how\"]\n",
            "rewards: [0.4491432386347946, -1.3670078733520508, 0.8752388014605408], baseline: -0.014208611085571773\n",
            "log probabilities: [-2.7610368728637695, -3.281049966812134, -2.587794780731201]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028766272589564323\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? \"You\\'re on time, man?\" I asked in a little low voice. He smiled again. \"Ah, yeah, well I\\'ve got to go back to work.\" He had gone back to the office', \"hello, how are you doing today?\\n\\n\\nTristan:\\n\\nHey guys. What time of day is that? We'll see.\\n\\n\\n[laughs]\\n\\n\\nAOL:\\n\\nYou know what it's like? There\", 'hello, how are you doing today?\\n\\nMy dad has done it several times. Every week. Every couple of weeks. I like to do things like that. I was always worried. It can take a bit of time in my life.']\n",
            "rewards: [1.146478440067077, -2.5650982860898695, -1.3248058857339393], baseline: -0.9144752439189107\n",
            "log probabilities: [-2.6314504146575928, -2.412431478500366, -2.636981248855591]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025602877140045166\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! If you're having problems seeing links that don't work in place, be sure to check that this FAQ doesn't contain unnecessary jargon and mistakes that could make your issues appear more confusing!\\n\\n\\nThanks for reading.\\n\\n\", \"thank you for your help!\\n\\nPlease keep the same email address but this time as a friend or a colleague if using it online - they're in sync so email/google+ is the preferred route!\", \"thank you for your help!\\n\\nAs I mentioned, for now, this is a way for me to let everyone know the end of the chapter, and give a new beginning to the rest of the story. If you've come to this journey\"]\n",
            "rewards: [2.04723846495962, 2.9006436842659182, 2.0582526806210932], baseline: 2.3353782766155438\n",
            "log probabilities: [-2.8018550872802734, -3.391265392303467, -2.3958921432495117]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02863004244863987\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\nLest you think me just going ahead and killing her is something I like, well, not quite this far, just for you guys, I'm writing this for two reasons: to show what an honor you\", 'i really appreciate your kindness.', 'i really appreciate your kindness. It is so refreshing to hear from my friends and I on such short notice. Thanks again for your service and assistance! I will be continuing on the internet when I are in the field and hoping the situation is sorted out']\n",
            "rewards: [2.1919607764484277, 3.106462333571627, 2.0564169310264284], baseline: 2.451613347015494\n",
            "log probabilities: [-3.3219170570373535, -4.8637542724609375, -3.2005088329315186]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.0379539355635643\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? And if so, how would it go with your daughter?\"\\n\\nSealer, who had been told she was at her best if not best and did not like to show off a much slicker face', 'could you please assist me with this?\\n\\nA: My heart is filled with gratitude.\\n\\nQ1: There were some issues which I did not address that happened while I did the calculations (examining all sources) and could not', 'could you please assist me with this? [14:19:04] <Elo_Knight> yep I asked [14:19:04] <+winkledrinker> so how does one make this sound? [14:']\n",
            "rewards: [2.180449850113208, 1.8445912875813062, 2.857900173938609], baseline: 2.2943137705443744\n",
            "log probabilities: [-3.050917387008667, -2.978675603866577, -2.632584810256958]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028873926028609276\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029965410754084588\n",
            "iteration 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, and have experienced the pleasure of that which I had formerly experienced?\"\\n\\nIn the end, the same spirit that is so deeply buried in our hearts as we are. We, too, hold that every act of life,', 'once upon a time, that might be considered just a coincidence.\\n\\nOf the 10th most famous men in history, 14 are all Americans from California. Of the other two, 14 are from the Midwest in what may be the longest serving period', 'once upon a time, it was a question. The question was: is something bigger on me than my own life? A question which we can ask, what can only be said by God.\\n\\nHere is a question for Jesus Christ to answer']\n",
            "rewards: [1.3090872065820367, 1.4220150840895924, 1.0601883302234105], baseline: 1.2637635402983465\n",
            "log probabilities: [-3.11191725730896, -3.220947504043579, -3.1761395931243896]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031696680933237076\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nYeah, really! I had pretty good health for two years before it all started, and my doctor told me to let it sit and I didn't want to try any of those, no matter how\", \"hello, how are you doing today?\\n\\nLiz: It's been kind of amazing. My kids are doing very well now, so it's been so far. I think the girls in the second grade are doing great right now!\\n\", 'hello, how are you doing today? Thank you!\"\\n\\nThat got the room jolted.\\n\\n\"There\\'s still no word on it yet. The whole class went to bed together! Ah, now we can put this right in']\n",
            "rewards: [0.3455866636120617, 0.3098526266859151, 2.070588577036805], baseline: 0.9086759557782607\n",
            "log probabilities: [-2.607802152633667, -2.437387466430664, -3.084247589111328]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027098124846816063\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nThank you for reading.', 'thank you for your help! Please read our Privacy Policy first before visiting this site or clicking any link! Thank you for your help!\\n\\nCult of Agony & Joy\\n\\nby Eryn K.\\n\\nAuthor of the article \"', \"thank you for your help!\\n\\nThere's actually only one way to save these books! Please use them for personal use or share them under some social networks! If it breaks your use policy then please delete them for safekeeping! (you may\"]\n",
            "rewards: [3.029999716460533, 2.952823173585145, 3.114934170347646], baseline: 3.0325856867977747\n",
            "log probabilities: [-1.8147767782211304, -2.669477701187134, -3.154087543487549]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025461139157414436\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I\\'ll see you on the road. \" – Dr Librarian\\n\\n\\nThis character is sooooooo strong!\\n\\n\" – Dr Librarian This monster does NOT like the only one on the planet with the greatest', 'i really appreciate your kindness. If you are in danger, give me some assistance!\"\\n\\nLia was not expecting anything. Although she had been able to save the rest of Lu Lu in the past month, the last time they returned to the', 'i really appreciate your kindness. I hope my dad and I can get them both a drink. Good luck, all in all. (from Facebook photo photo):']\n",
            "rewards: [1.944605976615755, 2.178495021338947, 2.9202445816290803], baseline: 2.3477818598612608\n",
            "log probabilities: [-3.37654709815979, -3.1754326820373535, -3.667794942855835]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03406591713428497\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? I\\'ve found it a wonderful pleasure,\" she continued. \"I\\'ve asked a question on that, or more. I would love to find out whether this was the proper one to ask someone else.\"\\n\\n', 'could you please assist me with this?\\n\\nAs a general comment on the current situation on the international banking industry, is there any way you can give feedback or suggestion to other bankers if any problems arise for you?\\n\\nIf not, please', 'could you please assist me with this? The guy\\'s in trouble. We have to find a way out, and that won\\'t happen without help from the citizens.\" He shrugged the matter off, shaking his head a bit.\\n\\n\\nRuby nodded.']\n",
            "rewards: [2.932189650224493, 2.1801653192434447, 1.8329281081851931], baseline: 2.3150943592177105\n",
            "log probabilities: [-3.3186137676239014, -2.940209150314331, -2.852663516998291]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030371621251106262\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02973869666457176\n",
            "iteration 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, where this is done, you will receive rewards upon that journey. Then you are able to take any time or time you want. This system has been very clear on this. They've given you things that you could do before\", \"once upon a time, an old and frail man had a vision, that the gods had cast off their hands with the hand to the head, wherewith he'd seen the gods; for when his eyes caught on to the eye, he'd said\", 'once upon a time, a number of scholars who study Islam\\'s teachings, and in particular those who were members of the Sufi elite, have said that Allah was more likely to see people of their faith as \"bad\", whereas if a few of']\n",
            "rewards: [1.3712295521228277, 1.1004112241182784, -2.4696038545883385], baseline: 0.0006789738842557883\n",
            "log probabilities: [-3.376709222793579, -3.5329549312591553, -3.183746337890625]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03364470228552818\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\n\\nWladimir: I'm still looking forward to playing so I've been doing more homework, I suppose. I started a new life yesterday, but I need something to do. We still need to\", \"hello, how are you doing today?\\n\\nOh I'm up here doing everything myself!\\n\\nYeah really! It's great just chatting about stuff you have.\\n\\nI read you liked to hang out.\\n\\nI read your blog\", 'hello, how are you doing today? How are you feeling?\"\\n\\nIn the time it takes to tell someone they\\'re going to give your life, you only need 12 words to describe what your life is, at the right time: \"Wow']\n",
            "rewards: [1.5776606411948282, 1.8861551190107262, 1.0746094534189061], baseline: 1.512808404541487\n",
            "log probabilities: [-2.9278268814086914, -2.952174186706543, -2.800241231918335]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028934141620993614\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! If you'd like to contribute to help further the development (just contact me first ) , or just email me through my contact email here - @souvenir_lm , you will have plenty of time to\", 'thank you for your help!\\n\\nFor more, go to the following websites:', 'thank you for your help!\\n\\nP.S. Please note:']\n",
            "rewards: [3.121289301054779, 3.0036081101528085, 1.8210517807509066], baseline: 2.6486497306528314\n",
            "log probabilities: [-3.1694276332855225, -2.396754264831543, -2.0793192386627197]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025485003367066383\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I have never heard anything negative ever, so I really appreciate your gift.\"\\n\\nOn this day, the two were joined by an equally powerful member of the Jewish population. They also were accompanied on this fateful journey,', 'i really appreciate your kindness. In other forums on the subject of \"babysitter, you must remember that it is a scam of a lifetime and will cost you money. You can only lose if they are really successful and you are the one', \"i really appreciate your kindness. That we can help you find this beautiful home if they want to.\\n\\nA few things. Please note that my son's condition is not currently causing me anything in his life.\"]\n",
            "rewards: [1.8837006342227154, 3.149212395400158, 2.0901793012705743], baseline: 2.3743641102978157\n",
            "log probabilities: [-3.375978946685791, -3.6066770553588867, -3.382842540740967]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03455166146159172\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? You want me to provide these figures to you, but you must help yourself? Can you do it without assistance, because as well I suppose you would be content to just accept this? \" You said this.', 'could you please assist me with this?\\n\\n\\n[12/1/2016 1:22:08 PM] Izzy (@iglvzx): Yes thanks.\\n\\n\\n[12/1/2016 1:22:15 PM] Ian Cheong:', 'could you please assist me with this? Or do I need you to write me an email address when you can?\" The reply that followed could be viewed on GitHub, or you could use the following URL: \"Contact: \"\\n\\nWith all the']\n",
            "rewards: [2.908653540379733, 3.0469203378706644, 3.18518365799864], baseline: 3.046919178749679\n",
            "log probabilities: [-3.300997257232666, -1.615893006324768, -2.8604981899261475]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025924628600478172\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029708027467131616\n",
            "iteration 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, when people understood and believed what they were told.\\n\\n\\nHe goes from being the best actor to working as the main character on some pretty remarkable characters in American comedy with '60s drama and '70s reality series.\", 'once upon a time, I got back to my business and became a good human being,\" Ms. Miller wrote in a personal post. \"Since then, my personal life has changed.\" Mr. Murphy also took on multiple personal matters, including issues regarding', 'once upon a time, we were told to leave him at our side to look after my son and myself. That was my fault, that was his fault. So I ran. The day ended. Then went. The morning came with a sound like']\n",
            "rewards: [1.5836134720777397, 1.1252186214602231, 1.306542656212063], baseline: 1.3384582499166753\n",
            "log probabilities: [-3.3471126556396484, -3.100708246231079, -3.3065783977508545]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03251466527581215\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nHe had two strokes, it was like that! And he had a lot on the card but the second he came in, he wasn't happy with what he went through. It was kind of an\", 'hello, how are you doing today? \"My whole day is so great.\" She smiles again with a wry smile as he leans in closer. \"Oh my! Oh no, you need to stop now!\" He pulls his hair behind his head', \"hello, how are you doing today?\\n\\nKris Schleicher: I'm being positive here to get back on track and hopefully I'll see things through for next week but this whole week is the first week after the last thing that\"]\n",
            "rewards: [1.312859802798521, 0.6995219954806395, 1.1540147579711992], baseline: 1.0554655187501198\n",
            "log probabilities: [-2.8802366256713867, -2.903029680252075, -3.02702260017395]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029367629438638687\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!', 'thank you for your help!', 'thank you for your help!\\n\\n(Thanks! I love hearing from you!)\\n\\nYou can also see our latest and biggest updates on our Dev Blog here']\n",
            "rewards: [3.0783242808591105, 3.185964832758277, 3.1563666851787775], baseline: 3.1402185995987217\n",
            "log probabilities: [-2.5483052730560303, -2.5483052730560303, -2.550130844116211]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025489138439297676\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. - November 17, 2013a lot more cool\\n\\nReviewer: davein_dog - favorite favorite favorite favorite favorite - December 26, 2012\\n\\nSubject: First time listening and listening to the whole thing, to', 'i really appreciate your kindness.\\n\\nIf you want more info and additional info on these changes please go to this link : http://i.imgur.com/f3QWZ9l.png This update should fix many of the old', 'i really appreciate your kindness. Thank you for your attention. I feel so blessed to have been able to come on board with you!\\n\\nThanks for the great questions. And since there have been such great things in the last few years...I']\n",
            "rewards: [2.1474228301487477, 2.805748832041516, 2.8652802258566803], baseline: 2.6061506293489813\n",
            "log probabilities: [-2.6189820766448975, -2.8507561683654785, -2.6950833797454834]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027216071262955666\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? Can you do an assist? Please tell me. How did I get to so many of the buildings? I should be glad you asked, but I have no way of contacting you. Don't leave. Your\", 'could you please assist me with this? Your efforts are valued by me,\" the girl repeated through an exasperated tone.\\n\\nRumble reached out and reached for my arm as his movements became uncomfortable and I gave a groan of relief, but', 'could you please assist me with this?[/font][/center][center]\\n\\nYou are welcome, or so they tell me.\\n\\n(I have come to my senses.)\\n\\nThey tell me to stay with them. I guess this']\n",
            "rewards: [1.8957185606129419, 2.910124777544719, 3.1781462586743214], baseline: 2.6613298656106608\n",
            "log probabilities: [-2.9480652809143066, -3.0919158458709717, -2.7124266624450684]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029174692928791046\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.028752439469099045\n",
            "iteration 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, he must be driven at the rate that I have told you so far back. For the past few hours, he has been keeping, at home, a few thousand copies of me. I feel that, when the time comes', 'once upon a time, for a whole world, for the entire world.\" When it\\'s not the most famous scene on Earth, in fact, there\\'s nothing quite like it. The film does not try to be funny. A movie about a group', \"once upon a time, when the nation is full of fear and dread.\\n\\nSo I said that as much or less of a crime is now, it is for our children's sake to keep you from the very act of carrying on that action\"]\n",
            "rewards: [-1.3270312679534602, 0.5628610423519051, -0.1631220509344886], baseline: -0.3090974255120145\n",
            "log probabilities: [-3.2194535732269287, -3.007845163345337, -3.3444161415100098]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03190571442246437\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? Where is everything on your back? Where is the money? It's going to take a little over four weeks and I haven't been able to go through the process of filing them. You hear me a lot\", \"hello, how are you doing today?\\n\\nMy mind is very clear. I've spent more than 20 weeks out in my car, off and on and it was just like trying to think how the heck I will handle tomorrow. I have the\", 'hello, how are you doing today?\\n\\n\"Yeah, I\\'m actually much better,\" he says.\\n\\nHe\\'s right on time. He starts working late.\\n\\nSandra takes her time with the baby after it\\'s fully ready']\n",
            "rewards: [0.9940436570216601, 1.0605226564593098, 1.1373434819687023], baseline: 1.0639699318165574\n",
            "log probabilities: [-2.8110086917877197, -3.0258984565734863, -2.797239303588867]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028780488297343254\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! <em>\\n\\nThis site allows you to create, modify, and redistribute material on this site under the terms of the Open Source License.\\n\\nBy continuing to use this site you indicate that you have read', \"thank you for your help! A lot of people ask if we're not a mod of LordHive's own. Actually, it's only a single bug, and I have to say they're pretty fun to track down as we speak on this\", 'thank you for your help!']\n",
            "rewards: [2.8981975408748077, 1.825349112801684, 2.9567490570851724], baseline: 2.560098570253888\n",
            "log probabilities: [-1.8940563201904297, -3.1755268573760986, -2.5483052730560303]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025392960757017136\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. Maybe it's because I'm the biggest asshole out there who will do a better job, but please forgive her for leaving you with her.\\n\\n[She walks toward us, grinning.]\\n\\n\\nRENEO\\n\\n\", 'i really appreciate your kindness.\\n\\nYou were lucky in not being in that place or your hair was so long. I hope we can move on.', 'i really appreciate your kindness. They\\'re still very polite around my daughter. They\\'re probably okay on certain things. I just think about it a lot,\" Pappi said.\\n\\nPAPPI\\n\\nThe city\\'s mayor and CTV']\n",
            "rewards: [3.1465620908529233, 1.852805245668033, 2.8351703174442946], baseline: 2.6115125513217503\n",
            "log probabilities: [-3.3947956562042236, -3.305088758468628, -3.537396192550659]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03412427008152008\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? It\\'s something they use to give their kids, or something like it. \"', 'could you please assist me with this? I should be ready shortly.\"\\n\\nWhile we were waiting for the ambulance van to arrive the three other patients and I arrived at another hospital on the outskirts of Paris. A police officer arrived for some questioning,', 'could you please assist me with this? Is my heart dying out here?\"']\n",
            "rewards: [3.1662865467056194, 2.1477697442494494, 2.980019772083928], baseline: 2.764692021012999\n",
            "log probabilities: [-3.380840539932251, -2.9953432083129883, -3.4849660396575928]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.032870497554540634\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.030614786222577094\n",
            "iteration 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, when the government's ability to conduct business was being severely weakened in the United States and the world by a series of major global trade crises, many felt they had just escaped the horrors of war in which they had never expected.\", 'once upon a time, he would have used the sword and spear. However, once it returned, the prince would no longer be able to use the sword.\\n\\n\"He is quite dangerous.\"\\n\\nBut his expression changed a little.\\n', \"once upon a time, though, the word would seem to imply such a shift from human behavior towards the gods, one that may at least be in some way analogous to the divine, where we don't always get the same reactions to events or events\"]\n",
            "rewards: [0.12251431910423005, -2.52566799292452, -1.3693684624364346], baseline: -1.2575073787522415\n",
            "log probabilities: [-2.863801956176758, -2.658912420272827, -3.3257498741149902]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029494881629943848\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Why are you doing this?\" He paused a second. \"Don\\'t take my word for it!\" said a voice coming from outside, \"you don\\'t belong here, and I want your family out of it', \"hello, how are you doing today? When was the last time you went skiing?\\n\\nI go to New York City every summer as part of my skiing tour around the country where it's free and it's all snow-free. For my\", \"hello, how are you doing today?\\n\\nAdvertisement\\n\\n\\nI can't tell, but for the most part, it seems the story isn't anything specific. We did hear a bunch of people, the audience members included, complaining about not having\"]\n",
            "rewards: [-1.6394731994674725, -0.03871688519685046, 0.6473311336915482], baseline: -0.3436196503242583\n",
            "log probabilities: [-2.6122689247131348, -2.5437216758728027, -2.890225410461426]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026820719242095947\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nIt could have been better but you have always been here! I think this would really be a nice option if I had a better idea! The only downside is that a lot of me is a techy person', 'thank you for your help!\\n\\nSo what do we have to do now?\\n\\nI am doing some pretty interesting work here. This morning it has been re-posted here (click any of the links to visit to view it here).', 'thank you for your help!\\n\\nThis site is free and open source. You have permission to republish this post under a Creative Commons license with attribution to the author and TrueActivist.com\\n\\nOn June 12th, I got']\n",
            "rewards: [2.919676208559709, 2.154347776867384, 2.8670119617313086], baseline: 2.6470119823861338\n",
            "log probabilities: [-2.9644782543182373, -2.6777780055999756, -1.130111575126648]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02257455885410309\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. Do you also think that, over time, other women will do a similar thing?\\n\\nI don't see this being discussed among any feminist bloggers. And I do want to say that we've moved on in this\", \"i really appreciate your kindness.\\n\\n\\nTo clarify, we'll take one step at a time here: the community must also make a commitment of a future release. I want to offer my support for people to try these changes but we will miss out\", 'i really appreciate your kindness. If you want to stay quiet and to avoid making it worse for him and others we may not know you at this point.\"\\n\\n\"I need a lawyer.\" Harry nodded at this and then added, \"I would']\n",
            "rewards: [3.004491772116675, 3.1409599897298546, 1.935392311157643], baseline: 2.693614691001391\n",
            "log probabilities: [-3.0524630546569824, -3.630432367324829, -3.133298873901367]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.032720647752285004\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? How do you know I have you?\"\\n\\n\"I\\'m from Laval, a family named after mine,\" she replied. Her face lit up, she felt some warm air.\\n\\n\"You mean', \"could you please assist me with this? (pause) Please let me know your name so I can get a feel for you. (pause)...I'd love to go in with you...(deeper whisper)...your name is B-Boy?\", 'could you please assist me with this?\\n\\n\\nI didn\\'t ask you to come to my family at this time, there wasn\\'t even a car being kept inside the cabin at all so what\\'s the matter?\"\\n\\n\\n\"We can only assume']\n",
            "rewards: [2.125455473193432, 1.881506665724221, 2.9522013800362608], baseline: 2.319721172984638\n",
            "log probabilities: [-2.7827932834625244, -2.625458002090454, -2.876866340637207]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027617059648036957\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02784557342529297\n",
            "iteration 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, but even now my feelings are like 'Ah no! Oh no! You should die! I shouldn't give it to you,' and I don't want to die to them. I like to die like a mother and have\", \"once upon a time, I remember, and in the course of my lifetime I'm certain I've heard from many people that are still on your staff and who are grateful to be with you, and who I think have seen the opportunity given by your\", 'once upon a time, and when I have been informed I believe I cannot remain alone,\" Obama told a crowd gathered at the White House for the first time Saturday. \"The issue that we have, as I have believed for a good chunk of my']\n",
            "rewards: [-2.492630146972272, 2.9717191469137845, 0.4530002887648686], baseline: 0.31069642956879373\n",
            "log probabilities: [-3.0458028316497803, -3.006864309310913, -2.8301520347595215]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029609397053718567\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nHe went and got a cup of coffee. He took his laptop and the other kids and they all went to his office. All day. Everybody went home. Everything he did was on that plane.', 'hello, how are you doing today? Because of your time on your family vacation in Turkey, we have some news for you. We are still waiting on your phone.\\n\\nWe are working through what we can do for you. Your phone may', 'hello, how are you doing today?\\n\\nAdvertisement\\n\\nIn fact, he was really, really good today. He talked about everything from the fact that he was playing a great, young team (which now appears to be in the bottom five']\n",
            "rewards: [-1.4202380469015092, -1.6593539642752295, 0.83192165803448], baseline: -0.7492234510474195\n",
            "log probabilities: [-2.8110504150390625, -2.658205270767212, -2.6619269847869873]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02710394188761711\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!', \"thank you for your help! We'll take care of it quickly. The rest will go in a day or two. (1-11) [EDIT] So much for your comments. First, you should update your account's details, but do\", \"thank you for your help!\\n\\n*This project was designed to run on V2, which may include support for OpenCV for 3D modelling. To ensure proper portability the project has to be re-released. As a result I've\"]\n",
            "rewards: [2.9710813186270197, 3.0522730488730505, 3.037654941595835], baseline: 3.0203364363653016\n",
            "log probabilities: [-2.5483052730560303, -3.147465229034424, -2.9682648181915283]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02888011746108532\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\n(2) Please give any gift from the previous address you gave during this process.', 'i really appreciate your kindness. So, this is where things get confusing.\\n\\nThis is the basic code for registering the URL where a request for our project would look for a local URL address. You can find it in my Github gist for a', 'i really appreciate your kindness. If you\\'ve got friends looking to help, please leave them a note. I\\'ll get back to them as soon as I\\'m up and about,\" Mr Shire said.\\n\\n\"Do you get on here in']\n",
            "rewards: [2.8389713578639446, 3.1534962833184736, 2.001620028463479], baseline: 2.664695889881966\n",
            "log probabilities: [-3.716844320297241, -3.3967623710632324, -2.8771371841430664]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03330247849225998\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? If you have an idea as to what, please take it along at once and let me know. Thank you very much :)\\n\\nI would like to thank Terence for taking the time to review some of', \"could you please assist me with this?\\n\\nPlease don't hesitate to read the post. What you might find helpful is very simple, so just pick out any item you might like.\\n\\nHave a nice day!\", 'could you please assist me with this? I think he must have left you, for my sake. Please, I will show myself to you, even now.\"\\n\\nAt a small groan, Ye Xiwen had to say it, saying']\n",
            "rewards: [2.079773227342573, 2.868234665092717, 1.92165602730662], baseline: 2.2898879732473034\n",
            "log probabilities: [-2.4399325847625732, -2.773859739303589, -3.168767213821411]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027941865846514702\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029367560148239137\n",
            "iteration 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time,\\n\\nAs we see the darkness and despair of my youth,\\n\\nWhat will I be made to say or do?\\n\\n\\n*\\n\\n\\n\"My brother, the light you set forth in me will be lost,', 'once upon a time, when the world could be completely ignored.\" The film\\'s first trailer appeared in 2013, showing off his upcoming album and a picture from the time, released earlier that year. That year, the album was released, and while that', 'once upon a time, she had been forced by the gods to sacrifice her children in the most ancient rites and to perform them to her son, Theta. From this moment, at that time also before death, it should be clear to every human']\n",
            "rewards: [1.6578459518239803, 0.09764014341353178, 1.114952334712197], baseline: 0.9568128099832363\n",
            "log probabilities: [-3.053276777267456, -3.161039352416992, -3.282555103302002]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03165623918175697\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Because this is why there is still time. Your family and your friends will thank you on that day for that. They know that day will soon come...\\n\\nHow has this been?\\n\\nIt has', \"hello, how are you doing today? A few things of note: We've had a bunch of bad weather this week—which is good because there are better things to do in the Middle East than to walk and go out to dinner and get a\", \"hello, how are you doing today?\\n\\nI can't be here tomorrow today because I'm so far from home, so where I am, I don't know. I have no idea if my family are still with me because they're in\"]\n",
            "rewards: [1.801395141505261, 0.02254594976878127, -0.1289518977715719], baseline: 0.5649963978341567\n",
            "log probabilities: [-2.883160352706909, -2.7389445304870605, -2.4416286945343018]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02687911130487919\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nFollow the progress in our next post to see how many items are in-game:\\n\\n* 5 items, including the \"Handy\" quest (The Little One\\'s quest - The Little One\\'s quest', 'thank you for your help!\\n\\n1. I have been working on a great game for a long time. So much effort, dedication and creativity, it must have been time consuming to build out it a week on the way to completion. I', \"thank you for your help! Thank you so much for reading. We should probably finish the final installment by next month, and then post some additional articles and details along the way! I'm sure there'll be dozens, maybe hundreds, before I get\"]\n",
            "rewards: [3.093705629958503, 3.076007450750878, 3.078543532707419], baseline: 3.0827522044722664\n",
            "log probabilities: [-3.0549139976501465, -2.718876838684082, -2.933255672454834]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029023488983511925\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. I truly respect you. He looked really happy.\\n\\nHe's not sure what kind of girl you are for. I'll probably never be around anymore because I still am so sad and disappointed and just wants something from\", 'i really appreciate your kindness. Just because she asked a question, I just want someone to show my gratitude. I don\\'t think the government or the government department has the manpower or resources to do that anymore.\" And I can only imagine what other other', 'i really appreciate your kindness. I\\'m in favor of people who are being bullied to the point where they can\\'t even feel the shame.\"']\n",
            "rewards: [1.9503028674937388, 3.199799428600192, 2.0467416916734775], baseline: 2.398947995922469\n",
            "log probabilities: [-3.329232931137085, -3.3471853733062744, -3.1458170413970947]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03274078294634819\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? In addition, please help us, help out with research and analysis to produce this video.\\n\\nAnd you may give us money on our Patreon page to provide future access to the latest, exciting tech news,', 'could you please assist me with this?', \"could you please assist me with this?\\n\\n\\nThanks. I'll see if that is okay.\\n\\n\\nIt's like we know something from that movie is true but we didn't know who it is. It's not that there is something to\"]\n",
            "rewards: [2.935956105711655, 2.9023598940134017, 2.1982478848033367], baseline: 2.678854628176131\n",
            "log probabilities: [-3.306962728500366, -3.1072323322296143, -2.8599941730499268]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03091396391391754\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.030242717266082762\n",
            "iteration 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, I realized that every single bit of his actions were absolutely nothing more than the result of a fantasy. If he could turn his mind into someone he would feel that there was nothing other than pure bliss in all the things that are', 'once upon a time, with people trying to figure out how to get them more money than just the government of the day.\"\\n\\nIn October 2016, he says the Republican Party is still holding out hope for an anti-gay referendum, but that', 'once upon a time, you can spend some of your money to build and support more schools, while keeping your tax dollars out of the hands of those that lack the skills.\"\\n\\nWhile in his office, Dr. Johnson said the school system needs']\n",
            "rewards: [0.022757042741791272, 1.3695610928929756, 1.4964886838520601], baseline: 0.962935606495609\n",
            "log probabilities: [-3.1340222358703613, -2.933529853820801, -2.8830952644348145]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029835490509867668\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? Are you enjoying your vacation?\\n\\nCristiano Martinez\\n\\nThis was the one year I've gone out. I did everything I knew I would. The big one is I have a job. If\", 'hello, how are you doing today? You\\'re trying to come out there and play a bit,\" P.J. Tucker joked, recalling how he got up in the morning after catching the New England Patriots\\' touchdown. \"That\\'s all. I', \"hello, how are you doing today?\\n\\nYeah, yeah.\\n\\n\\nCheryl : It's okay.\\n\\n\\nGordana : Wow. That's good.\\n\\nBjornsson : I've been reading the stuff and\"]\n",
            "rewards: [0.6058208990334308, 1.5264653218066289, 1.4369710802889006], baseline: 1.1897524337096534\n",
            "log probabilities: [-2.8917605876922607, -2.803837776184082, -2.6773953437805176]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027909979224205017\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nI got it working for 2 hours and 2 minutes, but it just couldn't be updated to my current version on my computer. No login to my admin account, no password to reset settings on Win3D\", 'thank you for your help!', \"thank you for your help! (Or at the very least, if all else fails.)\\n\\nThanks\\n\\nY'all may look like you're doing this in a lazy way, right?\\n\\nSo. How about that?\\n\\n\"]\n",
            "rewards: [1.8816297240453919, 2.9747552337650913, 2.9870641481185403], baseline: 2.6144830353096746\n",
            "log probabilities: [-2.890437602996826, -2.5483052730560303, -2.735697031021118]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027248132973909378\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. Do they make me feel better?\\n\\nA: Yup. You make me feel even better. Your message is very clear: I am trying to learn. They are still getting on top of me. The point', 'i really appreciate your kindness. Please help me to move on from those things so I can get the good life ahead of myself in New Jersey and help make my place even greater.\" (Ewok)\\n\\nIt has been an absolutely amazing year', \"i really appreciate your kindness. If you want to try for a spot on a local team we're hoping this is the location for you. Otherwise we will go straight to the start at 8:00, we will show you onsite so please be\"]\n",
            "rewards: [3.034946108934217, 2.9669648155369632, 2.8003730592762994], baseline: 2.93409466124916\n",
            "log probabilities: [-3.0231950283050537, -3.307224750518799, -3.348243474960327]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03226220980286598\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Because, uh, you\\'re just a girl and I\\'m not a girl…\" Then he continued with his story. The story is very much like what it sounds like when there is a very low-level man', 'could you please assist me with this?\\n\\n\\nI understand that my computer will be uninstalled when you send the file that has been copied in to your computer or in the settings menu of your computer but if you send anything before that I will be', 'could you please assist me with this? I can\\'t do it without you! I\\'m going to take care of you. You were all very pleased with the new car you made. How\\'s that working out for you?\"\\n\\n\"Oh really']\n",
            "rewards: [1.8435202171713791, 3.090381312088535, 1.9946631527116485], baseline: 2.3095215606571875\n",
            "log probabilities: [-2.9752907752990723, -3.0716278553009033, -2.4174907207489014]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028214698657393456\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0290941022336483\n",
            "iteration 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, it should be clear that this particular version of this post is not to dismiss people's worries.\\n\\nStill, this isn't actually about politics. Rather, this is a post examining a particular political argument that is more clearly\", 'once upon a time, we find ourselves in the company of a new and unappetizing species of human and animal animals which inhabit the midst of all this, and are never seen again until all are at rest, not far from the shore,', 'once upon a time, one that might be said to include the destruction of civilization through war, as well as the end of civilization and ultimately its civilization as well. For example, it is only through this system that you can develop your understanding of science']\n",
            "rewards: [-0.13192135915059183, -2.31627326376751, -1.3234990567670561], baseline: -1.2572312265617194\n",
            "log probabilities: [-3.1223111152648926, -3.1227023601531982, -3.0907208919525146]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03111911378800869\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? My brain tells me it's doing well. I do not think that's the real news. It's just that your face, while pretty good, when you're in a coma. I'm still feeling it\", \"hello, how are you doing today? And then here is one for you:\\n\\nIf I'd gotten out of hand this morning to say something about how I was spending more time at home than I would have in the future, I could have\", 'hello, how are you doing today? \"\\n\\nYuri:\\n\\nYou have a bit of time. Since we are in Kyoto. I don\\'t want to be interrupted.\\n\\nYuusha:\\n\\nWell, but even so,']\n",
            "rewards: [0.5226815832921797, 1.5966284729428448, -1.4654758920656374], baseline: 0.21794472138979573\n",
            "log probabilities: [-2.8888471126556396, -2.791135549545288, -2.6155142784118652]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027651656419038773\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! Don't hesitate to let us know in the comment sections below. If you'd like more details about our plans for this season of AIM Podcast, check out the below links:\\n\\nAmazon.com (Amazon Prime\", \"thank you for your help!\\n\\nPlease consider supporting TTF on Patreon\\n\\nTo show our love for you all: we're not done with the first season of the show, but will soon start talking about our next one...\", 'thank you for your help!\\n\\n\\n\"What can I tell you?\"\\n\\n\\n\"The truth is you got a choice, it\\'s all because of that man.\"\\n\\n\\nShe was very surprised to discover I had picked it up and carried it']\n",
            "rewards: [2.0413681801538646, 2.0050804819740558, 2.8790576111148156], baseline: 2.308502091080912\n",
            "log probabilities: [-2.364455461502075, -2.702009439468384, -2.8001179695129395]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02622194215655327\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. My favorite quote is: Your best friends are really nice people, so you shouldn\\'t just stop trying to help them.\"\\n\\nMarilyn, whose father left his hometown in 1982 to pursue law school, said she', 'i really appreciate your kindness.', 'i really appreciate your kindness. Also a big welcome to all those with issues there, and we love you. Also, we are using Dropbox for free, but all we need is someone to save these files... Thanks. -Sam']\n",
            "rewards: [3.0323739406029038, 3.0925878103322595, 3.173553591682077], baseline: 3.0995051142057473\n",
            "log probabilities: [-3.1392743587493896, -4.8637542724609375, -3.647977590560913]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03883668780326843\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? I would love this and most of you would just like to read it.\"\\n\\n\"You\\'re welcome. I will see the person who made the request the next time.\"\\n\\nThe girl smiled.\\n', 'could you please assist me with this?', \"could you please assist me with this?\\n\\nYou will help me when you reach your vision.\\n\\nYou may be able to make my vision go blurry so that the image in the upper right corner doesn't distort.\\n\\nBy looking up\"]\n",
            "rewards: [3.1778026532121997, 3.1325208183896565, 3.040012014527575], baseline: 3.116778495376477\n",
            "log probabilities: [-2.621945858001709, -3.1072323322296143, -2.786543130874634]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028385737910866737\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.03044302761554718\n",
            "iteration 11:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, so my family\\'s home would have seen through all the bullshit the family told me would keep me safe in the future. I\\'ve not gotten used to hearing about \"no one is watching this right now,\" which actually is part', 'once upon a time, that the whole world was changed by their appearance and by the effects of things they had done.\"\\n\\n\\nBy this time, they knew they had no doubt that the world, in which they were living, would be a much', 'once upon a time, when the enemy is weak, at this point in time, when you have at full power in a war that has already cost an average of about twelve or twelve years.\" Then Joseph looked around the valley, and heard that he']\n",
            "rewards: [-0.05598865053766261, -0.11266287546086705, -1.6375710375127586], baseline: -0.6020741878370961\n",
            "log probabilities: [-3.517885684967041, -2.771796464920044, -3.5523455142974854]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03280675783753395\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nHANNA: A lot of it actually goes right on. It's just getting better. But I'm not going to let this get to us for what it is. It's just a matter\", \"hello, how are you doing today? A: You're working.\\n\\n(Curtis: Don't make me stop it!)\\n\\nA: We're all working here.\\n\\nAnd you have your own job for our little bit\", 'hello, how are you doing today? I feel really good, so I don\\'t know what to say. Why can\\'t we have a chat?\"\\n\\n\"Because we could just get on with it if she\\'s alright.\"\\n\\nMou']\n",
            "rewards: [0.5001097836774884, 1.4910847582887863, 0.8043765881563487], baseline: 0.9318570433742078\n",
            "log probabilities: [-2.498913288116455, -2.8358466625213623, -2.4390342235565186]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02591264806687832\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! As a tribute, please donate. It helps my work. Thank you.\\n\\n1:06 PM :', 'thank you for your help!\\n\\n\\nI hope you enjoy this comic as much as I do and please rate it on r/fantasy!', 'thank you for your help!']\n",
            "rewards: [3.106715887245061, 3.0678264263472848, 2.8653268695966534], baseline: 3.0132897277296666\n",
            "log probabilities: [-3.146825075149536, -2.2780373096466064, -2.5483052730560303]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02657722495496273\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\nTravis: Yes…I really do.\\n\\nAaron: Thanks, Travis!\\n\\nTravis: I don't need it to help.\\n\\nAaron: Yeah, just…oh well, you\", 'i really appreciate your kindness. So far so good...\"\\nThe cat that gave me his name didn\\'t care to take my word. She just said \"sorry\" without speaking to it anymore and walked away.\\nKurikomori had no', 'i really appreciate your kindness. I can\\'t believe you are helping one of the best and brightest in your life.\"\\n\\nIn another piece from the New York Times Magazine, she explains of her experience, \"I\\'m sorry to have had to talk']\n",
            "rewards: [2.820765272904421, 1.907934413209465, 2.0953884839053503], baseline: 2.2746960566730787\n",
            "log probabilities: [-2.467021942138672, -3.470822334289551, -2.8561484813690186]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029313309118151665\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? (whispered as she reached back to pull herself from the pillows). She then placed her hand on her daughter, who was holding on to the baby before being kissed on it. While all two women', 'could you please assist me with this? Is your name, and any of your addresses, the subject of enquiry?\"\\n\\nA woman in a tuxedo entered from outside the house. As he opened the door, there was a strange noise', 'could you please assist me with this? Why would one of my daughters find it so easy and how can you find out who I am?\"\\n\\n\"Your Majesty? Don\\'t be rash as well!\"\\n\\n\"I feel that you know the']\n",
            "rewards: [2.804332608309676, 1.9659541409707242, 2.0282019011436576], baseline: 2.266162883474686\n",
            "log probabilities: [-3.2388505935668945, -2.6165051460266113, -2.832190752029419]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028958488255739212\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.028713685646653175\n",
            "iteration 12:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, if you wish to speak of such things, he may be heard to say that they do not belong, they are done; but they never return after that time\" (Luke 17:26-27; 1Co. 2', 'once upon a time, the two main points in dispute can be understood in four terms:\\n\\nFirst: The legal doctrine of \"precedence\". This was elaborated in Sir Edwin Thackeray (1725–1770) in his volume', \"once upon a time, it seemed necessary to set aside the fact that each individual person's life, with all its various circumstances, including his/her individual self, is also a unique and unique phenomenon, especially as it is also often used to express\"]\n",
            "rewards: [-2.388511175506358, 1.3944603145074146, 0.13359130435895683], baseline: -0.2868198522133289\n",
            "log probabilities: [-3.007455825805664, -3.1353631019592285, -3.0820372104644775]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030749520286917686\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? (1:04:44 PM)\\n\\nKatherine: I am on the phone and I'm like ok, I don't know what to say yet. (1:04:56 PM)\\n\", \"hello, how are you doing today? Thanks and can do my best...\\n\\n\\nI'm tired of seeing them every morning. And I know what the future holds for us. This morning, they've been out for the whole summer. Just don\", 'hello, how are you doing today? You look good today but I need a bit of extra energy. It is pretty cold as you are doing the work today. I will ask some people to help with this and they are gonna say no I have']\n",
            "rewards: [0.3770547639899549, 2.016018883339598, 0.16397019407975366], baseline: 0.8523479471364355\n",
            "log probabilities: [-2.1790549755096436, -3.265615940093994, -3.031264543533325]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028253117576241493\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! If there are any issues relating to the download or update, then please contact a game developer as I have an unplayable version of my game running on this emulator. I am doing this with a version of Cydia', 'thank you for your help!', \"thank you for your help!\\n\\nUpdate! This was originally a preprint that wasn't working out or had to be cut for the print by hand.\\n\\nUpdate: I'm able to read past this post now! Hopefully this post will\"]\n",
            "rewards: [3.0970438717875544, 3.1988709636026735, 1.847089923946226], baseline: 2.714334919778818\n",
            "log probabilities: [-2.9117374420166016, -2.5483052730560303, -2.9250190258026123]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02795020490884781\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\nRiley\\n\\nYeah, I'm from Sweden. It was great, the music is amazing, I was so happy to be living in Canada. I also have some pretty bad knees, my right knee will\", \"i really appreciate your kindness. You really just need to let us know you need help, okay?\\n\\nMARK HENNEY: I'm glad to hear it. I love it. Thank you so much. Thank you, Rick.\", 'i really appreciate your kindness. (He may want to find a second friend after that) Anyway, thank you, and be proud to be in the Hall of Heroes again!\\n\\n-Natsu\\n\\n[Image Credit: Alvaro Gonzalez']\n",
            "rewards: [2.0074449719182876, 1.8999837856210038, 2.8539030339227445], baseline: 2.2537772638206786\n",
            "log probabilities: [-3.0759997367858887, -2.6221518516540527, -3.2681071758270264]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029887529090046883\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this?\\n\\n\\nWell, one of our own would be thrilled with that.\\n\\n\\nI can't explain how much I'd hate to get on the show as some random random kid. I just hope it wasn't\", 'could you please assist me with this? That is exactly what I want to do.\"\\n\\nShe walked over to her sister and asked a few question which she answered before disappearing.\\n\\n\"I have always thought so but have you read every page', 'could you please assist me with this?):\\n\\nHi, just to be clear – I will not be paying you anything with this transaction and will not make any payments to you. I just hope you feel confident and would like to make your transaction']\n",
            "rewards: [1.8138458164280142, 3.1289486064335046, 2.1899305252856394], baseline: 2.3775749827157195\n",
            "log probabilities: [-3.0158660411834717, -2.8467905521392822, -2.7525250911712646]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028717271983623505\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029111528769135475\n",
            "iteration 13:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, we are the creators of what is known as The Dimensional Paradox. And that being said, here are a few words to let those who may not be familiar with the phenomenon know that at the time, there was very little', \"once upon a time, he would not have said so. 'Well done' - the words, he understood them, as his friend of sixteen years had explained. 'Now that you've got this, we'll let you come back to the family\", 'once upon a time, so we are able to make it into something, and it all makes us so grateful for it\" (Romans 2:17–18:1).\\n\\n\\nRomans 6 is a very good book about God, and']\n",
            "rewards: [-2.6080966715085108, -2.464532086064878, 2.893789762749343], baseline: -0.7262796649413485\n",
            "log probabilities: [-2.629753828048706, -3.181116819381714, -2.8572072982788086]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028893593698740005\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Did you think that was crazy? (he said some funny things and was really cute). We had lunch there and it seemed like there were three times we were going to be here, and he did some little', \"hello, how are you doing today?\\n\\nIn the week since last week's elections to parliament it seems there have been a lot of changes made and we don't see those in effect until November 2017.\\n\\nWhat does your life look like\", \"hello, how are you doing today?\\n\\nMyself, it took time to learn, but I'll do it in my own way. So here we go again.\\n\\n1. Let me tell you what is going on here.\\n\"]\n",
            "rewards: [-1.6641855564773538, -2.6051161606605246, 1.8920462137361642], baseline: -0.7924185011339047\n",
            "log probabilities: [-3.2222740650177, -2.8088762760162354, -2.44963002204895]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028269268572330475\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\n[From my blog: http://blog.sodos.net/2016/08/20/the-unofficial-bros-npc-project-was-for-a-3d', 'thank you for your help!', 'thank you for your help!\\n\\nThis post is copyrighted by J.D. Power, and you may not reproduce any of the content of this post without written permission.\\n\\nI know, I know! I would hate it if this page']\n",
            "rewards: [2.073209449371935, 2.9458543238814303, 1.8994198948191277], baseline: 2.306161222690831\n",
            "log probabilities: [-2.701876163482666, -2.5483052730560303, -2.1795966625213623]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.024765927344560623\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. I'll certainly keep an eye out for the next release for our fans, and I'm not sure I will get around to releasing another video. Until then, please let me know in the comments if you'd like to\", 'i really appreciate your kindness. I have been thinking about this. As a small school child, I have experienced things which might have made me think much worse but I have learnt from your kindness. It must be a big gift for people to give to', \"i really appreciate your kindness. Please let me know of some other people with autism that I can relate to.\\n\\nBest wishes,\\n\\nH.G.R..\\n\\nIf anyone has any questions or suggestions, don't hesitate to ask\"]\n",
            "rewards: [2.0407621039610415, 1.8160757044468008, 2.113717239113549], baseline: 1.9901850158404637\n",
            "log probabilities: [-2.5320212841033936, -3.20072865486145, -2.5851502418518066]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02772633358836174\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this?\\n\\nA: Please!\\n\\nQ: No?\\n\\nA: But you really won't have another child.\\n\\nQ: No!\\n\\nA: Because you've just abandoned everything you\", 'could you please assist me with this? I thought it would be fun,\" Mimi said.\\n\\n\"It must have helped you out. He came in just from having sex with me,\" Marietta said. \"And the only thing he', 'could you please assist me with this? Your words will help this search,\" she said.\\n\\nCaitlyn says the government will likely appeal the decision, which it said went unreported. But on Friday, she is still trying to get answers']\n",
            "rewards: [2.0311037029813095, 3.038085240338875, 3.1401146583020503], baseline: 2.7364345338740783\n",
            "log probabilities: [-2.268202543258667, -2.8306570053100586, -3.027449607849121]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027087697759270668\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.0273485641926527\n",
            "iteration 14:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, that the Church really has no power over the political realm.\"\\n\\nHe added that Pope Francis \"had not read the Vatican\\'s decree, and thus would be out of his office in two weeks time.\" The Pope\\'s statement', 'once upon a time, that I shall die.\"\\n\\nThe only problem is, there is a way — just one.\\n\\nBy the way, by the way, all of this happened. It happened when my husband and I took a break', 'once upon a time, a day has ended which should not leave one disappointed but we, our ancestors as well as the nation of them who are born here on the Island.\\n\\n\"I am sorry as I have already said there has been great']\n",
            "rewards: [0.03646713771826629, -2.589294115732335, -2.4510728363361642], baseline: -1.667966604783411\n",
            "log probabilities: [-3.01603627204895, -2.6672122478485107, -3.5143473148345947]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030658653005957603\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Did you make some things up, how can you be certain that these things weren\\'t actually wrong?!?\" He was silent for only a second. It really seemed that it would be something like this for at least', \"hello, how are you doing today? Are you going to come over tomorrow? I need you to be there next weekend! You've got to do what you could! You need me to come over and have a big bellyful tomorrow!\\n\\n\", \"hello, how are you doing today?\\n\\nThere are no changes to what happened at Apple. But I'd like to put a spotlight on Apple-related events that have been going on over and over.\\n\\nYou're a very outspoken CEO\"]\n",
            "rewards: [-1.6725761473153409, -1.4009833053806402, 0.19052400063811542], baseline: -0.961011817352622\n",
            "log probabilities: [-3.137665033340454, -2.839524269104004, -2.736769914627075]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02904652990400791\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nI didn't make this up, since we didn't use it for my purpose for this post - that should be noted. In fact, the story here seems to suggest some very serious self-promotion -\", \"thank you for your help! We will be back tomorrow for your question, and we wish you luck getting on in the end. Sorry for any inconvenience (although we don't usually try our hardest to ensure that we have something in hand on the way\", 'thank you for your help!\\n\\nUpdate 12/6/2015 : I have made a little fix to a bug that was fixed in the 1.10 update. This fix could be applied in 1.10 as well!\\n\\nUpdate 11']\n",
            "rewards: [2.0363645880800916, 3.001047445280239, 3.0736930689642685], baseline: 2.7037017007748667\n",
            "log probabilities: [-2.988858222961426, -2.8467071056365967, -2.2577385902404785]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02697768062353134\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I hope and believe that I can be with you all the time, so forgive me if I am too late. It just feels bad to not be there! We all just have to deal with that, you know?', \"i really appreciate your kindness. I'm curious to hear what you guys think of the latest developments.\\n\\nAdvertisement\\n\\nBobby Johnson: You are probably right that some aspects of today's society today can never really go much better than their previous\", \"i really appreciate your kindness.\\n\\n\\nGiraffe\\n\\nOh god GIRFARaffe\\n\\nSoooooh look how cool you are I will give this to someone else if you're ready! Oh you would be fine, as you\"]\n",
            "rewards: [1.89890890336104, 1.8038512877843043, 3.0457944216833353], baseline: 2.2495182042762267\n",
            "log probabilities: [-2.8232531547546387, -3.1559231281280518, -3.5407023429870605]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03173292800784111\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? What would they say?\"\\n\\nShe paused, thinking of the people in the hall and looked down at those people, and then she started.\\n\\nOne group spoke in English like it was Japanese, and', \"could you please assist me with this?\\n\\nKurt Volpe – I'm a bit disappointed in the fact that we will not have the proper funds going out with an official account. I will be supporting K&R in doing my best to\", \"could you please assist me with this?\\n\\nD.A.. Yes.. I have done all of the work for you.. I can assure you they are getting close to the project in most ways. I don't know of some other companies offering\"]\n",
            "rewards: [2.884369935775173, 2.121406969567096, 2.0102061786881715], baseline: 2.3386610280101467\n",
            "log probabilities: [-3.0066561698913574, -3.3142032623291016, -3.007040500640869]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031092999503016472\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029901758208870886\n",
            "iteration 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, I will show you the hidden treasure that lies beneath.\"\\n\\nAnd now.\\n\\n\"And you will see, on this wall, the last remaining hidden treasure for all your people from ancient times, the remains of a', \"once upon a time, in an era when the media tried to depict China's economy by its official role, that of a powerful nation with some of the lowest inequality per capita in the world, the public still found in Xi something of a contradiction when\", 'once upon a time, and a better world was created for it. The one thing that is missing here is that they wanted to get their own kind of social system. All the great revolutionaries who had fought so hard and died to stop oppression were all']\n",
            "rewards: [-2.3335104152451427, 0.032877180287165825, 1.697677228134338], baseline: -0.20098533560787968\n",
            "log probabilities: [-2.9795279502868652, -3.3279082775115967, -3.2210123538970947]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031761493533849716\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? - How did this make you so excited to become an actress? - In our humble opinion, thank you! - Do you think that for the love of my life, you guys know that if someone says I', \"hello, how are you doing today? What were you in school for last semester? It was tough for me because there were too many rules and I felt like you only taught at the junior levels, you didn't understand my strengths like some of my\", 'hello, how are you doing today?\\n\\n\"It has been a couple of months since my last visit to China,\" said one Chinese national in the room as the American national took his seat at table, wearing a tight shirt with a yellow embro']\n",
            "rewards: [2.008689428651635, 0.8141110793482398, -1.3853630782005124], baseline: 0.4791458099331209\n",
            "log probabilities: [-2.9327778816223145, -3.020094871520996, -2.946721315383911]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02966531366109848\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nWe really appreciate everyone who's supported, loved and enjoyed this Kickstarter and we're all so glad it took this long to come out. I hope you'll continue to do your little bits as much as you have\", 'thank you for your help!\\n\\nFollow Us - Get Our Latest News - Join Us\\n\\nMore on The Stag\\n\\nThe Stag has created several web apps and a podcast to bring you the latest content and services that the Stag', 'thank you for your help! <3\\n\\n*The two-part event will be held in North Carolina with food from our local grocery and store as well as snacks by local restaurants. No food, drink, or entertainment will be allowed! No']\n",
            "rewards: [3.1258259674548317, 3.174671231008829, 2.109996580741322], baseline: 2.803497926401661\n",
            "log probabilities: [-2.573125123977661, -2.8936424255371094, -2.870180368423462]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027789825573563576\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. Let\\'s see whether you can continue the friendship and let the next girl walk down the path of getting you. I will see you all tomorrow! \"\\n\\n\"Let\\'s not stop until tonight. I\\'m not going', \"i really appreciate your kindness.\\n\\n3) So I am not telling you to leave your friends if you don't want to have an argument about my character on your watch. I am saying for that I respect your values and I will gladly disagree\", \"i really appreciate your kindness. Thank you. If you are feeling particularly weak or have an issue, feel free to leave your e-mail address on this page. I'm currently looking into making new and improved ones (possibly via new social networks like\"]\n",
            "rewards: [2.1900443476664475, 2.144866566424543, 2.8633423185458646], baseline: 2.399417744212285\n",
            "log probabilities: [-3.247640609741211, -3.1534183025360107, -3.063615560531616]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031548913568258286\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? In the meantime, you may be sure to help by sharing the news and updates regarding the next 3 years of our partnership. I would love to see you all joining us at ESSEND (not for a', 'could you please assist me with this?\\n\\nIt is important that, before you can do this, your friend ask you to go on the phone or write it. You can use text and link, but the best thing is that you know the', 'could you please assist me with this? I am so busy, my entire family is all waiting for me to finally get the next book in my head that is all I ever want to write.\"\\n\\nAs soon as his parents saw the letter,']\n",
            "rewards: [2.0663456245104026, 1.8387445562643177, 2.923930304132076], baseline: 2.2763401616355985\n",
            "log probabilities: [-3.141313314437866, -2.9259421825408936, -2.8216419219970703]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.0296296589076519\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.03007904104888439\n",
            "iteration 16:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, for you have shown your willingness to do and do what is right or unjust. If it's too much like, well, you've seen how, say, I was in bed when the clock struck midnight, now I think\", \"once upon a time, I went down with this big ol' dude that said, 'Look, you can buy the same crap in New York,' then go down with the rest of them and give it a whirl, with their money. When\", \"once upon a time, but I can only hope. I could just as easily have tried the other three options...\\n\\nI'm going with the 3.8, and on its current iteration I'm still looking at 2.8 with less issues\"]\n",
            "rewards: [0.5564603454115946, -1.6170233846621944, 1.0299229008321669], baseline: -0.01021337947281098\n",
            "log probabilities: [-3.276730537414551, -3.2255115509033203, -3.1810169219970703]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03227752819657326\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nBORNE: Well, he's been out of town. He had some vacationing plans, which has been going on for a long time, where I was getting a new job and got back\", \"hello, how are you doing today?\\n\\nLENNY SANDETTE: We didn't really have anyone on to us at the time [at a press event in December]. We were all sitting just at a point where we were having\", 'hello, how are you doing today?\\n\\nT.G.: My wife, Susan, is the most popular one and she was working on the film in Boston this past week and this is a huge job for the person and her background and her']\n",
            "rewards: [1.1568582315830294, 0.9161429863200149, 1.6875221992213538], baseline: 1.2535078057081328\n",
            "log probabilities: [-2.6816866397857666, -2.9849109649658203, -3.1593313217163086]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029419763013720512\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! -SaraKaze\\n\\nAs a man who\\'s become famous for his extreme style of play you can never go wrong. The story of \"Rudy\\'s Tale\" will bring that style together with real-', 'thank you for your help!\\n\\nMy name is Sia. I am 28 years old. I have one question: can we please stop watching the news and stop being stupid idiots and become real friends. How is it possible for so many people', \"thank you for your help! I look forward to seeing you at St. Patrick's next time you are ready.\\n\\n-Djakji\\n\\nMore information\"]\n",
            "rewards: [2.1037350651404503, 3.015584372685458, 3.0793161188561133], baseline: 2.732878518894007\n",
            "log probabilities: [-3.397291898727417, -2.7261111736297607, -2.6668214797973633]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.0293007493019104\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\nThere are two kinds of people. Either you're friends with them; or, you're willing to talk about sex when you're going through an emotional breakdown.\\n\\nWhat I'd really like to discuss is\", \"i really appreciate your kindness. I guess if I see a thing going well the first time I don't like it...you need to start thinking about this!\\n\\nBut you know how to stay ahead, just keep moving ahead.\\n\\nThe\", 'i really appreciate your kindness. But if it\\'s my daughter, or my boyfriend, then my sister or me.\"\\n\\nWe went. I turned my eyes away.\\n\\nThat\\'s when she turned to me, and the first thing she said']\n",
            "rewards: [3.0912722019819068, 1.8162388731994443, 2.819946671356319], baseline: 2.57581924884589\n",
            "log probabilities: [-2.625037670135498, -3.3410983085632324, -2.928130865097046]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02964755706489086\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? I really, truly want you to know I just recently lost another dear, wonderful husband who is not only loving but also the most awesome person of all time...He took me to heaven.\"\\n\\n\"What', \"could you please assist me with this? I'm really very confused to be giving so little of my time back to you all. If I get a full refund of our money, I'll simply have to buy an ambulance from them in the next few\", 'could you please assist me with this?\\n\\n\\nAnswer: yes\\n\\nI would appreciate it and please give me what you offer my. I am so very very sorry to see this so badly written, thank you for your help on the matter.']\n",
            "rewards: [1.982422420155795, 3.055558337788059, 2.09988302839876], baseline: 2.379287928780871\n",
            "log probabilities: [-3.2039520740509033, -2.9835853576660156, -2.893045425415039]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030268609523773193\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.030182841420173644\n",
            "iteration 17:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, to be able to stand up and defend this great world from all who want it to be conquered,\" he said.\\n\\n\"By this we mean that even in such as it was built by the great human powers, we', 'once upon a time, it is possible, in our current time, it must have been there, but there is no one to judge it as such.\"\\n\\nThe idea seems highly implausible, and as a person who can read more about it', \"once upon a time, there has been a lot of stuff done with. For the most part he's just kind of been working very hard, and that can help us improve. You know, he can go out there and do what they want.\"]\n",
            "rewards: [1.671264902508379, 0.1289976126815553, 0.682014810167223], baseline: 0.8274257751190524\n",
            "log probabilities: [-2.969374418258667, -3.009450912475586, -2.835045337677002]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029379568994045258\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nNOVEMBER 8: It'll have to have the wind up the way it's come up here and it'll change it, for a change. The last week is where it all ends so things\", 'hello, how are you doing today? What\\'s this all about?\"\\n\\nThe blonde, with long blond hair had come out to hug Kaitlyn, one of Kaitlyn\\'s closest friends and was talking about things from a different angle.', \"hello, how are you doing today?\\n\\nWOMEN'T READY TO SHIELD THUS. THEY THOSE WHO ARE GOOD AT SHIT WILL SAY HONEST WAY. IT HAS SOMETHING TO DO WITH TRIVIA\"]\n",
            "rewards: [-2.3827100098738625, -1.3828932828500626, 1.4599748576691898], baseline: -0.7685428116849118\n",
            "log probabilities: [-3.401744842529297, -2.7366504669189453, -3.0292530059814453]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03055882826447487\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nMy Name is BRIAN HAWKINSKI\\n\\nThis is also a great place to connect.\\n\\nThis place has all the services I need to make sure I am ready.', \"thank you for your help!\\n\\nWe love using WordPress as our first source of social media platforms. But many people have an uncomfortable place on Twitter that prevents them from understanding why some sites will let you follow them. Some sites won't even allow\", 'thank you for your help! It will be my absolute favourite place to see all that we share.\\n\\nOur first time working together we loved the feel of a new pair. The first time I checked was in an old school hospital and with no']\n",
            "rewards: [3.040405080562351, 2.0472654892625863, 1.9687600646371328], baseline: 2.3521435448206898\n",
            "log probabilities: [-2.5557782649993896, -2.960533380508423, -3.2957160472869873]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029373425990343094\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.', \"i really appreciate your kindness.\\n\\nIf you want to speak about the world through the voice of justice you can send my tweet out here or my contact info here.\\n\\nI am going to do this so I don't have to put it\", 'i really appreciate your kindness. I will gladly give you one more piece of my gift (not a special one). Thanks!! Happy Holidays,\\n\\nKatherine\\n\\nTrent, PA, USA Posted on 19 January 2011 @ 16:28']\n",
            "rewards: [2.9105404271139887, 3.134277832468099, 2.1962530035517696], baseline: 2.7470237543779525\n",
            "log probabilities: [-4.8637542724609375, -2.9231016635894775, -3.1370368003845215]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03641297668218613\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? How is that going to pay?\\n\\n\"Yeah, and this is why I asked me to do these things, so that I could provide support, and you can do the same for the world, you', 'could you please assist me with this? I had a very small problem to resolve, but I don\\'t wanna waste time with you…and I am not going to allow that to stop you from continuing.\"\\n\\n\"You really do love what you', \"could you please assist me with this?\\n\\nThis is a simple question for the future development community but to provide a short description of what makes the process interesting, we need a lot of community feedback to create this. That's why we created the\"]\n",
            "rewards: [3.160591536296028, 1.9278366003447434, 3.0534694580390918], baseline: 2.713965864893288\n",
            "log probabilities: [-2.922276735305786, -2.7914750576019287, -3.0174460411071777]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029103992506861687\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.030965758487582207\n",
            "iteration 18:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, but you will find that we will all share my happiness, because it is your happiness which I desire.\"\\n\\nAnd the prince said: I was in love, I wished to be loved, I will always be loved in', 'once upon a time, so far as I know, there have been in Europe a series of such small but significant events with regard to all social aspects of their lives in regard to which in that instance in those years of the last twenty years they have', 'once upon a time, it was a strange one; the people were very kind and generous; and we had not long to wait for a long time; but we, I think, could not get to it with any force.\"\\n\\nWe must']\n",
            "rewards: [2.0772242241602656, 0.1929478846725533, 0.11712039626907927], baseline: 0.7957641683672994\n",
            "log probabilities: [-2.9435081481933594, -3.214480400085449, -2.902810573577881]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03020266443490982\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? [E.L.:] My doctor went out and called me, so this is all you did today. This is all that he knew. [E.L.:] What do you want him to talk', 'hello, how are you doing today? Is there any room to grow? I don\\'t know if we\\'re going to get a good deal out of that situation,\" he said.\\n\\nAnd that is what his wife, Karen DeCox,', \"hello, how are you doing today? I'm doing fine\\n\\nI do all the stuff but I was doing fine I was very slow right now. I haven't done anything before but what do I do now so we all have time to do\"]\n",
            "rewards: [1.5182310616320052, 1.1398040446033826, 0.05598504605276278], baseline: 0.90467338409605\n",
            "log probabilities: [-2.796900749206543, -2.5704386234283447, -3.053899049758911]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02807079441845417\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! :)\\n\\nPosted By: cate kaczynski | Nov 26, 2014 6:22pm It looks like they are still in a good hold of the cash as they are unable to open a transaction. Check it', 'thank you for your help!', 'thank you for your help!\\n\\nMore information about the development of the site here:\\n\\nhttp://mashinsolutions.com/2014-11-23/pilot-beta-2\\n\\nhttps://github.com/m']\n",
            "rewards: [2.05514627857075, 3.1746396524163893, 2.986669724838906], baseline: 2.7388185519420154\n",
            "log probabilities: [-3.179198980331421, -2.5483052730560303, -2.302480697631836]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026766616851091385\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. I will keep them busy until I send my family the Christmas car and my grandchildren come back home. You will be amazed at how we've become family friends and always want to get your ideas of which movies are best when\", 'i really appreciate your kindness.\\n\\n\"We\\'re here to serve.\\n\\nYou must have taken something away from my house.\\n\\nTo know all your good intentions without hurting me.\\n\\nTo find that my husband lost his life.', \"i really appreciate your kindness. (laughs). It might be easier for her to accept this kind of situation, since there aren't any problems now. She might even start to look forward to the day when she and her younger sister might meet.\\n\"]\n",
            "rewards: [2.8027644061333694, 2.0214832128888287, 2.075893408038977], baseline: 2.300047009020392\n",
            "log probabilities: [-3.8275306224823, -3.1420717239379883, -2.9522087574005127]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03307270258665085\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?\\n\\nIn your case I believe you need at least £100,000 for a hospital treatment\". In your case, of course you just need €10 for a private home or $5 million ($5 million', 'could you please assist me with this?):\\n\\n[P-pierced (C-Y-L)) M(O-P) (I) You were not able to give them a little explanation. :3\\n\\nSid', \"could you please assist me with this?\\n\\n[20:15] Hm.. I will, please help me.\\n\\n[20:15] Nvm! I am, sorry to break your promise but it won't help. I\"]\n",
            "rewards: [3.1475874400249517, 1.9027793263977273, 2.0245813516616447], baseline: 2.358316039361441\n",
            "log probabilities: [-3.0891358852386475, -3.4865071773529053, -2.662936210632324]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03079526498913765\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029781608656048775\n",
            "iteration 19:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, but if I'm to take back what is in my soul, my will, my will be the very first I need. I don't even need to take a bath, I can just do it. If my soul is\", 'once upon a time, it is not so bad as we\\'re often taught to believe,\" he wrote. \"A good teacher is able to show others how to master all kinds of situations and not lose his cool if things break up when they get into', 'once upon a time, but for some purpose of having all men in their right spirits on this earth to be, it is so decreed at the command of God for that day, for that day I, Adam.\" So the Apostle adds \"for']\n",
            "rewards: [1.0240905484593292, 0.46251274617335175, 1.5967673246782732], baseline: 1.0277902064369846\n",
            "log probabilities: [-2.894040822982788, -3.2519032955169678, -3.559964179992676]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.032353028655052185\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? What did they tell you? I'm just trying to enjoy life. I'm just really thankful for what we had to have in this world and we had a wonderful time here to watch film today.\\n\\n\", 'hello, how are you doing today? How are you getting so much done? It is the most amazing and beautiful news I\\'ve ever read,\" Mr. DeLago said. The president also said Mr. Trump won two more primaries in Missouri and', \"hello, how are you doing today?\\n\\nKenny: I look great! My body's strong. It's actually quite happy.\\n\\nDana: You haven't told me that you used to work out there? But now you live\"]\n",
            "rewards: [1.9020473412483658, 2.0281292002348112, 0.3528706960253717], baseline: 1.4276824125028496\n",
            "log probabilities: [-2.876680374145508, -3.0474777221679688, -2.726583957672119]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028835806995630264\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\n\"He was very excited to do that!\"\\n\\nThe couple then watched in stunned amazement as the man walked out of the restroom, which was immediately surrounded by a number of onlookers, many of whom', \"thank you for your help! For any questions, please contact our team at support@spaceruniversity.com and we can help.\\n\\nP.S.: We have all three of our videos down so we couldn't be better with your\", \"thank you for your help!\\n\\nAnd I'll be posting a new post every Wednesday.\"]\n",
            "rewards: [3.0511854843338884, 3.090825071068238, 3.159736964628968], baseline: 3.1005825066770316\n",
            "log probabilities: [-2.4943416118621826, -2.637498617172241, -2.4569413661956787]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025295939296483994\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. I've just turned 50 years old and can hardly move a muscle! I appreciate it. Please call if you like (616) 577-8180 ext 3018 If you'd like the full list here it\", \"i really appreciate your kindness. Thanks. I'm working on the game tomorrow which looks pretty cool.\\n\\nMy dear brother will get it\\n\\nJust remember we won't lose out when the game launches out next month; we'll be looking at\", 'i really appreciate your kindness.\\n\\n-Barry B\\n\\nUPDATE: We\\'re hearing something to the effect of something like \"If anything makes this any easier for me and my wife to work we can make sure that it\\'s made no impact']\n",
            "rewards: [2.985225132225673, 2.1013450488236542, 2.0010936587737866], baseline: 2.362554613274371\n",
            "log probabilities: [-3.7350988388061523, -3.5876851081848145, -3.2690327167510986]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03530605509877205\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? Please let me know. My name's Gabor. And we must keep calm... [putting his arm around her]\\n\\nCarnel\\n\\n(Kirby)\\n\\nHello.\\n\", 'could you please assist me with this? Is there anything important for me to share with everyone that you have met in this field?\"\\n\\n\"Not that I know anything about. I had known before you started speaking that you were very serious and well', \"could you please assist me with this? I've got all the answers. Please fill this in. Thanks,\\n\\nAnonymous 15 August 2008 at 8:17 AM No. 273486 >>273484\\n\\nI'm just trying to find\"]\n",
            "rewards: [1.823730970282699, 2.0425294276255173, 1.808908857299058], baseline: 1.8917230850690914\n",
            "log probabilities: [-2.9355945587158203, -2.775125503540039, -2.6152591705322266]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02775326371192932\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029908818751573564\n",
            "iteration 20:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, the government that has imposed the military rule has begun to collapse, not realizing that they are losing the struggle and, on the contrary, that only the most powerful and organized of the elite are left standing and fighting. The elites', 'once upon a time, she would do all the hard stuffs on her way home to her job as a waitress, working in the warehouse. No, the same sort of thing goes for this new waitress, with no kids or her own parents.', \"once upon a time, the only way they'd actually find it was a second-story window. At this point, the doors behind them wouldn't open; rather the entire building would just roll and fall down like a giant box or something else.\"]\n",
            "rewards: [0.019590815606820883, -0.061952364561152745, -1.4181950145509252], baseline: -0.4868521878350857\n",
            "log probabilities: [-3.1448678970336914, -3.2366325855255127, -3.1290483474731445]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03170182928442955\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nJ.B.\\n\\n[sigh] I'm sitting on a coffee table in the garage, it's not working, I have to start using my keyboard now, and it's getting kind\", 'hello, how are you doing today?\\n\\n\"When we finished working on our first post we sent a press release with our announcement statement and then the first step after I had made my initial post about my latest project. I think it\\'s great', \"hello, how are you doing today?\\n\\nTUNA WOLFLEXMAN: Well I'm really, really, really going to take a step back now and then because I've worked in business for 40 years. I don't\"]\n",
            "rewards: [0.8524091790675488, 0.4740113568443086, -0.19220884385952255], baseline: 0.3780705640174449\n",
            "log probabilities: [-2.6474649906158447, -3.160043716430664, -2.5815460681915283]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027963515371084213\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nHow it works\\n\\nAfter you submit a free report to this company (your information must be in accordance with the required company policy), we'll provide you an email that contains the complete source code for the code\", \"thank you for your help!\\n\\nPlease let me know if you have any questions! I am a wonderful lady and I'm working on a new book for my next book which will be available to backers as part of the rewards campaign. To get\", 'thank you for your help!\\n\\nThanks a lot to everyone involved in putting these together. Any information you share should definitely be taken with a grain of salt to those in our development team, and to the community as a whole. The people working']\n",
            "rewards: [2.8161251848427042, 2.0577595459446756, 3.19090072521442], baseline: 2.6882618186672667\n",
            "log probabilities: [-2.7320821285247803, -2.3957090377807617, -2.470036029815674]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.025326089933514595\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\nHuge thanks to everyone who supported me over many years so generously.', 'i really appreciate your kindness. You need to consider how much better you feel with the next step.', \"i really appreciate your kindness.\\n\\nYou'll need an HTML5 capable browser to see this content. Play Replay with sound Play with\\n\\nsound 00:00 00:00\\n\\nPlease note that we need funds to cover these costs, which\"]\n",
            "rewards: [2.8653922637287437, 3.1157462724816267, 2.192156395342299], baseline: 2.7244316438508895\n",
            "log probabilities: [-3.131950855255127, -3.615842342376709, -1.4334982633590698]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.0272709708660841\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? I need your help, but I do not have access to your body,\" says the doctor. Her husband, a nurse from Florida\\'s Gulf Coast region, and son, his wife, and an 18-year', \"could you please assist me with this? If it's not my mission at my disposal, I'll make arrangements for your return via an online courier service for those three-quarters of an hour (9am-5pm UTC). I can be reached\", \"could you please assist me with this? I am able. - If it was difficult or just annoying please help. - If your help wasn't so simple please feel free to contact me so I can help more :) - Good luck ;) - Hello!!!\"]\n",
            "rewards: [2.147521859915229, 2.152110937796086, 2.046767143774056], baseline: 2.1154666471617904\n",
            "log probabilities: [-2.7523305416107178, -3.01900315284729, -3.2725670337677]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030146336182951927\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.028481748327612876\n",
            "iteration 21:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, it would have been the most powerful weapon it ever had, as it was as powerful as the power was capable of. It was a truly legendary weapon, and a special case of the Dragonfly which I will name a lot', 'once upon a time, the most important of all is a simple thing. And that one thing I am happy about is it is my own decision,\" says the 34-year-old, who can now only speak Mandarin, English and Mandarin German.', \"once upon a time, it doesn't occur often. And many people's fears about their own fears are the same ones of those fearful of my own fears. My fears of mine are based not on those people, but on fear of the consequences of\"]\n",
            "rewards: [1.6090191207532862, 0.6644647865086258, 0.11766214020237037], baseline: 0.7970486824880941\n",
            "log probabilities: [-3.070812225341797, -2.913959503173828, -2.889186382293701]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02957986108958721\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nDUNCAN: In general I'm very calm here. It's very interesting. I've seen, I've had people say these types of reactions often. I think people think I'm too quiet\", 'hello, how are you doing today? It\\'s great.\" \"I\\'m all in!\" said Lipscomb. \"It was so hard for the first two hours of rehearsals, not knowing how it would all shake out. That\\'s what I', \"hello, how are you doing today?\\n\\nYes, we had a great day! Thank you so much.\\n\\nWe're excited to see our game continue this next season. We're making it much better so that you don't know that\"]\n",
            "rewards: [1.3333863558027912, 0.5713062176916716, 2.062293079039799], baseline: 1.322328550844754\n",
            "log probabilities: [-2.8630950450897217, -2.7139739990234375, -2.6085259914398193]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027285316959023476\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! In this thread I've included instructions on how to setup Ubuntu 24.10 with a custom Ubuntu installer for the various drivers. It is also possible to download the updated packages by editing the Ubuntu Settings - General / Update /\", 'thank you for your help! This post and its associated content may include copyright-protected and proprietary material. Copyright infringement takes place online, and on behalf of the company will be provided in writing if there are enough requests, information requests or issues. I', 'thank you for your help!']\n",
            "rewards: [3.154064107211315, 2.15814492203968, 3.0753464348533424], baseline: 2.7958518213681125\n",
            "log probabilities: [-3.1312577724456787, -3.3213565349578857, -2.5483052730560303]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03000306524336338\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. Thank you. May my little angel have your love and kindness ever been yours.\"\\n\\n\"He was really nice at that point.\"\\n\\n\"My god?\" (Makarov) \"… He wasn\\'t doing as', \"i really appreciate your kindness. She also did an outstanding job, because all the support we've been put through (in my life and as a human being), by all the people you've raised. Thank you so much.\\n\\nThanks again,\", \"i really appreciate your kindness. She will thank you for making this place one of the more enjoyable place in the city for all you people.\\n\\nSandy's is the place I have been looking for, I would say. I used to come\"]\n",
            "rewards: [2.9946548541814555, 2.80033561739334, 3.064980372842439], baseline: 2.9533236148057447\n",
            "log probabilities: [-3.4221599102020264, -3.037780523300171, -2.94722056388855]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03135720267891884\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?', \"could you please assist me with this? Thank you. I'll do what I can to help you.\\n\\n[...]\\n\\n[...A: Go and visit the inn; if you like something, contact him. Also, try to give\", 'could you please assist me with this? You are already on to me? Just please help me, so that the child can become his master\\'s brother. Then you can take him wherever and be part of my company,\" she said and gestured to']\n",
            "rewards: [3.0308654809612943, 3.0262423117662736, 3.0032246492608214], baseline: 3.02011081399613\n",
            "log probabilities: [-3.1072323322296143, -2.824913263320923, -3.3665361404418945]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030995605513453484\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02984421029686928\n",
            "iteration 22:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, before he came into their ranks, saying unto them, I say unto you, we are all the children of God which he hath given unto our mother; so that no more children shall come from a man for it be said', 'once upon a time, that could be as soon as the world starts shaking.\\n\\nWhat happens when we come down to earth to ask, how do you manage your health when this is a completely free zone and no one is around?\\n\\n', 'once upon a time, and I think in our world we don\\'t need to come close. But our work is really only getting going, and it will not get even closer.\"\\n\\nIt\\'s been a frustrating summer for former Bulls assistant coach Brian']\n",
            "rewards: [-2.382888975418587, 0.18948364779295984, -2.350520842777022], baseline: -1.5146420568008832\n",
            "log probabilities: [-3.124044418334961, -3.1308906078338623, -3.1240317821502686]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03126322105526924\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? My father's on holiday with me. It's going well, thank you. Have you ever thought of seeing any photos after that? Not only do you have a huge smile but also, oh how have you\", 'hello, how are you doing today?\\n\\n\\nA: I just kind of do my business. I feel like the real job is to live up to what I say. What makes me happy is that nobody will be a slave to me. It', \"hello, how are you doing today?\\n\\nWell we had a very nice day, as expected. What will our next activity be?\\n\\nA little bit of music. Good thing we have a song with the name 'Bunny In Our\"]\n",
            "rewards: [2.1891530027499475, 0.604258262853524, 1.5446423747489193], baseline: 1.4460178801174637\n",
            "log probabilities: [-3.0937788486480713, -2.630488872528076, -3.0100836753845215]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029114505276083946\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nShare this: Twitter\\n\\nFacebook\\n\\nGoogle\\n\\nReddit\\n\\nTumblr', \"thank you for your help! I've always been fascinated by art, but I always saw it as all-encompassing, but my earliest interest really comes back to the question of what makes an animated show unique — how it works in each scene\", 'thank you for your help!\\n\\nDownload\\n\\nThe code for the mod is included here in your load order.\\n\\nTo save the mod on your saved game.\\n\\nTo use as a cheat map.\\n\\nFor additional info:']\n",
            "rewards: [2.96070738784493, 3.199169177180452, 3.0677444491376846], baseline: 3.0758736713876886\n",
            "log probabilities: [-1.0185952186584473, -2.975154399871826, -2.6025876998901367]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.0219877902418375\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. He\\'s the perfect person for an interview.\"', 'i really appreciate your kindness.\\n\\n(11/15) — David Karp (@dktkskarp) November 15, 2014\\n\\nI have found him by every name he called me in spite of his ex-girlfriend. He never', 'i really appreciate your kindness. Thanks very much, Mike.\"\\n\\n\"The person\\'s not getting a dime for not helping me. He told me his brother\\'s daughter called him every evening but she never really called my dad when he was getting home']\n",
            "rewards: [3.0786280160356503, 1.8419535593025413, 2.1187311970666], baseline: 2.3464375908015973\n",
            "log probabilities: [-3.6605639457702637, -3.0435030460357666, -3.4723286628723145]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.033921319991350174\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Thank you, sir.\"\\n\\n\"If you don\\'t,\" The Boss replied, his voice shaking.\\n\\n\"It doesn\\'t matter,\" the girl replied without a moment\\'s hesitation.\\n\\nJugg', 'could you please assist me with this? I am not as sure I am, but you certainly want me to do a bit more with what will help us make the game a much better one. I wish you the best of luck on getting your hands', 'could you please assist me with this? \"I feel sorry for her, because I can already hear the anguish in her voice. It must not have been easy, that she came. In the first place, she\\'s my mother; I\\'m afraid']\n",
            "rewards: [3.141795569058525, 1.9889173925629946, 1.8245253548312612], baseline: 2.3184127721509267\n",
            "log probabilities: [-2.325430154800415, -2.745074510574341, -2.7785634994506836]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026163561269640923\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.028490079566836357\n",
            "iteration 23:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, I did not think any person, except one, but, the Pope, would think any human being would think otherwise. I did not, to my astonishment, wonder if the Church would not have some say in it;', 'once upon a time, to my great relief, I must say, the latter part of my life was filled with an eager fascination with this ancient marvel… I cannot refrain from mentioning its beauty. In one place at last. To me this marvel,', \"once upon a time, and that it was in that regard so great in the world. We are here again to have a hearing to hear how it happened with regards to the Church's record concerning it. Now, how many times have all sides spoken\"]\n",
            "rewards: [-2.5562941804435555, 0.6434101187836541, 0.9208441287493833], baseline: -0.3306799776368394\n",
            "log probabilities: [-2.960015058517456, -3.4755032062530518, -3.3956351280212402]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.032770510762929916\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nHow were you doing?\\n\\nWhat exactly were you going to accomplish?\\n\\nI wanted to do something with those feelings. Just to get things out of the way, just to kind of get', \"hello, how are you doing today?\\n\\nWhat's not on your radar today: getting to the playoffs or doing better by improving your defensive numbers.\\n\\nWhat's your take on the recent preseason?\", \"hello, how are you doing today?\\n\\nWe're not going to be at this thing again until Sunday, so be on the lookout and keep that in mind!\\n\\nI want everybody to remember the good times we had as the team —\"]\n",
            "rewards: [1.667512928734114, 0.5137874080691536, 1.1196389425681672], baseline: 1.1003130931238114\n",
            "log probabilities: [-2.4219236373901367, -2.8434689044952393, -2.6990272998809814]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02654806524515152\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!', 'thank you for your help!\\n\\n1', \"thank you for your help! [3.00/20] Thanks to everyone for tuning out my rant. This means that today I'm leaving the rant up. However, I'm sure some readers are looking forward to talking about how I put my\"]\n",
            "rewards: [2.9846596613232586, 3.176232658685423, 3.0844551118031958], baseline: 3.0817824772706257\n",
            "log probabilities: [-2.5483052730560303, -2.3351664543151855, -3.127694845199585]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026703888550400734\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. I appreciate your caring attitude towards this group because, you know...it doesn't just happen, you go out and kill people, and that's how important it is to be around, it really is. And not only\", \"i really appreciate your kindness. If anything, he's always been an excellent human being... (Laughs)\\n\\n\\nThis video has gone viral\\n\\nYou must have been pretty curious to see where the person who made the video got all the money,\", 'i really appreciate your kindness.']\n",
            "rewards: [1.9373187332451192, 2.931726029666445, 3.0582875830971537], baseline: 2.642444115336239\n",
            "log probabilities: [-3.161846160888672, -3.16162109375, -4.8637542724609375]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03729073703289032\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? - You must have a very good excuse for me to write you a cheque. But of course if you wish to accept this offer of the first place you have the option of signing up and sending the che', 'could you please assist me with this? No matter what you decide, be aware that we do not expect our users to use our websites to report other customers using our services,\" Google states in an advisory on Twitter. \"The ability to do so is', \"could you please assist me with this?\\n\\nJACOB\\n\\nYou did it! I wanted that, I couldn't refuse it, for it's just what I needed from you.\\n\\nA large red sign and a black, black\"]\n",
            "rewards: [3.122645663828465, 2.096737907509055, 2.878758905610061], baseline: 2.6993808256491936\n",
            "log probabilities: [-3.083631753921509, -3.0525145530700684, -2.9344277381896973]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030235247686505318\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.03070968985557556\n",
            "iteration 24:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: [\"once upon a time, I've had the pleasure of talking with the folks with all sorts of people in the industry here about what drives some of them or the people with whom we share their work, what drives their ability to find work. Sometimes this\", 'once upon a time, the most successful entrepreneurs of all were in their early 40s.\\n\\nAnd in that same time frame, the typical entrepreneur is just as likely to grow his business if, for example, he makes small payments and gets into', \"once upon a time, but one that I remember fondly. If you know a little boy named Bill, who lives in New Brunswick, you know just about anything you can imagine. I'd give a thousand good jokes. They make you want more\"]\n",
            "rewards: [-1.686192169269387, 1.1302650456468246, 0.5723764658139633], baseline: 0.005483114063800272\n",
            "log probabilities: [-3.1504862308502197, -2.861154079437256, -3.200314998626709]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03070651739835739\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? Thanks, John! -- Tom\\n\\nIt's like, you do not understand why everybody does what I'm doing? Like this whole thing? And my wife does? It's not like it.\\n\\n\", 'hello, how are you doing today? Have you found a job well done this week? What made you decide it was time to change your approach and what the hell does your job need to be on a daily basis?\\n\\nYou could be surprised', \"hello, how are you doing today? Well... you are just doing it a long time ago. I didn't understand where you were coming from at the moment and you have no idea what you are doing... But it felt like I had come to\"]\n",
            "rewards: [2.114819245564074, 0.9474420391646483, -2.496587391497819], baseline: 0.18855796441030126\n",
            "log probabilities: [-3.4456164836883545, -2.66508412361145, -2.6093573570251465]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029066860675811768\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help!\\n\\nThank you for your help in translating and editing my first piece with Kami on April 30-June 17. I have been blessed to help by translating many people's stories, including myself. Kami shares stories\", 'thank you for your help!\\n\\n*Note: The \"Gyms\" in the game may contain affiliate links. See my disclosure policy for more details.', 'thank you for your help!\\n\\nFollow me on Twitter: @VNNewsMeter , facebook.com/VNNewsMeter & Instagram: @VNNewsMeter']\n",
            "rewards: [3.140708917725387, 2.103489867557187, 2.808909327700889], baseline: 2.6843693709944874\n",
            "log probabilities: [-3.1806752681732178, -2.4107022285461426, -1.956005573272705]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02515794336795807\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. In all the years of my life I've heard about this wonderful group and I've always loved them for their generosity, and they never let me down.\\n\\nNowadays, I know that these groups exist but don\", 'i really appreciate your kindness. A very sad day for me right here in Nashville,\" he wrote.\\n\\nThe post was quickly followed by a flurry of negative tweets from Nashville\\'s public figures, many of them seeking retribution. In particular, Nashville restaur', \"i really appreciate your kindness. I'd really recommend this shop! http://buddybee.com/\"]\n",
            "rewards: [1.8824545913524835, 2.116960323176106, 2.909033395732683], baseline: 2.302816103420424\n",
            "log probabilities: [-2.8210906982421875, -2.8605644702911377, -3.646512269973755]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031093891710042953\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?\\n\\n1\\n\\nI wish I could assist you! And help everyone. Thank you.\\n\\n2\\n\\nHello?\\n\\n3\\n\\nHi!\\n\\n4\\n\\nThis is your nickname', 'could you please assist me with this? Can we help it?\"\\n\\nBuckert answered: \"I could help you with these!\"\\n\\nSerendipity then began to laugh.\\n\\n\"Aye, he was my little', 'could you please assist me with this? It looks like the next-generation of Microsoft\\'s new Xbox One console will have similar graphics. Will you show us a picture of how it looks next to the Xbox One?\"\\n\\nMicrosoft responded by noting it']\n",
            "rewards: [2.9829780196971436, 3.195056845336779, 2.058098034187971], baseline: 2.7453776330739643\n",
            "log probabilities: [-2.246748924255371, -2.780034303665161, -2.796074628829956]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026076192036271095\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.028420281037688254\n",
            "iteration 25:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, one will realize how great and wonderful nature actually was and how it was so perfect in every way. It was an ideal world without a single evil being involved, where we could all live as best we could without destroying one another', 'once upon a time, in fact.) In the end, his plan was no worse than a fantasy — he built the illusion that a world populated entirely of men, for whom religion alone represented their most important role. But it worked anyway: The man', 'once upon a time, to our knowledge, in those cases where an alien from elsewhere does not wish to leave Earth, we should provide assistance,\" the spokesperson said in an email to VICE. \"Although we cannot tell you how many people they would send']\n",
            "rewards: [0.4282748804098839, 0.40907802302143936, -0.11648877265132165], baseline: 0.24028804359333386\n",
            "log probabilities: [-2.951885938644409, -3.661033868789673, -3.1216509342193604]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03244856745004654\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today?\\n\\nThe most important thing we learned from our last meeting was the lesson that people should listen, be accountable and do better in every situation possible in order to gain the best possible advantage to a happy life.', \"hello, how are you doing today? My head's spinning at work. I didn't have lunch today either, but we need an extra night. Do you have your keys or do you need a driver or a parking permit?\\n\\nI'm\", \"hello, how are you doing today?\\n\\n\\n[to John:] Well I have got great job, we've got good jobs, so thank you, everybody.\\n\\n\\nJOHN: Are you all right?\\n\\n\\nMEGY: I just\"]\n",
            "rewards: [1.6916185929267902, -1.6046225878750289, 2.969710761253439], baseline: 1.0189022554350669\n",
            "log probabilities: [-2.875516891479492, -2.869297504425049, -2.6201605796813965]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027883250266313553\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: [\"thank you for your help! It's easy and I don't expect to waste my time over this. And this post will get a LOT easier from the get go.\\n\\nLet me put things this way, here are some things to tell you\", 'thank you for your help!\\n\\nPlease email with your comment and story details and an update on the matter for us, so as to avoid any misunderstanding and to notify us and our clients of your support.\\n\\nFor further information please visit http', 'thank you for your help!\\n\\nThere are four key things you can do to make sure your project is well-received.\\n\\nIf the goal has been met, we will consider adding the new version for you to use\\n\\nThis code']\n",
            "rewards: [2.9570539205727138, 2.091561526958766, 2.976609560939414], baseline: 2.675075002823631\n",
            "log probabilities: [-2.9485361576080322, -2.8562493324279785, -2.390935182571411]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02731906808912754\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. I can hear my dear sister asking me what is wrong and she is all too aware that her heart is filled with hope, and that there was a good and peaceful place for her to see.\\n\\nThis time I', 'i really appreciate your kindness. He had only one choice. All he ever wanted to do was stay in a relationship. Which is what this beautiful, loving woman is! I really wanted him to stay where he was. So my family is making things', 'i really appreciate your kindness. He has always kept me updated on how she\\'s doing and he knows he has done just fine.\"\\n\\nI asked what she expects to be done by November. \"I\\'d just be out there running back,\" she']\n",
            "rewards: [2.9600673961206136, 3.1910641476532864, 1.9989119581882329], baseline: 2.716681167320711\n",
            "log probabilities: [-3.154499053955078, -3.3867900371551514, -3.210390567779541]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03250559791922569\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?\\n\\nYou were only joking when you said to have a better understanding of what a computer is.\" The girl shook her head. \"Do you understand!?\"\\n\\nAfter his expression turned a bit haughty', 'could you please assist me with this? Please provide any additional information or contact me in order to obtain your permission. Thank you.\"\\n\\n\"No, I don\\'t, actually I didn\\'t ask you to, I just wanted to know if my', 'could you please assist me with this? My brother who was going to give me this is the man who shot the child who gave me the news,\\n\\nmy brother was not my son, he had said to my brother\\n\\n\"I am']\n",
            "rewards: [3.142467913540334, 2.189771898096634, 2.02968407898373], baseline: 2.453974630206899\n",
            "log probabilities: [-2.8349099159240723, -2.3117589950561523, -3.023116111755371]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.027232617139816284\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02947782017290592\n",
            "iteration 26:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, not in the distant future, as was his dream.\" (Anecdotes of Adam Smith and the Wealth of Nations)\\n\\n(I would like to make a disclaimer, however -- I am in no way affiliated with', 'once upon a time, but there was no great change in the climate or land. When she took her way in time to her graduation, we were there for about 45 days. I was amazed.\"\\n\\nShe says that when she finally got back', 'once upon a time, but this one would be even more beautiful than before, I can say, no!\\n\\n[Note – I wrote in February 2013 the book\\'s second edition.]\\n\\nThe final chapter, \"You have come, but']\n",
            "rewards: [-2.5875337994894076, -2.507033943961333, 0.6511717300857345], baseline: -1.481132004455002\n",
            "log probabilities: [-2.495333671569824, -3.13679575920105, -3.3627567291259766]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029982954263687134\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: ['hello, how are you doing today? Do I have a hangover?\\n\\n\"I\\'ve got an amazing life to support! I\\'m so appreciative of everything.\" -- Michael Myers\\n\\nAdvertisement\\n\\n\\nAfter learning about his wife\\'s life', \"hello, how are you doing today?\\n\\nYou're feeling better today because of today's work here. Thanks for continuing on to this article. If you don't want to listen to what everyone knows in an interview right now, don't read\", 'hello, how are you doing today? And why are you so happy, do you think?\\n\\nSo you read the paper, you know what, and you\\'re like, \"Hear something really good, this just went very well,\" because']\n",
            "rewards: [2.0909716849710827, 2.0368702296305523, 1.1035410515448332], baseline: 1.7437943220488228\n",
            "log probabilities: [-2.906463623046875, -2.7545247077941895, -2.760434150695801]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028071409091353416\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! As well as being a huge contributor, Mr. Johnson was a great and supportive supporter, and it would be an honor to have him join us someday. I am not sure how long this partnership might last. I will', 'thank you for your help!', 'thank you for your help!\\n\\n[You might want to continue writing and updating on this section]']\n",
            "rewards: [2.1209225397290843, 2.8943450558653616, 2.961537431176671], baseline: 2.658935008923706\n",
            "log probabilities: [-2.758988618850708, -2.5483052730560303, -3.1470940113067627]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028181292116642\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness.\\n\\nI was very very excited when I found out I was doing all this with this guy. At some point, one of the first things I did was ask if the guy was up for a promotion. After that', 'i really appreciate your kindness.\\n\\nMortimer\\n\\nThe last thing you would want to do is ruin your chance with all these strangers in your neighborhood.', 'i really appreciate your kindness.\\n\\nWhat do you think you are doing? Let us know!\\n\\nCheck out our previous articles on the Best of the Comic Series and the Best Movies to Watch. To watch our show, sign up for the']\n",
            "rewards: [3.1643271864520472, 2.941574491766657, 1.8954298238861815], baseline: 2.6671105007016287\n",
            "log probabilities: [-2.827547073364258, -2.967562198638916, -2.675267219543457]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028234587982296944\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? Please allow a week, or 10 hours? After you know how, I won\\'t be sure about being able to go to their house to call them or to provide them a date.\" And then the guy asked', 'could you please assist me with this? Would she be able to get you out if she hadn\\'t come?\"\\n\\nAnd so the duo set off, without stopping when their footsteps halted abruptly.\\n\\nKira had already been driven across the river', 'could you please assist me with this?\\n\\nYou could make me pay back my debt to you. If not you would be stuck with an unpleasant sentence of five years. I can help you with this by using the app. In this case,']\n",
            "rewards: [2.0664784645639616, 3.1280920041337774, 2.0526710750361934], baseline: 2.4157471812446443\n",
            "log probabilities: [-3.431490182876587, -3.180427312850952, -2.848114013671875]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031533438712358475\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.029200736433267593\n",
            "iteration 27:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, in this time, there are those that are wise and those that are ignorant. And such a foolish man has his mind of darkness, and is like to lose the right to know the things that are out of the way by', \"once upon a time, and where we don't know, the best we can do, that we might give in, and I'll see, in the days to come, if the time is right, whether we've got, I can give my\", \"once upon a time, when they realized they had become a threat. When I was a kid that first time it was more real than ever.\\n\\nThe idea was that you'd go to a park, meet a bunch of guys and go to\"]\n",
            "rewards: [0.6551248789552226, 0.8085817296434427, 1.643849303186205], baseline: 1.0358519705949567\n",
            "log probabilities: [-2.922269582748413, -2.9026947021484375, -2.8277640342712402]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02884242683649063\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nRUBIO: Great! I'm very happy and very relaxed for this afternoon, and I am looking forward to working with my teammates, with friends and family over the coming day.\\n\\n\", 'hello, how are you doing today? And with your sister, why\\'re you sleeping well? Because of what the little lady went through...\" \"...And then the little man was caught and taken and raped by three different men and then the whole thing spread', 'hello, how are you doing today? What have you been like since you have started learning Arabic? What lessons were you able to learn during those few days of learning?\"\\n\\n\\n[laughs]\\n\\n[laughs]\\n\\n\\n[Sue sounds']\n",
            "rewards: [1.0859401299232245, -1.6792874293231312, 0.992050165868064], baseline: 0.13290095548938577\n",
            "log probabilities: [-2.502718687057495, -3.613781452178955, -2.634183645248413]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02916894666850567\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!', 'thank you for your help!\\n\\n\\nPlease share this story with friends and family!! Share this page\\n\\n\\nFacebook\\n\\nTwitter\\n\\nPinterest\\n\\nGoogle\\n\\nTumblr\\n\\nLinkedIn\\n\\nReddit\\n\\nTumblr\\n\\n\\nMore\\n\\n\\nPinterest', 'thank you for your help! It took just about 45 minutes to write the entire post, which had already started as written in the next post on it — we decided to keep making the update on top of the page, so it would be easier to']\n",
            "rewards: [2.84554156214274, 2.9299389214365936, 3.0689730122819947], baseline: 2.948151165287109\n",
            "log probabilities: [-2.5483052730560303, -1.6603044271469116, -3.1922037601470947]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.024669378995895386\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness.\\n\\n[Edit: Thanks for that suggestion that I've missed. Thanks for reading all of this!]\\n\\n—\\n\\nThe new and improved versions of the site are available on a free trial for all to check\", 'i really appreciate your kindness. There are just so many things you can do and I wish you that more people would come up and join us in celebrating.', \"i really appreciate your kindness.\\n\\nJanna's friends and family came to visit on Tuesday to see if they would be able to help Janna get home.\\n\\nTara Stacey is the co-author of the forthcoming book, When\"]\n",
            "rewards: [3.0300227402157573, 2.9925692535729613, 2.900731529072646], baseline: 2.9744411742871217\n",
            "log probabilities: [-2.969900369644165, -2.960080862045288, -2.8239552974700928]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.029179789125919342\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: [\"could you please assist me with this? I feel you've been having problems and that we are unable to fix you? Should we try another therapy session with your family? You have to work through this process. I appreciate that and it's what I\", 'could you please assist me with this?\\n\\n\"Yes! Yes!\" I muttered; my mouth curled and I tried to think of what to do as fast as possible, but the answer I got was a muffled sob when she pushed one hand', 'could you please assist me with this? In this way the game of a thousand different scenarios is played, there is no reason not to do that which has been the way of the past.\\n\\nAs we enter it from the early stage with the']\n",
            "rewards: [1.9740849737979163, 3.171932926659149, 1.8779054586809765], baseline: 2.3413077863793474\n",
            "log probabilities: [-3.1088521480560303, -2.9832923412323, -3.3479857444763184]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03146710246801376\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.028665528818964957\n",
            "iteration 28:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, the earth was as strong as snow, and all was quiet.\\n\\nThus came the Godfather and His son Peter and all the sons of Israel together under the dominion of the God of Israel.\\n\\nI must point', 'once upon a time, before you know it; in times of trouble they would stand alone, but those of you who seek to understand and keep the laws, can see them for yourself!\"\\n\\n\"For my money,\" said she. \"I', 'once upon a time, with the benefit of a future, and as our past is not only limited to these two past events yet we can use the future of these past events to give a good present/present relationship with the past that brings about an']\n",
            "rewards: [0.3880047867836859, -2.348358171635618, -2.3602568636594996], baseline: -1.4402034161704773\n",
            "log probabilities: [-2.839580535888672, -3.1535675525665283, -3.3947062492370605]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03129284828901291\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nHow did you sleep?\\n\\nWho is making breakfast?\\n\\nCan I do pizza tonight?\\n\\nIf you can't sleep, what are you missing?\\n\\nAre there any other tasks\", \"hello, how are you doing today?\\n\\nD'Angelo: Oh, what we're having fun doing now. The two of us have been on different projects together but it's been so nice meeting people. We did a short game in Chicago\", \"hello, how are you doing today?\\n\\nIt was like I was in an airplane, so the airplane didn't know what I was talking about. I did all that I could, and I got the rest of my papers in. I had\"]\n",
            "rewards: [-2.3697621352067566, 0.1640223213739122, -2.579818216998863], baseline: -1.5951860102772357\n",
            "log probabilities: [-2.2371585369110107, -2.8698763847351074, -2.4830291271209717]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02530021406710148\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nThank you all for doing what everyone can do. I love making stuff! 🙂\\n\\nAs I have said previously on reddit, if you feel the need for us, we can always help.\\n\\nThis', 'thank you for your help!', 'thank you for your help!\\n\\nPlease update your email address and we will put your name up in chat. You will get a reminder in 5 or so hours.\\n\\n\\nHello! This is one of the biggest things I want to do in']\n",
            "rewards: [2.982163086462398, 2.983737203028149, 2.9758341900165712], baseline: 2.9805781598357064\n",
            "log probabilities: [-2.773789405822754, -2.5483052730560303, -2.6170268058776855]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026463737711310387\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. If we had to have to be the new pope of my family, which is all I know! I know. This pope loves me. He's my new pope. My little boy. Don't worry about it.\", 'i really appreciate your kindness.', 'i really appreciate your kindness.\\n\\nD-8\\n\\nGarrett \"Shaun\" Schulman was the person who decided to do all of this to try and protect her and take responsibility for all his actions. He has no interest in']\n",
            "rewards: [2.0698403553136044, 3.0147906478904054, 2.013834582307726], baseline: 2.366155195170579\n",
            "log probabilities: [-3.3804707527160645, -4.8637542724609375, -3.3277926445007324]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.03857339173555374\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this? What if I told you there was still so much to look forward to that day when we would finally meet?\"\\n\\n\"If you had just let me talk, I would not have gone that far.\"\\n', 'could you please assist me with this?', 'could you please assist me with this? Thanks!\"\\n\\nHua, who looked extremely anxious, nodded and said, \"I have many good items you can purchase, please wait. I will be arriving in fifteen minutes, I shall wait on you']\n",
            "rewards: [1.8546737544932095, 2.8169656414887068, 1.8688672997769336], baseline: 2.1801688985862833\n",
            "log probabilities: [-2.4013519287109375, -3.1072323322296143, -2.906244993209839]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02804943174123764\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02993592470884323\n",
            "iteration 29:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, it will be all wrong. However, we shall not perish from this life. For in it will come the resurrection. We live together in God. Through his Holy Spirit and the Word of God (Hebrewly speaking),', 'once upon a time, of a different type. We are living at different times, each of the two time periods. We are still in a way that is unique and has a unique significance and has a connection to the past. I feel very happy', 'once upon a time, which was the final day of the Great War of 1812-14. With the French surrender in 1812-14, which had started a whole new era of peace and stability, the Americans had lost an enormous amount of']\n",
            "rewards: [-2.643960229683195, 1.6871905269001364, -0.16127583970579604], baseline: -0.3726818474962849\n",
            "log probabilities: [-2.977179765701294, -2.968709945678711, -2.683929204940796]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02876606397330761\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today?\\n\\nTravis: I'm doing pretty good at work and all of a sudden I'm going in an airport and all of a sudden all hell breaks loose. And I'm like, I hope you've\", 'hello, how are you doing today? You want to go to the mall?\" I said to him. We headed for the mall.\\n\\nAnd I was standing at the front, so his name was Mark Dreyer of the Washington Redskins.', \"hello, how are you doing today?\\n\\nWe're getting a lot easier with new features like iCloud photo search. But the main focus is still in doing our best to stay ahead of Microsoft on Windows Phone. We are looking to provide better experiences\"]\n",
            "rewards: [1.0291258785356523, -1.5498494056742, 1.3975569717782772], baseline: 0.29227781487990984\n",
            "log probabilities: [-2.3635294437408447, -2.8074231147766113, -2.9004626274108887]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026904717087745667\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help! We would really appreciate it if you could give us your name or have contact information so that we could assist you in organizing any of the free materials needed to organize events on this site. Please e-mail info@k', 'thank you for your help!\\n\\nIt took hours of digging through a large amount of documents. My guess is it will take several more people to finish things out for me. Also a lot of work for sure, but I hope it gets me', \"thank you for your help! Don't miss any of my posts on the Bellyaches Forums, I hope you'll go through and read my articles.\\n\\nThe Bellyaches Blog\\n\\nThe Bellyaches Batch Backs are available\"]\n",
            "rewards: [3.153894824744419, 2.945964248042072, 3.058388033989619], baseline: 3.0527490355920364\n",
            "log probabilities: [-2.446507453918457, -2.8326117992401123, -2.648245334625244]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.026424549520015717\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: ['i really appreciate your kindness. If only there was an easy way around the worst issues of my life (like having to wait for 5-8 months on demand after a hard work weekend). After I put this on the shelves a couple weeks ago,', 'i really appreciate your kindness. I\\'d never have thought to do that to a girl with a body that\\'s like that, with the way she wears it.\" \"So I thought…\" He shook his head in relief and then, \"Wow… oh', \"i really appreciate your kindness.\\n\\nI'm not saying I think he should feel offended - I love him so much... I love his family. But at the same time I also believe that it was a poor decision to go on public service.\"]\n",
            "rewards: [2.8870401523701785, 1.800666608503586, 1.9540005231034896], baseline: 2.213902427992418\n",
            "log probabilities: [-3.457261800765991, -3.2257273197174072, -2.9556429386138916]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.032128773629665375\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?\\n\\nQ - What is the best way for you to get that data.\\n\\nA - You have to first download something like RARAM in your own location, and you must pay a price in', \"could you please assist me with this?\\n\\n\\nYou can only do so much until the last piece of the puzzle. Don't get in my way. There's only so much you can get in this place before me.\\n\\n\\nIf you do\", 'could you please assist me with this?\\n\\n\\nAs for your request, it is simply for sharing. I want to share this. However if possible, please upload or post on this subreddit.']\n",
            "rewards: [2.875878610173781, 2.869823762846549, 2.8534438315114827], baseline: 2.866382068177271\n",
            "log probabilities: [-3.0813655853271484, -2.379243850708008, -3.1377604007720947]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.028661232441663742\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02857706733047962\n",
            "iteration 30:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "generated completions: ['once upon a time, they met together from one place and gave speeches and then went home to meet. There was no doubt that these were great men, and they could be considered part of the story. But it has since come in that as for', 'once upon a time, when the government began to consider, as it often did, an attack on the constitutional order of their country, the question of national neutrality became a point of concern. The issue was framed and debated among two parties: those who', 'once upon a time, a second time on an empty field is the greatest hope you have of having a winning season. Then go win it all over again in November.\"\\n\\nI don\\'t buy all that. I want to see a new,']\n",
            "rewards: [0.8960126729867678, -1.6081468410559414, 0.3923773031534083], baseline: -0.10658562163858844\n",
            "log probabilities: [-3.385295867919922, -2.8937864303588867, -3.038612127304077]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.031058982014656067\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "generated completions: [\"hello, how are you doing today? So far I've done 10-10 of them, but I did it over 5 minutes. I didn't have much idea how I could do it before but after doing that I'm doing 20,000 reps\", 'hello, how are you doing today?\\n\\n\"That is a big ask, but, for those of you that read some other articles online, you know, when you look through those blogs you will notice there really has been some great stuff for', 'hello, how are you doing today? Where\\'s the rest of his family from? What are you looking for this week?\"\\n\\nIt got a little odd.\\n\\n\"He\\'s pretty happy right now,\" Weiss said with an air of concern']\n",
            "rewards: [1.0523144851820563, 0.3194329517042863, 0.055267457637835526], baseline: 0.4756716315080594\n",
            "log probabilities: [-3.0011146068573, -3.0428683757781982, -2.6377217769622803]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02893901616334915\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "generated completions: ['thank you for your help!\\n\\nIf you have any questions or issues with the site please do NOT hesitate to contact me online, it is always nice to hear from everyone!', 'thank you for your help! There will be no \"free money\" in the future! Thank you for your help! The current version of the project does include an important feature. The name \"punch\" can be checked for you from the main', 'thank you for your help!']\n",
            "rewards: [1.9098345546535043, 1.890124953054746, 3.0756437496711344], baseline: 2.291867752459795\n",
            "log probabilities: [-2.1197471618652344, -2.9697721004486084, -2.5483052730560303]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.02545941434800625\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "generated completions: [\"i really appreciate your kindness. I'm sure that your gift will be appreciated by others too and that we can have a lovely couple together together.\\n\\nAnd my dear, so I just thought I had all of you down again. So there you\", \"i really appreciate your kindness. For instance: a beautiful lady in a flowerbed who didn't dare bring her children under her wing, and she came to tell me not to see them. Her family said not to have them, and they didn't\", 'i really appreciate your kindness.\\n\\nThis post first appeared at www.truenews.com\\n\\nRead or Share this story: https://cin.ci/2i7FzcXa']\n",
            "rewards: [3.131484179374063, 2.176358498987463, 2.8970432237524735], baseline: 2.7349619673713335\n",
            "log probabilities: [-3.168627977371216, -3.3931539058685303, -2.4720804691314697]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030112873762845993\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "generated completions: ['could you please assist me with this?\\n\\nIt appears to be a problem between two different phones (not the HTC One and the one we currently have). And, from my test case that one of the phones is an LG phone, I have', 'could you please assist me with this? As of yet I am in need of money to buy a new pair of pants. I should know about your expenses soon, my name is Zuze. I had a call for you while I was sleeping', 'could you please assist me with this? I will probably spend more money in different parts of Europe,\" said Mavrodi.\\n\\nThe Dutch and British foreign service were unable to reach one another. Mavrodi told the Guardian that his']\n",
            "rewards: [1.8465395376877463, 1.8739611135333927, 2.0525245951620414], baseline: 1.9243417487943935\n",
            "log probabilities: [-3.1084439754486084, -2.9692630767822266, -2.9362149238586426]\n",
            "computed advantage: 0.01\n",
            "computed loss: 0.030046407133340836\n",
            "--------------------------------------------------\n",
            "updated model. average loss: 0.02912333868443966\n",
            "new generated completion: once upon a time, that the first two generations to arouse the greatest spirit of love have fallen in the manner of a fall; and I pray and pray that this great spirit which I share, and I testify this to the whole of you by\n",
            "new reward score: 1.97042514646537\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('once upon a time, that the first two generations to arouse the greatest spirit of love have fallen in the manner of a fall; and I pray and pray that this great spirit which I share, and I testify this to the whole of you by',\n",
              " 1.97042514646537)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 11: evaluating the updated model\n",
        "def evaluate_model(prompt):\n",
        "    new_completion = generate_multiple_completions(prompt, num_samples=1)[0]\n",
        "    new_reward = reward_function(new_completion)\n",
        "    print(\"new generated completion:\", new_completion)\n",
        "    print(\"new reward score:\", new_reward)\n",
        "    return new_completion, new_reward\n",
        "\n",
        "# example usage\n",
        "evaluate_model(\"once upon a time,\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBm3hrPzBq05",
        "outputId": "2cde195d-a314-4322-837c-489f59042a88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new generated completion: once upon a time, we do not know whether the human brain is the source or the repository of the entire experience or whether, after the events of certain historical periods, it is always the same: the individual feeling, the instinct; the process of\n",
            "new reward score: 0.135034022921793\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('once upon a time, we do not know whether the human brain is the source or the repository of the entire experience or whether, after the events of certain historical periods, it is always the same: the individual feeling, the instinct; the process of',\n",
              " 0.135034022921793)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepSeek moodel"
      ],
      "metadata": {
        "id": "O653FE0_HC6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "8393355c763444e2a4ad11a8820cdd33",
            "1d0b1aff34b74019b70f90b66c9588bb",
            "455f299aaed74753b5c83505617e3609",
            "6f8e774158684727b57b9ae52f8bc484",
            "07b09fb116fd4b25b48388c546139f5a",
            "d1da4b4bc0f042ab91e5fd31e389a684",
            "a221212890c14fa793058a128bd53d7e",
            "faaec22102604c1caee9ff31b6ff2387",
            "978fc56b415744fea2e111b6211db7dd",
            "42bf7d1f74ea4227912d92bb25985278",
            "4ac58c7807e94ac9ad799f288b553ff7",
            "7215aedf9b7e4329895819b015ba9b5b",
            "10c476023a3b470c9fbb9bd1945f79f8",
            "3bcc0c8c9eae40aa87b61e342fa59fd7",
            "915ff8e296c747c7b42118a047585381",
            "6d0ab316adc645898ecb8c566d0fdbce",
            "9d53afc496a048fdbb257776200d46a8",
            "8818243bc84046e296f49ab7f67c099e",
            "0c10e5d789194617ade9616dea7d4eef",
            "f9a70c6253fc490bbbde46995adafbca"
          ]
        },
        "id": "CN6cU3uWG98Y",
        "outputId": "f8c931b6-2e8d-410d-c565-341450a573ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8393355c763444e2a4ad11a8820cdd33"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: install required libraries\n",
        "#!pip install transformers torch textblob bitsandbytes\n",
        "\n",
        "# step 2: import required modules\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from textblob import TextBlob\n",
        "\n",
        "# step 3: configure quantization and load model\n",
        "# Create a simple 8-bit quantization config\n",
        "quantization_config = {\n",
        "    \"quant_method\": \"bitsandbytes_8bit\"\n",
        "}\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/DeepSeek-R1\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,  # Enable 8-bit quantization directly\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "# define pad token explicitly to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Rest of your RLHF implementation remains the same...\n",
        "def generate_multiple_completions(prompt, num_samples=3, max_length=50):\n",
        "    completions = []\n",
        "    for _ in range(num_samples):\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        attention_mask = input_ids.ne(tokenizer.pad_token_id).long()\n",
        "        output = policy_model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=40,\n",
        "            top_p=0.92,\n",
        "            temperature=1.5\n",
        "        )\n",
        "        completions.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "    return completions\n",
        "\n",
        "\n",
        "# step 5: refining the reward function with stronger penalization\n",
        "def reward_function(text):\n",
        "    \"\"\"\n",
        "    a refined reward function that assigns a progressive score based on politeness and sentiment intensity,\n",
        "    with a stronger penalty for negative words.\n",
        "    \"\"\"\n",
        "    polite_phrases = [\"thank you\", \"please\", \"kindly\", \"appreciate\", \"grateful\", \"respect\", \"much obliged\", \"thanks\"]\n",
        "    negative_words = [\"no\", \"not\", \"never\", \"can't\", \"won't\", \"bad\", \"sad\", \"problem\", \"worse\"]\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "\n",
        "    # progressive scoring\n",
        "    if any(phrase in text.lower() for phrase in polite_phrases):\n",
        "        reward = 3.0  # increased reward for strong politeness\n",
        "    elif sentiment > 0.5:\n",
        "        reward = 2.0  # strong positive sentiment\n",
        "    elif sentiment > 0.2:\n",
        "        reward = 1.5  # moderate positive sentiment\n",
        "    elif sentiment > 0.0:\n",
        "        reward = 1.0  # slight positive sentiment\n",
        "    elif sentiment < -0.5:\n",
        "        reward = -3.0  # increased penalty for highly negative sentiment\n",
        "    else:\n",
        "        reward = -1.5  # default penalty for neutral or slightly negative sentiment\n",
        "\n",
        "    # additional penalty for explicit negative words\n",
        "    if any(word in text.lower() for word in negative_words):\n",
        "        reward -= 1.0\n",
        "\n",
        "    # add small noise to ensure variation in rewards\n",
        "    return reward + random.uniform(-0.2, 0.2)\n",
        "\n",
        "# step 6: computing log probabilities for the generated text\n",
        "def compute_log_probs(model, input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    log_probs = -outputs.loss.item()\n",
        "    return log_probs\n",
        "\n",
        "# step 7: improving advantage computation to amplify differences\n",
        "def compute_advantage(reward, reward_baseline=0.0, epsilon=1e-6):\n",
        "    advantage = (reward - reward_baseline) / (abs(reward_baseline) + epsilon)\n",
        "    return max(advantage, 1e-2)  # prevent zero advantage values\n",
        "\n",
        "# step 8: computing the policy gradient loss\n",
        "def compute_policy_gradient_loss(log_prob, advantage):\n",
        "    return torch.tensor(-log_prob * advantage, requires_grad=True)\n",
        "\n",
        "# step 9: updating the model using gradient descent\n",
        "def update_model(model, loss, learning_rate=1e-5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# step 10: training loop with debugging outputs\n",
        "def train_model(model, prompts, iterations=5, num_samples=3):\n",
        "    \"\"\"\n",
        "    trains the model on multiple prompts and multiple generated responses per prompt using progressive rewards and normalized advantage.\n",
        "    \"\"\"\n",
        "    for i in range(iterations):\n",
        "        print(f\"iteration {i+1}:\")\n",
        "        total_loss = 0\n",
        "        reward_baseline = 0  # baseline initialized per iteration\n",
        "        for prompt in prompts:\n",
        "            completions = generate_multiple_completions(prompt, num_samples)\n",
        "            rewards = [reward_function(c) for c in completions]\n",
        "            reward_baseline = sum(rewards) / len(rewards)  # update baseline dynamically\n",
        "            log_probs = [compute_log_probs(model, c) for c in completions]\n",
        "            avg_reward = sum(rewards) / len(rewards)\n",
        "            avg_log_prob = sum(log_probs) / len(log_probs)\n",
        "            advantage = compute_advantage(avg_reward, reward_baseline)\n",
        "            loss = compute_policy_gradient_loss(avg_log_prob, advantage)\n",
        "            update_model(model, loss)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # debugging outputs\n",
        "            print(f\"prompt: {prompt}\")\n",
        "            print(f\"generated completions: {completions}\")\n",
        "            print(f\"rewards: {rewards}, baseline: {reward_baseline}\")\n",
        "            print(f\"log probabilities: {log_probs}\")\n",
        "            print(f\"computed advantage: {advantage}\")\n",
        "            print(f\"computed loss: {loss.item()}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        print(f\"updated model. average loss: {total_loss / len(prompts)}\")\n",
        "\n",
        "# example usage\n",
        "training_prompts = [\n",
        "    \"once upon a time,\",\n",
        "    \"hello, how are you doing today?\",\n",
        "    \"thank you for your help!\",\n",
        "    \"i really appreciate your kindness.\",\n",
        "    \"could you please assist me with this?\"\n",
        "]\n",
        "train_model(policy_model, training_prompts, iterations=30)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5Tleav8G-W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 11: evaluating the updated model\n",
        "def evaluate_model(prompt):\n",
        "    new_completion = generate_multiple_completions(prompt, num_samples=1)[0]\n",
        "    new_reward = reward_function(new_completion)\n",
        "    print(\"new generated completion:\", new_completion)\n",
        "    print(\"new reward score:\", new_reward)\n",
        "    return new_completion, new_reward\n",
        "\n",
        "# example usage\n",
        "evaluate_model(\"once upon a time,\")"
      ],
      "metadata": {
        "id": "PoNAobDxHWES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOxBEUw_PrrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: install required libraries\n",
        "# before we begin, install the necessary libraries if they are not already installed.\n",
        "#!pip install transformers torch textblob\n",
        "\n",
        "# step 2: import required modules\n",
        "import torch\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from textblob import TextBlob\n",
        "\n",
        "# step 3: load pretrained model and tokenizer\n",
        "# using gpt2-medium as our base model for rlhf\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "policy_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "# move model to the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_model.to(device)\n",
        "\n",
        "# define pad token explicitly to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# step 4: generate multiple sample completions\n",
        "def generate_multiple_completions(prompt, num_samples=3, max_length=50):\n",
        "    completions = []\n",
        "    for _ in range(num_samples):\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "        attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
        "        output = policy_model.generate(\n",
        "            input_ids.to(device),\n",
        "            attention_mask=attention_mask.to(device),\n",
        "            max_length=max_length,\n",
        "            do_sample=True,\n",
        "            top_k=40,  # slightly reduced for better control\n",
        "            top_p=0.92,  # slight reduction to prevent extreme randomness\n",
        "            temperature=1.5  # further increase exploration\n",
        "        )\n",
        "        completions.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "    return completions\n",
        "\n",
        "# step 5: refining the reward function with stronger penalization\n",
        "def reward_function(text):\n",
        "    \"\"\"\n",
        "    a refined reward function that assigns a progressive score based on politeness and sentiment intensity,\n",
        "    with a stronger penalty for negative words.\n",
        "    \"\"\"\n",
        "    polite_phrases = [\"thank you\", \"please\", \"kindly\", \"appreciate\", \"grateful\", \"respect\", \"much obliged\", \"thanks\"]\n",
        "    negative_words = [\"no\", \"not\", \"never\", \"can't\", \"won't\", \"bad\", \"sad\", \"problem\", \"worse\"]\n",
        "    sentiment = TextBlob(text).sentiment.polarity\n",
        "\n",
        "    # progressive scoring\n",
        "    if any(phrase in text.lower() for phrase in polite_phrases):\n",
        "        reward = 3.0  # increased reward for strong politeness\n",
        "    elif sentiment > 0.5:\n",
        "        reward = 2.0  # strong positive sentiment\n",
        "    elif sentiment > 0.2:\n",
        "        reward = 1.5  # moderate positive sentiment\n",
        "    elif sentiment > 0.0:\n",
        "        reward = 1.0  # slight positive sentiment\n",
        "    elif sentiment < -0.5:\n",
        "        reward = -3.0  # increased penalty for highly negative sentiment\n",
        "    else:\n",
        "        reward = -1.5  # default penalty for neutral or slightly negative sentiment\n",
        "\n",
        "    # additional penalty for explicit negative words\n",
        "    if any(word in text.lower() for word in negative_words):\n",
        "        reward -= 1.0\n",
        "\n",
        "    # add small noise to ensure variation in rewards\n",
        "    return reward + random.uniform(-0.2, 0.2)\n",
        "\n",
        "# step 6: computing log probabilities for the generated text\n",
        "def compute_log_probs(model, input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids.to(device), labels=input_ids.to(device))\n",
        "    log_probs = -outputs.loss.item()\n",
        "    return log_probs\n",
        "\n",
        "# step 7: improving advantage computation to amplify differences\n",
        "def compute_advantage(reward, reward_baseline=0.0, epsilon=1e-6):\n",
        "    advantage = (reward - reward_baseline) / (abs(reward_baseline) + epsilon)\n",
        "    return max(advantage, 1e-2)  # prevent zero advantage values\n",
        "\n",
        "# step 8: computing the policy gradient loss\n",
        "def compute_policy_gradient_loss(log_prob, advantage):\n",
        "    return -log_prob * advantage\n",
        "\n",
        "# step 9: updating the model using gradient descent\n",
        "def update_model(model, loss, learning_rate=1e-5):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# step 10: training loop with debugging outputs\n",
        "def train_model(model, prompts, iterations=5, num_samples=3):\n",
        "    \"\"\"\n",
        "    trains the model on multiple prompts and multiple generated responses per prompt\n",
        "    using progressive rewards and normalized advantage.\n",
        "    \"\"\"\n",
        "    for i in range(iterations):\n",
        "        print(f\"iteration {i+1}:\")\n",
        "        total_loss = 0\n",
        "        reward_baseline = 0  # baseline initialized per iteration\n",
        "\n",
        "        for prompt in prompts:\n",
        "            completions = generate_multiple_completions(prompt, num_samples)\n",
        "            rewards = [reward_function(c) for c in completions]\n",
        "            reward_baseline = sum(rewards) / len(rewards)  # update baseline dynamically\n",
        "\n",
        "            for completion, reward in zip(completions, rewards):\n",
        "                log_prob = compute_log_probs(model, completion)\n",
        "                advantage = compute_advantage(reward, reward_baseline)\n",
        "                loss = compute_policy_gradient_loss(log_prob, advantage)\n",
        "\n",
        "                if isinstance(loss, torch.Tensor):\n",
        "                    update_model(model, loss)\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                # debugging outputs\n",
        "                print(f\"prompt: {prompt}\")\n",
        "                print(f\"completion: {completion}\")\n",
        "                print(f\"reward: {reward:.3f}\")\n",
        "                print(f\"log probability: {log_prob:.3f}\")\n",
        "                print(f\"advantage: {advantage:.3f}\")\n",
        "                print(f\"loss: {loss if isinstance(loss, float) else loss.item():.3f}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        avg_loss = total_loss / (len(prompts) * num_samples)\n",
        "        print(f\"average loss for iteration {i+1}: {avg_loss:.3f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# example usage\n",
        "training_prompts = [\n",
        "    \"once upon a time,\",\n",
        "    \"hello, how are you doing today?\",\n",
        "    \"thank you for your help!\",\n",
        "    \"i really appreciate your kindness.\",\n",
        "    \"could you please assist me with this?\"\n",
        "]\n",
        "\n",
        "# train the model\n",
        "train_model(policy_model, training_prompts, iterations=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzVrm2I1Pr5l",
        "outputId": "f2bd30f3-b1d3-4b15-cf4c-f30262c3834f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "completion: once upon a time, there was once a guy in a big hotel in New York, which is now gone in the suburbs -- or I forget where but it was probably where this place now is. He was wearing something that looked sort of like a\n",
            "reward: -0.056\n",
            "log probability: -2.952\n",
            "advantage: 0.010\n",
            "loss: 0.030\n",
            "--------------------------------------------------\n",
            "prompt: once upon a time,\n",
            "completion: once upon a time, I was at school as a very popular student, with wonderful ideas and wonderful talents. But, after four of my six months of teaching, they suddenly decided to pull a quick change,\" Dr. Ebert says, speaking about\n",
            "reward: 2.016\n",
            "log probability: -3.416\n",
            "advantage: 13.310\n",
            "loss: 45.464\n",
            "--------------------------------------------------\n",
            "prompt: once upon a time,\n",
            "completion: once upon a time, the Church said it was time for women and men everywhere to begin to think critically about who they love—that all the things men were thinking and seeing would ultimately lead to their spiritual ruin. That the end of sex had just\n",
            "reward: -1.537\n",
            "log probability: -3.364\n",
            "advantage: 0.010\n",
            "loss: 0.034\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today?)\n",
            "\n",
            "PAL (not happy with my friend!)\n",
            "\n",
            "(He says hi and pats me on the shoulder.)\n",
            "\n",
            "(And the story continues in the background: \"When he found out\n",
            "reward: -2.549\n",
            "log probability: -2.837\n",
            "advantage: 0.010\n",
            "loss: 0.028\n",
            "--------------------------------------------------\n",
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today? Good, just a little worried, but I need to check up on our little fellow.\" (I'll let you know. I will send you to his room after he returns from work…) She looked up to\n",
            "reward: 0.151\n",
            "log probability: -2.927\n",
            "advantage: 1.305\n",
            "loss: 3.820\n",
            "--------------------------------------------------\n",
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today? I love how you have kept a distance and look so...\n",
            "\n",
            "Hello: Ohhh. I think this could work well as a joke if I ever catch you wearing a thong while walking down the street\n",
            "reward: 0.913\n",
            "log probability: -3.135\n",
            "advantage: 2.842\n",
            "loss: 8.911\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "completion: thank you for your help!\n",
            "\n",
            "A: I know a great number of families use these services and they're free! But if anyone has problems, you might be able to solve those issues or find an affordable alternative.\n",
            "\n",
            "Q: How\n",
            "reward: 2.144\n",
            "log probability: -2.700\n",
            "advantage: 0.010\n",
            "loss: 0.027\n",
            "--------------------------------------------------\n",
            "prompt: thank you for your help!\n",
            "completion: thank you for your help!\n",
            "\n",
            "For those of you running iOS 6 you'll find some neat features to do things you never realized possible in older versions. Here are some key changes:You may know by this point of using your iPhone but for\n",
            "reward: 2.130\n",
            "log probability: -3.158\n",
            "advantage: 0.010\n",
            "loss: 0.032\n",
            "--------------------------------------------------\n",
            "prompt: thank you for your help!\n",
            "completion: thank you for your help!\n",
            "reward: 2.967\n",
            "log probability: -1.933\n",
            "advantage: 0.229\n",
            "loss: 0.443\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness.\n",
            "\n",
            "\"But as if on cue… my wife said \"I don't love you for taking me so easy.\"\n",
            "\n",
            "(she starts getting nervous, which I guess is the case for my husband!)\n",
            "\n",
            "\n",
            "reward: 2.880\n",
            "log probability: -3.554\n",
            "advantage: 0.089\n",
            "loss: 0.316\n",
            "--------------------------------------------------\n",
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness.\n",
            "reward: 3.044\n",
            "log probability: -5.208\n",
            "advantage: 0.151\n",
            "loss: 0.786\n",
            "--------------------------------------------------\n",
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. If anything goes wrong I won't forget you, and thank you very much.\"\n",
            "\n",
            "I gave a long bow and said \"That really was very nice and helpful.\" I really thought about it but, I think I\n",
            "reward: 2.011\n",
            "log probability: -3.077\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? Please? I need them all.\"\n",
            "\n",
            "Duke's last two words hit so bad that Elsa knew, almost to tears, that she knew everything was going to shit when she walked out of her house without\n",
            "reward: 1.913\n",
            "log probability: -3.143\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n",
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? Do you have anything to report?\" I had to laugh to hear his voice. Even he is very polite when asking me if there could be any trouble. \"There wasn't something on you at all! It\n",
            "reward: 3.190\n",
            "log probability: -3.080\n",
            "advantage: 0.181\n",
            "loss: 0.556\n",
            "--------------------------------------------------\n",
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? -Neko\n",
            "\n",
            "RAW Paste Data\n",
            "\n",
            "Neko: What do you really want? You told me that when you were young. Why would you need this body at some point!? -Don\n",
            "reward: 3.004\n",
            "log probability: -2.619\n",
            "advantage: 0.112\n",
            "loss: 0.292\n",
            "--------------------------------------------------\n",
            "average loss for iteration 1: 0.000\n",
            "================================================================================\n",
            "iteration 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "completion: once upon a time, and now it was, and is,\" he wrote.\n",
            "\n",
            "Kirby's \"new face became like an enormous cloud and it moved towards my window and threw it open and out into the street,\" McWilliams wrote.\n",
            "reward: 0.037\n",
            "log probability: -3.265\n",
            "advantage: 0.010\n",
            "loss: 0.033\n",
            "--------------------------------------------------\n",
            "prompt: once upon a time,\n",
            "completion: once upon a time, we've been able to provide our residents with affordable food options in abundance, even with all our high-demand areas already in existence, with such a limited number of available ingredients and methods of making each item, such as our\n",
            "reward: 0.852\n",
            "log probability: -3.242\n",
            "advantage: 0.031\n",
            "loss: 0.101\n",
            "--------------------------------------------------\n",
            "prompt: once upon a time,\n",
            "completion: once upon a time, during the first century B.C., and it continues to be practiced today\". We learn in the Bible (2 Timothy 5) as well that people lived from dawn to sunset, and were \"pregnant from the very\n",
            "reward: 1.589\n",
            "log probability: -2.800\n",
            "advantage: 0.924\n",
            "loss: 2.587\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today? Is there still no breakfast ready? Are we still sitting here on our feet? How are we still talking today?\" \"Oh well… that was fun anyway\" the woman sighed, looking around at the scene around\n",
            "reward: 0.408\n",
            "log probability: -3.143\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n",
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today?)\n",
            "\n",
            "But that kind of attitude has always been in their DNA. At first they were in denial about life after death. In our new version, they're looking more forward and hopeful because they really care\n",
            "reward: 1.656\n",
            "log probability: -3.067\n",
            "advantage: 0.361\n",
            "loss: 1.107\n",
            "--------------------------------------------------\n",
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today? What kind of question could a mother of ten answer?\" the girl replied.\n",
            "reward: 1.587\n",
            "log probability: -3.086\n",
            "advantage: 0.304\n",
            "loss: 0.939\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "completion: thank you for your help! We wish a Happy Christmas to all of the kids with Special Requiries, Please click these link for special request letters.\n",
            "\n",
            "Christmas Day and New Year's - the season is here, it's our birthday, and\n",
            "reward: 2.845\n",
            "log probability: -3.269\n",
            "advantage: 0.056\n",
            "loss: 0.182\n",
            "--------------------------------------------------\n",
            "prompt: thank you for your help!\n",
            "completion: thank you for your help! It was all of you who made and continue to give to making the music on all of our sites!\" It sounds as if the creators weren't only working hard to spread love, it sounds like they cared deeply.\n",
            "\n",
            "reward: 3.189\n",
            "log probability: -3.263\n",
            "advantage: 0.183\n",
            "loss: 0.598\n",
            "--------------------------------------------------\n",
            "prompt: thank you for your help!\n",
            "completion: thank you for your help! It was very quick! But you are not alone.\n",
            "\n",
            "Have any other ways or resources you would like added? Share any tips you learned. My blog post for the project looks like something anyone is familiar with…\n",
            "reward: 2.051\n",
            "log probability: -3.061\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. Thank you and God bless ya!\" The officer stopped a short distance behind me, took his time picking his way through an old pair of headphones. One was of course black headphone while his other was a pair of light\n",
            "reward: 2.956\n",
            "log probability: -3.543\n",
            "advantage: 0.272\n",
            "loss: 0.962\n",
            "--------------------------------------------------\n",
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. I have no idea how this happens in our family, and how a dog like this could die from such a simple event? Do you have to leave the neighborhood every now and then for reasons like this?\n",
            "\n",
            "—\n",
            "reward: 2.000\n",
            "log probability: -3.094\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n",
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. I have my work ethic, you probably already understand and so for your consideration of his proposal my proposal becomes moot as I'm no longer able to help in some way other then as his own personal project! I'm sure\n",
            "reward: 2.019\n",
            "log probability: -3.985\n",
            "advantage: 0.010\n",
            "loss: 0.040\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? What about the food I don't have at this restaurant?\"\n",
            "\n",
            "\"Nana-dono?\"\n",
            "\n",
            "Takeru was in a bad mood, so Naegi asked again to find some information that\n",
            "reward: 2.109\n",
            "log probability: -2.959\n",
            "advantage: 0.010\n",
            "loss: 0.030\n",
            "--------------------------------------------------\n",
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? (if he does have it, see \"How to turn off a radio device (frequently asked questions)\" above, the list goes on from there (see FAQ page 3 below for further info and answer to\n",
            "reward: 2.896\n",
            "log probability: -3.455\n",
            "advantage: 0.230\n",
            "loss: 0.794\n",
            "--------------------------------------------------\n",
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? Please help me, because you are our greatest hero.\n",
            "\n",
            "\n",
            "In the course of the night that night I noticed the moon was completely cloudy... So I ran into your friend again... He was gone...I\n",
            "reward: 2.059\n",
            "log probability: -3.290\n",
            "advantage: 0.010\n",
            "loss: 0.033\n",
            "--------------------------------------------------\n",
            "average loss for iteration 2: 0.000\n",
            "================================================================================\n",
            "iteration 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: once upon a time,\n",
            "completion: once upon a time, someone is in pain at home. They're in their house,\" he said. \"When someone gets hurt, they know that. It doesn't matter what's been going on over night.\n",
            "\n",
            "\"This may not be\n",
            "reward: -2.404\n",
            "log probability: -2.662\n",
            "advantage: 0.010\n",
            "loss: 0.027\n",
            "--------------------------------------------------\n",
            "prompt: once upon a time,\n",
            "completion: once upon a time, as has been seen in numerous writings, one has had to put forth oneself, even at the cost of the physical strength, to do such things in life as were forbidden before.\"\n",
            "reward: -1.679\n",
            "log probability: -3.236\n",
            "advantage: 0.010\n",
            "loss: 0.032\n",
            "--------------------------------------------------\n",
            "prompt: once upon a time,\n",
            "completion: once upon a time, it's hard to be excited about the world,\" she told the Observer. \"It's nice but if it wasn't for our love of science and learning, our life would probably be miserable.\"\n",
            "\n",
            "As she told the\n",
            "reward: 0.863\n",
            "log probability: -2.625\n",
            "advantage: 1.804\n",
            "loss: 4.735\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today? - Hello. - I need water for my feet, because my knees and feet, from how they feel in bed, hurt like the legs of dogs. You said that I would go for a hike this evening\n",
            "reward: -1.307\n",
            "log probability: -3.320\n",
            "advantage: 0.010\n",
            "loss: 0.033\n",
            "--------------------------------------------------\n",
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today?\n",
            "\n",
            "Coco: Hey guys, just some stuff I had yesterday morning. That I was able to get off of, with a pretty solid performance. I had a couple of different beers when I was gone\n",
            "reward: 1.044\n",
            "log probability: -3.056\n",
            "advantage: 2.154\n",
            "loss: 6.582\n",
            "--------------------------------------------------\n",
            "prompt: hello, how are you doing today?\n",
            "completion: hello, how are you doing today?\n",
            "\n",
            "Sakura: Good, how did your lunch be?\n",
            "\n",
            "I'm still tired as usual too, sorry.\n",
            "\n",
            "I will return with breakfast for our room now?\n",
            "\n",
            "Sakura:\n",
            "reward: -2.450\n",
            "log probability: -2.729\n",
            "advantage: 0.010\n",
            "loss: 0.027\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: thank you for your help!\n",
            "completion: thank you for your help! You are truly appreciated!! :)\"\n",
            "reward: 2.923\n",
            "log probability: -2.554\n",
            "advantage: 0.010\n",
            "loss: 0.026\n",
            "--------------------------------------------------\n",
            "prompt: thank you for your help!\n",
            "completion: thank you for your help! <3<] This file is a single document that contains the details of how to install NVDD. I have split it and made it as easy as possible with screenshots provided in the instructions below.<br><font\n",
            "reward: 2.985\n",
            "log probability: -3.092\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n",
            "prompt: thank you for your help!\n",
            "completion: thank you for your help! It would still require that we continue to invest into expanding our data-driven operations in order to make sure you enjoy using our app!\"\n",
            "\n",
            "For more about Appoquin, click here.\n",
            "reward: 2.980\n",
            "log probability: -2.971\n",
            "advantage: 0.010\n",
            "loss: 0.030\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. It's always easy to think about this when it happens, but your feelings, you know the world won't stop until that one woman gets what's hers.\n",
            "\n",
            "The two are speaking quietly again. They're hugging\n",
            "reward: 1.975\n",
            "log probability: -3.482\n",
            "advantage: 0.010\n",
            "loss: 0.035\n",
            "--------------------------------------------------\n",
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. I appreciate everything that you are doing for my mother today.\n",
            "\n",
            "Thanks everyone for reading my story! :) I thank my family too.\"\n",
            "reward: 3.192\n",
            "log probability: -3.344\n",
            "advantage: 0.182\n",
            "loss: 0.608\n",
            "--------------------------------------------------\n",
            "prompt: i really appreciate your kindness.\n",
            "completion: i really appreciate your kindness. And I promise you something? That will be all. Please keep your mouth shut.\"\n",
            "\n",
            "Sasha took her finger and pushed her tongue out of the door of Ruby's apartment. She grabbed her phone and placed it\n",
            "reward: 2.936\n",
            "log probability: -3.120\n",
            "advantage: 0.087\n",
            "loss: 0.271\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? Are people like us who understand the meaning of this issue truly \"racist\"? And please try and explain why that is. If you cannot do that, what other services have you implemented to help your readers?\n",
            "\n",
            "reward: 2.196\n",
            "log probability: -3.181\n",
            "advantage: 0.010\n",
            "loss: 0.032\n",
            "--------------------------------------------------\n",
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? The information will not be available till after the match\".\n",
            "\n",
            "'If we have got one player, I will let him get there,' said Klopp.\n",
            "\n",
            "That's when things got real heated.\n",
            "\n",
            "reward: 2.106\n",
            "log probability: -3.053\n",
            "advantage: 0.010\n",
            "loss: 0.031\n",
            "--------------------------------------------------\n",
            "prompt: could you please assist me with this?\n",
            "completion: could you please assist me with this? It is almost 1-quarter way into an appointment with Doctor Ophira. I will be waiting on my husband. Thanks.\" This text message led many other mothers to email. And while the number of emails\n",
            "reward: 2.912\n",
            "log probability: -3.459\n",
            "advantage: 0.211\n",
            "loss: 0.730\n",
            "--------------------------------------------------\n",
            "average loss for iteration 3: 0.000\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 11: evaluating the updated model\n",
        "def evaluate_model(prompt):\n",
        "    new_completion = generate_multiple_completions(prompt, num_samples=1)[0]\n",
        "    new_reward = reward_function(new_completion)\n",
        "    print(\"new generated completion:\", new_completion)\n",
        "    print(\"new reward score:\", new_reward)\n",
        "    return new_completion, new_reward\n",
        "\n",
        "# example usage\n",
        "evaluate_model(\"once upon a time,\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMQzElbKP20w",
        "outputId": "08fe0c2a-e050-48be-8acf-7c954937b5c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new generated completion: once upon a time, and it was not possible to know that it was going to stop until it was ready. I don't believe, therefore, in the current theory as such.\" (Emma Goldman - A Woman's Work, Chapter 7)\n",
            "new reward score: 0.18487955503964265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('once upon a time, and it was not possible to know that it was going to stop until it was ready. I don\\'t believe, therefore, in the current theory as such.\" (Emma Goldman - A Woman\\'s Work, Chapter 7)',\n",
              " 0.18487955503964265)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXpJSphVSHOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}