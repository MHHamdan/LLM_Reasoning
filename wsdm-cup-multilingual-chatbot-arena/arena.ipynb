{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9be4b8a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-24T17:19:26.926732Z",
     "iopub.status.busy": "2025-01-24T17:19:26.926476Z",
     "iopub.status.idle": "2025-01-24T17:19:26.930678Z",
     "shell.execute_reply": "2025-01-24T17:19:26.929898Z"
    },
    "papermill": {
     "duration": 0.013995,
     "end_time": "2025-01-24T17:19:26.931925",
     "exception": false,
     "start_time": "2025-01-24T17:19:26.917930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339b2b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:19:26.946867Z",
     "iopub.status.busy": "2025-01-24T17:19:26.946665Z",
     "iopub.status.idle": "2025-01-24T17:19:28.787556Z",
     "shell.execute_reply": "2025-01-24T17:19:28.786670Z"
    },
    "papermill": {
     "duration": 1.849967,
     "end_time": "2025-01-24T17:19:28.789137",
     "exception": false,
     "start_time": "2025-01-24T17:19:26.939170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da680953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:19:28.804290Z",
     "iopub.status.busy": "2025-01-24T17:19:28.803947Z",
     "iopub.status.idle": "2025-01-24T17:19:32.630946Z",
     "shell.execute_reply": "2025-01-24T17:19:32.629783Z"
    },
    "papermill": {
     "duration": 3.836001,
     "end_time": "2025-01-24T17:19:32.632444",
     "exception": false,
     "start_time": "2025-01-24T17:19:28.796443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (48439, 8) | Test shape: (3, 5)\n",
      "Index(['id', 'prompt', 'response_a', 'response_b', 'winner', 'model_a',\n",
      "       'model_b', 'language'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vieš po Slovensky?</td>\n",
       "      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n",
       "      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>reka-core-20240904</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00010ed04b536f56ebe43eef1100c13906abea12bf9855...</td>\n",
       "      <td>You will be given a piece of news. Analyze it ...</td>\n",
       "      <td>Let's break down the news and analyze it accor...</td>\n",
       "      <td>```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemma-2-27b-it</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003800d510e38803efba5ceaec122bc66408fe367b0be...</td>\n",
       "      <td>Dört basamaklı, rakamları birbirinden ve sıfır...</td>\n",
       "      <td>Bu soruyu çözmek için, verilen koşulları adım ...</td>\n",
       "      <td>Bu problemi adım adım çözelim:\\n\\n1) ABCD - DC...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...   \n",
       "1  00010ed04b536f56ebe43eef1100c13906abea12bf9855...   \n",
       "2  0003800d510e38803efba5ceaec122bc66408fe367b0be...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0                                 vieš po Slovensky?   \n",
       "1  You will be given a piece of news. Analyze it ...   \n",
       "2  Dört basamaklı, rakamları birbirinden ve sıfır...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0   Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n",
       "1  Let's break down the news and analyze it accor...   \n",
       "2  Bu soruyu çözmek için, verilen koşulları adım ...   \n",
       "\n",
       "                                          response_b   winner  \\\n",
       "0  Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a   \n",
       "1  ```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...  model_a   \n",
       "2  Bu problemi adım adım çözelim:\\n\\n1) ABCD - DC...  model_a   \n",
       "\n",
       "              model_a                     model_b language  \n",
       "0          o1-preview          reka-core-20240904   Slovak  \n",
       "1      gemma-2-27b-it        gemini-1.5-flash-002  Russian  \n",
       "2  gpt-4-0125-preview  claude-3-5-sonnet-20240620  Turkish  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/train.parquet')\n",
    "test = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/test.parquet')\n",
    "\n",
    "print(\"Train shape:\", train.shape, \"| Test shape:\", test.shape)\n",
    "print(train.columns)\n",
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d90284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:19:32.648085Z",
     "iopub.status.busy": "2025-01-24T17:19:32.647864Z",
     "iopub.status.idle": "2025-01-24T17:19:32.661993Z",
     "shell.execute_reply": "2025-01-24T17:19:32.661283Z"
    },
    "papermill": {
     "duration": 0.023038,
     "end_time": "2025-01-24T17:19:32.663194",
     "exception": false,
     "start_time": "2025-01-24T17:19:32.640156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    24481\n",
       "1    23958\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'] = (train['winner'] == 'model_a').astype(int)\n",
    "# label = 1 means model_a, 0 means model_b\n",
    "\n",
    "train['label'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246fc278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:19:32.680328Z",
     "iopub.status.busy": "2025-01-24T17:19:32.679937Z",
     "iopub.status.idle": "2025-01-24T17:19:33.599489Z",
     "shell.execute_reply": "2025-01-24T17:19:33.598825Z"
    },
    "papermill": {
     "duration": 0.930502,
     "end_time": "2025-01-24T17:19:33.601135",
     "exception": false,
     "start_time": "2025-01-24T17:19:32.670633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_text(row):\n",
    "    # You could also add special tokens or separators\n",
    "    return f\"PROMPT: {row['prompt']} RESP_A: {row['response_a']} RESP_B: {row['response_b']}\"\n",
    "\n",
    "train['combined_text'] = train.apply(combine_text, axis=1)\n",
    "test['combined_text']  = test.apply(combine_text, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51aae5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:19:33.617266Z",
     "iopub.status.busy": "2025-01-24T17:19:33.616999Z",
     "iopub.status.idle": "2025-01-24T17:21:25.492131Z",
     "shell.execute_reply": "2025-01-24T17:21:25.491116Z"
    },
    "papermill": {
     "duration": 111.884825,
     "end_time": "2025-01-24T17:21:25.493871",
     "exception": false,
     "start_time": "2025-01-24T17:19:33.609046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,   # limit features for speed\n",
    "    ngram_range=(1,2),    # unigrams + bigrams\n",
    "    analyzer='word', \n",
    "    stop_words=None       # or consider language-specific stopwords\n",
    ")\n",
    "\n",
    "# Fit on train set, transform both train and test\n",
    "X_train_tfidf = tfidf.fit_transform(train['combined_text'])\n",
    "X_test_tfidf  = tfidf.transform(test['combined_text'])\n",
    "\n",
    "y_train = train['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d667aeeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:21:25.510752Z",
     "iopub.status.busy": "2025-01-24T17:21:25.510467Z",
     "iopub.status.idle": "2025-01-24T17:21:32.364995Z",
     "shell.execute_reply": "2025-01-24T17:21:32.364005Z"
    },
    "papermill": {
     "duration": 6.86401,
     "end_time": "2025-01-24T17:21:32.366428",
     "exception": false,
     "start_time": "2025-01-24T17:21:25.502418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184fa05a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:21:32.383234Z",
     "iopub.status.busy": "2025-01-24T17:21:32.382972Z",
     "iopub.status.idle": "2025-01-24T17:21:36.076054Z",
     "shell.execute_reply": "2025-01-24T17:21:36.074996Z"
    },
    "papermill": {
     "duration": 3.702643,
     "end_time": "2025-01-24T17:21:36.077615",
     "exception": false,
     "start_time": "2025-01-24T17:21:32.374972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5016515276630884\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_tfidf, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "temp_clf = LogisticRegression(max_iter=200)\n",
    "temp_clf.fit(X_tr, y_tr)\n",
    "val_preds = temp_clf.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696e24cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:21:36.094188Z",
     "iopub.status.busy": "2025-01-24T17:21:36.093937Z",
     "iopub.status.idle": "2025-01-24T17:21:36.102493Z",
     "shell.execute_reply": "2025-01-24T17:21:36.101631Z"
    },
    "papermill": {
     "duration": 0.017887,
     "end_time": "2025-01-24T17:21:36.103832",
     "exception": false,
     "start_time": "2025-01-24T17:21:36.085945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>327228</td>\n",
       "      <td>model_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139415</td>\n",
       "      <td>model_b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1235630</td>\n",
       "      <td>model_b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   winner\n",
       "0   327228  model_b\n",
       "1  1139415  model_b\n",
       "2  1235630  model_b"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = clf.predict(X_test_tfidf)\n",
    "pred_winner = [\"model_a\" if p == 1 else \"model_b\" for p in test_preds]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"winner\": pred_winner\n",
    "})\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b11164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:21:36.119535Z",
     "iopub.status.busy": "2025-01-24T17:21:36.119306Z",
     "iopub.status.idle": "2025-01-24T17:21:36.128364Z",
     "shell.execute_reply": "2025-01-24T17:21:36.127803Z"
    },
    "papermill": {
     "duration": 0.018192,
     "end_time": "2025-01-24T17:21:36.129524",
     "exception": false,
     "start_time": "2025-01-24T17:21:36.111332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40caa02",
   "metadata": {
    "papermill": {
     "duration": 0.007368,
     "end_time": "2025-01-24T17:21:36.144616",
     "exception": false,
     "start_time": "2025-01-24T17:21:36.137248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c17ed7",
   "metadata": {
    "papermill": {
     "duration": 0.00716,
     "end_time": "2025-01-24T17:21:36.159138",
     "exception": false,
     "start_time": "2025-01-24T17:21:36.151978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153dd1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:21:36.174547Z",
     "iopub.status.busy": "2025-01-24T17:21:36.174321Z",
     "iopub.status.idle": "2025-01-24T17:22:02.318215Z",
     "shell.execute_reply": "2025-01-24T17:22:02.317264Z"
    },
    "papermill": {
     "duration": 26.153539,
     "end_time": "2025-01-24T17:22:02.320010",
     "exception": false,
     "start_time": "2025-01-24T17:21:36.166471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b07cfcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:22:02.337828Z",
     "iopub.status.busy": "2025-01-24T17:22:02.337261Z",
     "iopub.status.idle": "2025-01-24T17:22:02.392985Z",
     "shell.execute_reply": "2025-01-24T17:22:02.392071Z"
    },
    "papermill": {
     "duration": 0.066059,
     "end_time": "2025-01-24T17:22:02.394290",
     "exception": false,
     "start_time": "2025-01-24T17:22:02.328231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb6b05c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:22:02.410408Z",
     "iopub.status.busy": "2025-01-24T17:22:02.410184Z",
     "iopub.status.idle": "2025-01-24T17:22:03.832978Z",
     "shell.execute_reply": "2025-01-24T17:22:03.832058Z"
    },
    "papermill": {
     "duration": 1.436686,
     "end_time": "2025-01-24T17:22:03.838803",
     "exception": false,
     "start_time": "2025-01-24T17:22:02.402117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48439, 8) (3, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00007cff95d7f7974642a785aca248b0f26e60d3312fac...</td>\n",
       "      <td>vieš po Slovensky?</td>\n",
       "      <td>Áno, hovorím po slovensky. Ako vám môžem pomôcť?</td>\n",
       "      <td>Áno, veď som tu! Môžem ti pomôcť s otázkami al...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>reka-core-20240904</td>\n",
       "      <td>Slovak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00010ed04b536f56ebe43eef1100c13906abea12bf9855...</td>\n",
       "      <td>You will be given a piece of news. Analyze it ...</td>\n",
       "      <td>Let's break down the news and analyze it accor...</td>\n",
       "      <td>```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemma-2-27b-it</td>\n",
       "      <td>gemini-1.5-flash-002</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003800d510e38803efba5ceaec122bc66408fe367b0be...</td>\n",
       "      <td>Dört basamaklı, rakamları birbirinden ve sıfır...</td>\n",
       "      <td>Bu soruyu çözmek için, verilen koşulları adım ...</td>\n",
       "      <td>Bu problemi adım adım çözelim:\\n\\n1) ABCD - DC...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>Turkish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00072026c68f5418ef2da238394e418ce72a534b9b22d5...</td>\n",
       "      <td>현재 추천된 탑 3 종목인 Cabaletta Bio (CABA), Rocket Ph...</td>\n",
       "      <td>죄송하지만 저는 금융 조언을 제공할 수 없습니다. 저는 AI 모델이며, 투자 결정에...</td>\n",
       "      <td>현재 추천된 탑 3 종목에 순위를 매기기 위해서는 여러 가지 요소들을 고려해야 합니...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>gemma-2-2b-it</td>\n",
       "      <td>llama-3.1-nemotron-70b-instruct</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007ce7cf6bc1b5a8f8a4669b854fb12030863c970d9dc...</td>\n",
       "      <td>Please be boring</td>\n",
       "      <td>Alright, I'll be as boring as possible.\\n\\nTod...</td>\n",
       "      <td>Understood. Here is a straightforward, unadorn...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>reka-flash-20240722</td>\n",
       "      <td>grok-2-2024-08-13</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  00007cff95d7f7974642a785aca248b0f26e60d3312fac...   \n",
       "1  00010ed04b536f56ebe43eef1100c13906abea12bf9855...   \n",
       "2  0003800d510e38803efba5ceaec122bc66408fe367b0be...   \n",
       "3  00072026c68f5418ef2da238394e418ce72a534b9b22d5...   \n",
       "4  0007ce7cf6bc1b5a8f8a4669b854fb12030863c970d9dc...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0                                 vieš po Slovensky?   \n",
       "1  You will be given a piece of news. Analyze it ...   \n",
       "2  Dört basamaklı, rakamları birbirinden ve sıfır...   \n",
       "3  현재 추천된 탑 3 종목인 Cabaletta Bio (CABA), Rocket Ph...   \n",
       "4                                  Please be boring    \n",
       "\n",
       "                                          response_a  \\\n",
       "0   Áno, hovorím po slovensky. Ako vám môžem pomôcť?   \n",
       "1  Let's break down the news and analyze it accor...   \n",
       "2  Bu soruyu çözmek için, verilen koşulları adım ...   \n",
       "3  죄송하지만 저는 금융 조언을 제공할 수 없습니다. 저는 AI 모델이며, 투자 결정에...   \n",
       "4  Alright, I'll be as boring as possible.\\n\\nTod...   \n",
       "\n",
       "                                          response_b   winner  \\\n",
       "0  Áno, veď som tu! Môžem ti pomôcť s otázkami al...  model_a   \n",
       "1  ```json\\n{\\n  \"contains_orgs\": true,\\n  \"orgs\"...  model_a   \n",
       "2  Bu problemi adım adım çözelim:\\n\\n1) ABCD - DC...  model_a   \n",
       "3  현재 추천된 탑 3 종목에 순위를 매기기 위해서는 여러 가지 요소들을 고려해야 합니...  model_b   \n",
       "4  Understood. Here is a straightforward, unadorn...  model_a   \n",
       "\n",
       "               model_a                          model_b language  \n",
       "0           o1-preview               reka-core-20240904   Slovak  \n",
       "1       gemma-2-27b-it             gemini-1.5-flash-002  Russian  \n",
       "2   gpt-4-0125-preview       claude-3-5-sonnet-20240620  Turkish  \n",
       "3        gemma-2-2b-it  llama-3.1-nemotron-70b-instruct  English  \n",
       "4  reka-flash-20240722                grok-2-2024-08-13  English  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provided\n",
    "train = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/train.parquet')\n",
    "test = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/test.parquet')\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43c8d01d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:22:03.858069Z",
     "iopub.status.busy": "2025-01-24T17:22:03.857800Z",
     "iopub.status.idle": "2025-01-24T17:22:16.637347Z",
     "shell.execute_reply": "2025-01-24T17:22:16.636414Z"
    },
    "papermill": {
     "duration": 12.790821,
     "end_time": "2025-01-24T17:22:16.639255",
     "exception": false,
     "start_time": "2025-01-24T17:22:03.848434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summarization_model_path = \"/kaggle/input/bart-large-cnn/transformers/default/1/bart-large-cnn\"\n",
    "summarizer_tokenizer = AutoTokenizer.from_pretrained(summarization_model_path)\n",
    "summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(summarization_model_path).to(device)\n",
    "\n",
    "# Optionally create a pipeline for easy summarization\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\", \n",
    "    model=summarizer_model, \n",
    "    tokenizer=summarizer_tokenizer,\n",
    "    device=0 if device.type == \"cuda\" else -1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44f480a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:22:16.668749Z",
     "iopub.status.busy": "2025-01-24T17:22:16.668476Z",
     "iopub.status.idle": "2025-01-24T17:22:17.060347Z",
     "shell.execute_reply": "2025-01-24T17:22:17.059594Z"
    },
    "papermill": {
     "duration": 0.406563,
     "end_time": "2025-01-24T17:22:17.061752",
     "exception": false,
     "start_time": "2025-01-24T17:22:16.655189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.model.to(\"cpu\")\n",
    "# Optionally summarizer.tokenizer is still fine on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56df8ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:22:17.079189Z",
     "iopub.status.busy": "2025-01-24T17:22:17.078871Z",
     "iopub.status.idle": "2025-01-24T17:22:17.084134Z",
     "shell.execute_reply": "2025-01-24T17:22:17.083350Z"
    },
    "papermill": {
     "duration": 0.015364,
     "end_time": "2025-01-24T17:22:17.085386",
     "exception": false,
     "start_time": "2025-01-24T17:22:17.070022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def maybe_summarize(text, tokenizer, summarizer, max_token_len=2024, max_summary_len=1028):\n",
    "    # Tokenize to check length\n",
    "    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
    "    if len(tokens) > max_token_len:\n",
    "        # Summarize if the text is too long\n",
    "        summary = summarizer(\n",
    "            text,\n",
    "            max_length=max_summary_len,\n",
    "            min_length=30,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return summary[0]['summary_text']\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12f5570a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T17:22:17.102230Z",
     "iopub.status.busy": "2025-01-24T17:22:17.101932Z",
     "iopub.status.idle": "2025-01-24T17:22:17.527109Z",
     "shell.execute_reply": "2025-01-24T17:22:17.525526Z"
    },
    "papermill": {
     "duration": 0.435041,
     "end_time": "2025-01-24T17:22:17.528426",
     "exception": true,
     "start_time": "2025-01-24T17:22:17.093385",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-49a86c493e2d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train['prompt'] = train['prompt'].apply(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaybe_summarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m train['response_a'] = train['response_a'].apply(\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaybe_summarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
      "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-49a86c493e2d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m train['prompt'] = train['prompt'].apply(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaybe_summarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m train['response_a'] = train['response_a'].apply(\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaybe_summarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4f2ca9ad1df5>\u001b[0m in \u001b[0;36mmaybe_summarize\u001b[0;34m(text, tokenizer, summarizer, max_token_len, max_summary_len)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_token_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Summarize if the text is too long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         summary = summarizer(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_summary_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             )\n\u001b[1;32m   1300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2065\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "train['prompt'] = train['prompt'].apply(\n",
    "    lambda x: maybe_summarize(str(x), summarizer_tokenizer, summarizer)\n",
    ")\n",
    "train['response_a'] = train['response_a'].apply(\n",
    "    lambda x: maybe_summarize(str(x), summarizer_tokenizer, summarizer)\n",
    ")\n",
    "train['response_b'] = train['response_b'].apply(\n",
    "    lambda x: maybe_summarize(str(x), summarizer_tokenizer, summarizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723e128",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e55aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T16:17:58.883092Z",
     "iopub.status.busy": "2025-01-24T16:17:58.882768Z",
     "iopub.status.idle": "2025-01-24T16:17:58.946415Z",
     "shell.execute_reply": "2025-01-24T16:17:58.945288Z",
     "shell.execute_reply.started": "2025-01-24T16:17:58.883069Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "accumulation_steps = 4\n",
    "for step, batch in enumerate(train_loader):\n",
    "    # Forward pass\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss / accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights only every `accumulation_steps` steps\n",
    "    if step % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "# Check tokenized input lengths\n",
    "def check_token_lengths(dataset, tokenizer):\n",
    "    for i in range(len(dataset)):\n",
    "        row = dataset.iloc[i]\n",
    "        tokens = tokenizer.encode(\n",
    "            row['prompt'] + \" [SEP] \" + row['response_a'] + \" [SEP] \" + row['response_b'], \n",
    "            truncation=False\n",
    "        )\n",
    "        if len(tokens) > 512:\n",
    "            print(f\"Row {i} exceeds max length: {len(tokens)} tokens\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if not check_token_lengths(train, tokenizer):\n",
    "    print(\"Some rows exceed the max token length.\")\n",
    "\n",
    "def truncate_text(text, max_length=1024):\n",
    "    # Truncate text if it exceeds the max allowable length\n",
    "    return text[:max_length]\n",
    "\n",
    "train['prompt'] = train['prompt'].apply(lambda x: truncate_text(x))\n",
    "train['response_a'] = train['response_a'].apply(lambda x: truncate_text(x))\n",
    "train['response_b'] = train['response_b'].apply(lambda x: truncate_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61537f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c92c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T02:58:20.923726Z",
     "iopub.status.busy": "2025-01-24T02:58:20.923396Z",
     "iopub.status.idle": "2025-01-24T02:58:41.768077Z",
     "shell.execute_reply": "2025-01-24T02:58:41.767300Z",
     "shell.execute_reply.started": "2025-01-24T02:58:20.923700Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets scikit-learn pandas\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Path to the uploaded model in Kaggle\n",
    "model_path = \"/kaggle/input/xlm-roberta-base/transformers/default/1/xlm-roberta-base\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c15ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:11:30.064479Z",
     "iopub.status.busy": "2025-01-24T03:11:30.064176Z",
     "iopub.status.idle": "2025-01-24T03:11:33.042097Z",
     "shell.execute_reply": "2025-01-24T03:11:33.041372Z",
     "shell.execute_reply.started": "2025-01-24T03:11:30.064456Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and test Parquet files\n",
    "train = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/train.parquet')\n",
    "test = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/test.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53986828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:15:54.933461Z",
     "iopub.status.busy": "2025-01-24T03:15:54.933158Z",
     "iopub.status.idle": "2025-01-24T03:15:57.976960Z",
     "shell.execute_reply": "2025-01-24T03:15:57.976269Z",
     "shell.execute_reply.started": "2025-01-24T03:15:54.933437Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the train and test Parquet files\n",
    "train = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/train.parquet')\n",
    "test = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/test.parquet')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/xlm-roberta-base/transformers/default/1/xlm-roberta-base\")\n",
    "\n",
    "# Tokenize combined input\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        inputs = self.tokenizer(\n",
    "            row['prompt'] + \" [SEP] \" + row['response_a'] + \" [SEP] \" + row['response_b'],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        label = torch.tensor(0 if row['winner'] == \"model_a\" else 1)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ChatbotDataset(train_data, tokenizer, max_length=512)\n",
    "val_dataset = ChatbotDataset(val_data, tokenizer, max_length=512)\n",
    "test_dataset = ChatbotDataset(test, tokenizer, max_length=512)\n",
    "\n",
    "# Prepare dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db70ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:49:38.337445Z",
     "iopub.status.busy": "2025-01-24T03:49:38.337109Z",
     "iopub.status.idle": "2025-01-24T03:49:38.467272Z",
     "shell.execute_reply": "2025-01-24T03:49:38.466345Z",
     "shell.execute_reply.started": "2025-01-24T03:49:38.337421Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get one batch of data from the train_loader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Extract input_ids, attention_mask, and labels\n",
    "input_ids = batch['input_ids']\n",
    "attention_mask = batch['attention_mask']\n",
    "labels = batch['labels']\n",
    "\n",
    "# Print the batch details\n",
    "print(\"Input IDs Shape:\", input_ids.shape)\n",
    "print(\"Attention Mask Shape:\", attention_mask.shape)\n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "\n",
    "# # Print a sample input and its corresponding label\n",
    "# print(\"\\nSample Input IDs:\", input_ids[0])\n",
    "# print(\"Sample Attention Mask:\", attention_mask[0])\n",
    "# print(\"Sample Label:\", labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663466c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:20:00.323625Z",
     "iopub.status.busy": "2025-01-24T03:20:00.323341Z",
     "iopub.status.idle": "2025-01-24T03:23:53.488371Z",
     "shell.execute_reply": "2025-01-24T03:23:53.487420Z",
     "shell.execute_reply.started": "2025-01-24T03:20:00.323604Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze token lengths\n",
    "token_lengths = [len(input_id) for batch in train_loader for input_id in batch['input_ids']]\n",
    "print(\"\\nToken Lengths Statistics:\")\n",
    "print(f\"Mean Length: {sum(token_lengths)/len(token_lengths):.2f}\")\n",
    "print(f\"Max Length: {max(token_lengths)}\")\n",
    "print(f\"Min Length: {min(token_lengths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702401f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:37:53.687081Z",
     "iopub.status.busy": "2025-01-24T03:37:53.686669Z",
     "iopub.status.idle": "2025-01-24T03:37:53.691444Z",
     "shell.execute_reply": "2025-01-24T03:37:53.690374Z",
     "shell.execute_reply.started": "2025-01-24T03:37:53.687050Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff6435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:37:54.546675Z",
     "iopub.status.busy": "2025-01-24T03:37:54.546400Z",
     "iopub.status.idle": "2025-01-24T03:42:10.788178Z",
     "shell.execute_reply": "2025-01-24T03:42:10.787318Z",
     "shell.execute_reply.started": "2025-01-24T03:37:54.546655Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_lengths = [len(tokenizer.encode(row['prompt'] + \" [SEP] \" + row['response_a'] + \" [SEP] \" + row['response_b'])) for _, row in train.iterrows()]\n",
    "print(f\"Original Token Lengths - Mean: {np.mean(original_lengths):.2f}, Max: {np.max(original_lengths)}, Min: {np.min(original_lengths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271cbd4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb62878",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e776b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:36:33.486102Z",
     "iopub.status.busy": "2025-01-24T01:36:33.485797Z",
     "iopub.status.idle": "2025-01-24T01:36:34.746603Z",
     "shell.execute_reply": "2025-01-24T01:36:34.745644Z",
     "shell.execute_reply.started": "2025-01-24T01:36:33.486073Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train and test Parquet files\n",
    "train = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/train.parquet')\n",
    "test = pd.read_parquet('/kaggle/input/datasets1/wsdm-cup-multilingual-chatbot-arena (1)/test.parquet')\n",
    "\n",
    "# # # Display the data\n",
    "# print(train.head())\n",
    "# print(train.info())\n",
    "# print(test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3179b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T03:12:05.445442Z",
     "iopub.status.busy": "2025-01-24T03:12:05.445119Z",
     "iopub.status.idle": "2025-01-24T03:12:05.449009Z",
     "shell.execute_reply": "2025-01-24T03:12:05.448185Z",
     "shell.execute_reply.started": "2025-01-24T03:12:05.445412Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Language distribution\n",
    "#train['language'].value_counts().plot(kind='bar', title='Language Distribution')\n",
    "#plt.show()\n",
    "\n",
    "# Winner distribution\n",
    "#train['winner'].value_counts().plot(kind='bar', title='Winner Distribution')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61d987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T23:29:58.969600Z",
     "iopub.status.busy": "2025-01-23T23:29:58.969287Z",
     "iopub.status.idle": "2025-01-23T23:29:58.977983Z",
     "shell.execute_reply": "2025-01-23T23:29:58.977200Z",
     "shell.execute_reply.started": "2025-01-23T23:29:58.969575Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_path = '/kaggle/input/all-minilm-l6-v2/all-MiniLM-L6-v2'\n",
    "\n",
    "# Check if the config.json file exists\n",
    "config_file_path = f\"{model_path}/config.json\"\n",
    "print(\"Config file exists:\", os.path.exists(config_file_path))\n",
    "\n",
    "# Load and print the content of config.json\n",
    "with open(config_file_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "print(\"Config file content:\", config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a87bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T23:50:33.187271Z",
     "iopub.status.busy": "2025-01-23T23:50:33.186934Z",
     "iopub.status.idle": "2025-01-23T23:50:33.308800Z",
     "shell.execute_reply": "2025-01-23T23:50:33.308036Z",
     "shell.execute_reply.started": "2025-01-23T23:50:33.187242Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Path to the model files\n",
    "model_path = '/kaggle/input/all-minilm-l6-v2/all-MiniLM-L6-v2'\n",
    "\n",
    "# Load the model and tokenizer directly from the local files\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Test the model with a sample sentence\n",
    "inputs = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "#print(\"Model outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680aa77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T23:51:31.696778Z",
     "iopub.status.busy": "2025-01-23T23:51:31.696434Z",
     "iopub.status.idle": "2025-01-23T23:56:45.941123Z",
     "shell.execute_reply": "2025-01-23T23:56:45.940190Z",
     "shell.execute_reply.started": "2025-01-23T23:51:31.696727Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the model path\n",
    "model_path = '/kaggle/input/all-minilm-l6-v2/all-MiniLM-L6-v2'\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Combine responses for embedding\n",
    "train['combined_responses'] = train['response_a'] + \" \" + train['response_b']\n",
    "test['combined_responses'] = test['response_a'] + \" \" + test['response_b']\n",
    "\n",
    "# Function to encode responses using the transformers model\n",
    "def encode_responses(responses, model, tokenizer, device='cpu', batch_size=32):\n",
    "    \"\"\"Encodes responses using the provided model and tokenizer.\"\"\"\n",
    "    embeddings = []\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(responses), batch_size):\n",
    "            batch = responses[i:i + batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            # Use `pooler_output` for sentence embeddings\n",
    "            embeddings.append(outputs.pooler_output.cpu())\n",
    "\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "# Encode training and testing data\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "X_train_bert = encode_responses(train['combined_responses'].tolist(), model, tokenizer, device=device)\n",
    "X_test_bert = encode_responses(test['combined_responses'].tolist(), model, tokenizer, device=device)\n",
    "\n",
    "# Check shapes of the embeddings\n",
    "print(\"Train embeddings shape:\", X_train_bert.shape)\n",
    "print(\"Test embeddings shape:\", X_test_bert.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb2cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T00:18:34.079700Z",
     "iopub.status.busy": "2025-01-24T00:18:34.079385Z",
     "iopub.status.idle": "2025-01-24T00:18:34.094816Z",
     "shell.execute_reply": "2025-01-24T00:18:34.094043Z",
     "shell.execute_reply.started": "2025-01-24T00:18:34.079675Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the 'winner' column as numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train['winner'])  # 0: model_a, 1: model_b\n",
    "\n",
    "# Check the mapping\n",
    "print(\"Label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226e78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T00:19:19.784295Z",
     "iopub.status.busy": "2025-01-24T00:19:19.783963Z",
     "iopub.status.idle": "2025-01-24T00:19:19.823870Z",
     "shell.execute_reply": "2025-01-24T00:19:19.822920Z",
     "shell.execute_reply.started": "2025-01-24T00:19:19.784266Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_bert, y_train, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a66b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T00:20:12.781946Z",
     "iopub.status.busy": "2025-01-24T00:20:12.781468Z",
     "iopub.status.idle": "2025-01-24T00:20:12.788104Z",
     "shell.execute_reply": "2025-01-24T00:20:12.787217Z",
     "shell.execute_reply.started": "2025-01-24T00:20:12.781904Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_split.shape, X_val.shape, y_train_split.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce4326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T00:52:59.595932Z",
     "iopub.status.busy": "2025-01-24T00:52:59.595592Z",
     "iopub.status.idle": "2025-01-24T00:52:59.601100Z",
     "shell.execute_reply": "2025-01-24T00:52:59.600292Z",
     "shell.execute_reply.started": "2025-01-24T00:52:59.595906Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a feedforward neural network\n",
    "class PreferenceModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(PreferenceModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),  # Hidden layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),            # Dropout for regularization\n",
    "            nn.Linear(128, 64),         # Hidden layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)            # Output layer (binary classification)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c5b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T00:56:28.903951Z",
     "iopub.status.busy": "2025-01-24T00:56:28.903644Z",
     "iopub.status.idle": "2025-01-24T00:56:28.908616Z",
     "shell.execute_reply": "2025-01-24T00:56:28.907811Z",
     "shell.execute_reply.started": "2025-01-24T00:56:28.903926Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming y_train is a NumPy array or list of labels\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Ensure labels are of type Long\n",
    "\n",
    "# Use the embeddings directly if they are already PyTorch tensors\n",
    "X_train_tensor = X_train_bert.float()  # Convert to float if not already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9159a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:01:34.826462Z",
     "iopub.status.busy": "2025-01-24T01:01:34.826155Z",
     "iopub.status.idle": "2025-01-24T01:01:34.989636Z",
     "shell.execute_reply": "2025-01-24T01:01:34.988961Z",
     "shell.execute_reply.started": "2025-01-24T01:01:34.826436Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming y_train is a list or numpy array of labels\n",
    "class_counts = Counter(y_train)\n",
    "print(\"Class Distribution:\", class_counts)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Counter object or pandas Series\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3187c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:17:06.764653Z",
     "iopub.status.busy": "2025-01-24T01:17:06.764317Z",
     "iopub.status.idle": "2025-01-24T01:17:06.776662Z",
     "shell.execute_reply": "2025-01-24T01:17:06.775993Z",
     "shell.execute_reply.started": "2025-01-24T01:17:06.764623Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"y_train Sample:\", y_train[:10])\n",
    "print(\"Class Distribution:\", Counter(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533700a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:27:47.874980Z",
     "iopub.status.busy": "2025-01-24T01:27:47.874642Z",
     "iopub.status.idle": "2025-01-24T01:27:59.845988Z",
     "shell.execute_reply": "2025-01-24T01:27:59.845253Z",
     "shell.execute_reply.started": "2025-01-24T01:27:47.874951Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the improved classifier\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "# Model setup\n",
    "input_size = 384  # Embedding size\n",
    "num_classes = 2   # Binary classification (0 or 1)\n",
    "\n",
    "\n",
    "\n",
    "# Define a simple linear model\n",
    "model = SimpleClassifier(input_size=384, num_classes=2)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        # During training\n",
    "\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093094b9",
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-24T15:01:22.282Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        text = f\"{row['response_a']} [SEP] {row['response_b']}\"\n",
    "        inputs = self.tokenizer(text, max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        label = 0 if row['winner'] == 'model_a' else 1\n",
    "        return {**inputs, \"label\": torch.tensor(label)}\n",
    "\n",
    "# Load Dataset\n",
    "train = pd.read_csv(\"train.csv\")  # Replace with actual file path\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "train_dataset = ChatbotDataset(train, tokenizer, max_len=512)\n",
    "test_dataset = ChatbotDataset(test, tokenizer, max_len=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Model Setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, predictions, target_names=[\"model_a\", \"model_b\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72041a6f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-24T03:10:11.381799Z",
     "iopub.status.idle": "2025-01-24T03:10:11.382154Z",
     "shell.execute_reply": "2025-01-24T03:10:11.381997Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_bert = X_test_bert.to(device)\n",
    "    test_outputs = model(X_test_bert)\n",
    "    predictions = torch.argmax(test_outputs, dim=1).cpu()\n",
    "\n",
    "# Generate a classification report\n",
    "print(classification_report(y_test, predictions, target_names=[\"Class 0\", \"Class 1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48ddb5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 6537533,
     "sourceId": 10564835,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 227332,
     "modelInstanceId": 205586,
     "sourceId": 240592,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 227378,
     "modelInstanceId": 205631,
     "sourceId": 240650,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 227795,
     "modelInstanceId": 206048,
     "sourceId": 241196,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.351872,
   "end_time": "2025-01-24T17:22:20.755288",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-24T17:19:24.403416",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
