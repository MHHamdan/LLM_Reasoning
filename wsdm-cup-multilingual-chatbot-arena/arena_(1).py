# -*- coding: utf-8 -*-
"""arena (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b128ED3HAvKJr7v0oZDXdoBeYOyZvIjS
"""

!pip install --force-reinstall "numpy<2"
!pip install --upgrade "torch>=2.0" "transformers>=4.40" "tensorflow<2.16"
!pip install --force-reinstall "pybind11>=2.12"  # Required for some compiled modules

import pandas as pd
import numpy as np
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from sklearn.model_selection import train_test_split
from typing import Dict, List, Optional
import warnings
import subprocess
from google.colab import drive

warnings.filterwarnings('ignore')

def set_seed(seed: int = 42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

class PreferenceDataset(torch.utils.data.Dataset):
    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int = 512):
        self.df = df
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx: int):
        row = self.df.iloc[idx]

        # Concatenate prompt with each response
        text_a = f"{row['prompt']} [SEP] {row['response_a']}"
        text_b = f"{row['prompt']} [SEP] {row['response_b']}"

        # Tokenize both texts
        encoded = self.tokenizer(
            text_a,
            text_b,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors=None
        )

        # Add label
        encoded['label'] = 0 if row['winner'] == 'model_a' else 1

        return encoded

class PreferenceTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.pop('labels')
        outputs = model(**inputs)

        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)

        return (loss, outputs.logits) if return_outputs else loss

def main():
    # Install Kaggle API
    subprocess.run(["pip", "install", "-q", "kaggle"], check=True)

    # Mount Google Drive to access datasets and Kaggle API key
    drive.mount('/content/drive')

    # Set up Kaggle API key (update the path to your kaggle.json)
    kaggle_json_path = "/content/drive/MyDrive/Generativeai/kaggle.json"  # UPDATE THIS PATH
    subprocess.run(f"mkdir -p ~/.kaggle", shell=True, check=True)
    subprocess.run(f"cp {kaggle_json_path} ~/.kaggle/", shell=True, check=True)
    subprocess.run(f"chmod 600 ~/.kaggle/kaggle.json", shell=True, check=True)

    # Set random seed
    set_seed(42)

    # Load datasets
    print("Loading datasets...")
    train = pd.read_parquet("/content/drive/MyDrive/GenerativeAI/wsdm-cup-multilingual-chatbot-arena/train.parquet")
    test = pd.read_parquet("/content/drive/MyDrive/GenerativeAI/wsdm-cup-multilingual-chatbot-arena/test.parquet")
    print(f"Train shape: {train.shape}, Test shape: {test.shape}")

    # Initialize tokenizer and model
    print("Initializing model and tokenizer...")
    model_path = "/content/drive/MyDrive/GenerativeAI/bert-base-multilingual-cased"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_path,
        num_labels=2,
        ignore_mismatched_sizes=True
    )

    # Split data into train and validation sets
    train_df, val_df = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)

    # Create datasets
    train_dataset = PreferenceDataset(train_df, tokenizer)
    val_dataset = PreferenceDataset(val_df, tokenizer)

    # Initialize data collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Training arguments
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_dir='./logs',
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        max_grad_norm=1.0,
        report_to=["none"],
        learning_rate=2e-5,
        fp16=True,
        gradient_accumulation_steps=2,
        save_total_limit=2,
        logging_steps=100,
    )

    # Initialize trainer
    trainer = PreferenceTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )

    # Train model
    print("Training model...")
    trainer.train()

    # Generate predictions
    print("Generating predictions...")
    test_dataset = PreferenceDataset(test, tokenizer)
    predictions = []
    model.eval()
    device = next(model.parameters()).device

    with torch.no_grad():
        for i in range(len(test_dataset)):
            # Get both encodings
            text_a = f"{test.iloc[i]['prompt']} [SEP] {test.iloc[i]['response_a']}"
            text_b = f"{test.iloc[i]['prompt']} [SEP] {test.iloc[i]['response_b']}"

            # Encode both texts
            inputs_a = tokenizer(
                text_a,
                truncation=True,
                max_length=512,
                padding='max_length',
                return_tensors='pt'
            ).to(device)

            inputs_b = tokenizer(
                text_b,
                truncation=True,
                max_length=512,
                padding='max_length',
                return_tensors='pt'
            ).to(device)

            # Get logits for both
            outputs_a = model(**inputs_a)
            outputs_b = model(**inputs_b)

            # Compare logits
            pred = 'model_a' if outputs_a.logits[0, 1] > outputs_b.logits[0, 1] else 'model_b'
            predictions.append(pred)

    # Create submission
    submission = pd.DataFrame({
        'id': test['id'],
        'winner': predictions
    })
    submission.to_csv("submission.csv", index=False)
    print("Submission file created successfully!")

    # Submit to Kaggle competition (update competition name)
    competition_name = "wsdm-cup-multilingual-chatbot-arena"  # UPDATE THIS WITH YOUR COMPETITION NAME
    submission_path = "submission.csv"
    message = "Submission from Colab training script"

    print("\nSubmitting to Kaggle...")
    result = subprocess.run(
        ["kaggle", "competitions", "submit", "-c", competition_name, "-f", submission_path, "-m", message],
        capture_output=True,
        text=True
    )

    print("Kaggle submission output:")
    print(result.stdout)
    if result.stderr:
        print("Submission errors:")
        print(result.stderr)
    else:
        print("Submission successful!")

if __name__ == "__main__":
    main()



#!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121 --force-reinstall