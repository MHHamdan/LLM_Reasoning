{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":10564835,"sourceType":"datasetVersion","datasetId":6537533},{"sourceId":240592,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":205586,"modelId":227332},{"sourceId":240650,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":205631,"modelId":227378},{"sourceId":241196,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":206048,"modelId":227795},{"sourceId":242092,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":206793,"modelId":228539},{"sourceId":243601,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":208092,"modelId":229796},{"sourceId":243760,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":208225,"modelId":229922}],"dockerImageVersionId":30840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\nfrom sklearn.model_selection import train_test_split\nfrom typing import Dict, List, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef set_seed(seed: int = 42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nclass PreferenceDataset(torch.utils.data.Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer, max_length: int = 512):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        \n        # Concatenate prompt with each response\n        text_a = f\"{row['prompt']} [SEP] {row['response_a']}\"\n        text_b = f\"{row['prompt']} [SEP] {row['response_b']}\"\n        \n        # Tokenize both texts\n        encoded = self.tokenizer(\n            text_a,\n            text_b,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors=None\n        )\n        \n        # Add label\n        encoded['label'] = 0 if row['winner'] == 'model_a' else 1\n        \n        return encoded\n\nclass PreferenceTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop('labels')\n        outputs = model(**inputs)\n        \n        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n        \n        return (loss, outputs.logits) if return_outputs else loss\n\ndef main():\n    # Set random seed\n    set_seed(42)\n    \n    # Load datasets\n    print(\"Loading datasets...\")\n    train = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\n    test = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\")\n    print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n    \n    # Initialize tokenizer and model\n    print(\"Initializing model and tokenizer...\")\n    model_path = \"/kaggle/input/google-bert/transformers/default/1/bert-base-multilingual-cased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path,\n        num_labels=2,\n        ignore_mismatched_sizes=True\n    )\n    \n    # Split data into train and validation sets\n    train_df, val_df = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)\n    \n    # Create datasets\n    train_dataset = PreferenceDataset(train_df, tokenizer)\n    val_dataset = PreferenceDataset(val_df, tokenizer)\n    \n    # Initialize data collator\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir='./results',\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        max_grad_norm=1.0,\n        report_to=[\"none\"],\n        learning_rate=2e-5,\n        fp16=True,\n        gradient_accumulation_steps=2,\n        save_total_limit=2,\n        logging_steps=100,\n    )\n    \n    # Initialize trainer\n    trainer = PreferenceTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=data_collator,\n    )\n    \n    # Train model\n    print(\"Training model...\")\n    trainer.train()\n    \n    # Generate predictions\n    print(\"Generating predictions...\")\n    test_dataset = PreferenceDataset(test, tokenizer)\n    predictions = []\n    model.eval()\n    device = next(model.parameters()).device\n    \n    with torch.no_grad():\n        for i in range(len(test_dataset)):\n            # Get both encodings\n            text_a = f\"{test.iloc[i]['prompt']} [SEP] {test.iloc[i]['response_a']}\"\n            text_b = f\"{test.iloc[i]['prompt']} [SEP] {test.iloc[i]['response_b']}\"\n            \n            # Encode both texts\n            inputs_a = tokenizer(\n                text_a,\n                truncation=True,\n                max_length=512,\n                padding='max_length',\n                return_tensors='pt'\n            ).to(device)\n            \n            inputs_b = tokenizer(\n                text_b,\n                truncation=True,\n                max_length=512,\n                padding='max_length',\n                return_tensors='pt'\n            ).to(device)\n            \n            # Get logits for both\n            outputs_a = model(**inputs_a)\n            outputs_b = model(**inputs_b)\n            \n            # Compare logits\n            pred = 'model_a' if outputs_a.logits[0, 1] > outputs_b.logits[0, 1] else 'model_b'\n            predictions.append(pred)\n    \n    # Create submission\n    submission = pd.DataFrame({\n        'id': test['id'],\n        'winner': predictions\n    })\n    submission.to_csv(\"submission.csv\", index=False)\n    print(\"Submission file created successfully!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2025-01-26T23:39:25.431098Z","iopub.execute_input":"2025-01-26T23:39:25.431420Z","iopub.status.idle":"2025-01-27T02:46:21.400485Z","shell.execute_reply.started":"2025-01-26T23:39:25.431398Z","shell.execute_reply":"2025-01-27T02:46:21.399754Z"}}},{"cell_type":"markdown","source":"import os\nimport gc\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\nimport warnings\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport transformers\nfrom datasets import Dataset\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType\n)\nfrom transformers import (\n    GemmaTokenizerFast,\n    GemmaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    BitsAndBytesConfig\n)\n\n# Set environment variables for better memory management\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nwarnings.simplefilter('ignore')\n\n@dataclass\nclass PATHS:\n    train_path: str = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet'\n    test_path: str = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet'\n    model_path: str = '/kaggle/input/googlegemma-7b/transformers/default/1/gemma-2b'\n    output_dir: str = './gemma-finetuned'\n\n@dataclass\nclass CFG:\n    max_length: int = 256\n    train_batch_size: int = 1\n    eval_batch_size: int = 1\n    num_epochs: int = 3\n    learning_rate: float = 1e-4\n    seed: int = 42\n    gradient_accumulation_steps: int = 16\n    warmup_ratio: float = 0.1\n    valid_size: float = 0.1\n    \n    # LoRA specific configs\n    lora_r: int = 8\n    lora_alpha: int = 32\n    lora_dropout: float = 0.1\n\ndef cleanup():\n    \"\"\"Aggressive cleanup of GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ndef set_seed(seed: int = 42) -> None:\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        for k, v in inputs.items():\n            if isinstance(v, torch.Tensor):\n                inputs[k] = v.to(model.device)\n        \n        outputs = model(**inputs)\n        loss = outputs.loss\n        \n        return (loss, outputs) if return_outputs else loss\n\ndef prepare_training_data(df: pd.DataFrame, tokenizer: GemmaTokenizerFast, max_length: int) -> Dataset:\n    def prepare_features(examples):\n        texts = [\n            f\"<prompt>: {prompt}\\n\\n<response_a>: {resp_a}\\n\\n<response_b>: {resp_b}\"\n            for prompt, resp_a, resp_b in zip(\n                examples['prompt'],\n                examples['response_a'],\n                examples['response_b']\n            )\n        ]\n        \n        tokenized = tokenizer(\n            texts,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n        )\n        \n        labels = [0 if winner == 'model_a' else 1 for winner in examples['winner']]\n        \n        return {\n            'input_ids': tokenized['input_ids'],\n            'attention_mask': tokenized['attention_mask'],\n            'labels': labels\n        }\n    \n    dataset = Dataset.from_pandas(df)\n    return dataset.map(\n        prepare_features,\n        batched=True,\n        remove_columns=dataset.column_names,\n        desc=\"Processing dataset\"\n    )\n\ndef create_peft_config():\n    return LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        r=CFG.lora_r,\n        lora_alpha=CFG.lora_alpha,\n        lora_dropout=CFG.lora_dropout,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        modules_to_save=[\"score\"]\n    )\n\ndef train():\n    set_seed(CFG.seed)\n    cleanup()\n    \n    if torch.cuda.is_available():\n        torch.cuda.set_device(0)\n    \n    print(\"Loading data...\")\n    train_df = pl.read_parquet(PATHS.train_path).to_pandas()\n    \n    train_df, valid_df = train_test_split(\n        train_df, \n        test_size=CFG.valid_size, \n        random_state=CFG.seed,\n        stratify=train_df['winner']\n    )\n    \n    print(\"Loading tokenizer and model...\")\n    tokenizer = GemmaTokenizerFast.from_pretrained(\n        PATHS.model_path,\n        padding_side=\"right\",\n        truncation_side=\"right\",\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Configure 4-bit quantization\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n    )\n    \n    # Load model with quantization\n    print(\"Loading model with 4-bit quantization...\")\n    model = GemmaForSequenceClassification.from_pretrained(\n        PATHS.model_path,\n        num_labels=2,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        use_cache=False\n    )\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    print(\"Preparing model for LoRA...\")\n    peft_config = create_peft_config()\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    \n    print(\"Preparing datasets...\")\n    train_dataset = prepare_training_data(train_df, tokenizer, CFG.max_length)\n    valid_dataset = prepare_training_data(valid_df, tokenizer, CFG.max_length)\n    cleanup()\n    \n    training_args = TrainingArguments(\n        output_dir=PATHS.output_dir,\n        num_train_epochs=CFG.num_epochs,\n        per_device_train_batch_size=CFG.train_batch_size,\n        per_device_eval_batch_size=CFG.eval_batch_size,\n        gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n        warmup_ratio=CFG.warmup_ratio,\n        learning_rate=CFG.learning_rate,\n        fp16=True,\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"steps\",\n        save_steps=500,\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        report_to='none',\n        remove_unused_columns=False,\n        dataloader_pin_memory=False,\n        gradient_checkpointing=True,\n    )\n    \n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(\n            tokenizer=tokenizer,\n            padding=True,\n            max_length=CFG.max_length\n        ),\n    )\n    \n    print(\"Starting training...\")\n    trainer.train()\n    \n    model.save_pretrained(PATHS.output_dir)\n    tokenizer.save_pretrained(PATHS.output_dir)\n    print(f\"Model saved to {PATHS.output_dir}\")\n    \n    return model, tokenizer\n\ndef predict(model, tokenizer, test_df: pd.DataFrame):\n    test_dataset = prepare_training_data(test_df, tokenizer, CFG.max_length)\n    \n    trainer = CustomTrainer(\n        model=model,\n        args=TrainingArguments(\n            output_dir=\"./\",\n            per_device_eval_batch_size=CFG.eval_batch_size,\n            report_to='none'\n        ),\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(\n            tokenizer=tokenizer,\n            padding=True,\n            max_length=CFG.max_length\n        ),\n    )\n    \n    predictions = trainer.predict(test_dataset).predictions\n    return np.argmax(predictions, axis=1)\n\ndef main():\n    print(f'PyTorch version: {torch.__version__}')\n    print(f'Transformers version: {transformers.__version__}')\n    \n    try:\n        model, tokenizer = train()\n        \n        test_df = pl.read_parquet(PATHS.test_path).to_pandas()\n        predictions = predict(model, tokenizer, test_df)\n        \n        test_df['winner'] = ['model_a' if pred == 0 else 'model_b' for pred in predictions]\n        test_df[['id', 'winner']].to_csv('submission.csv', index=False)\n        print(\"Predictions saved to submission.csv\")\n        \n    finally:\n        cleanup()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2025-01-28T04:09:50.681181Z","iopub.execute_input":"2025-01-28T04:09:50.681553Z","iopub.status.idle":"2025-01-28T04:09:54.094237Z","shell.execute_reply.started":"2025-01-28T04:09:50.681526Z","shell.execute_reply":"2025-01-28T04:09:54.092954Z"}}},{"cell_type":"markdown","source":"!pip install -q -U bitsandbytes\n!pip install -q -U accelerate\n!pip install -q -U peft","metadata":{"execution":{"iopub.status.busy":"2025-01-28T04:11:29.538843Z","iopub.execute_input":"2025-01-28T04:11:29.539285Z","iopub.status.idle":"2025-01-28T04:21:36.920658Z","shell.execute_reply.started":"2025-01-28T04:11:29.539248Z","shell.execute_reply":"2025-01-28T04:21:36.919585Z"}}},{"cell_type":"code","source":"import os\nimport gc\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\nimport warnings\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport transformers\nfrom datasets import Dataset\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    TaskType\n)\nfrom transformers import (\n    GemmaTokenizerFast,\n    GemmaForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding\n)\n\n# Suppress warnings and configure environment\nwarnings.simplefilter('ignore')\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n@dataclass\nclass PATHS:\n    train_path: str = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet'\n    test_path: str = '/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet'\n    model_path: str = '/kaggle/input/googlegemma-7b/transformers/default/1/gemma-2b'\n    output_dir: str = './gemma-finetuned'\n\n@dataclass\nclass CFG:\n    max_length: int = 256\n    train_batch_size: int = 1\n    eval_batch_size: int = 1\n    num_epochs: int = 3\n    learning_rate: float = 1e-4\n    seed: int = 42\n    gradient_accumulation_steps: int = 16\n    warmup_ratio: float = 0.1\n    valid_size: float = 0.1\n    \n    # LoRA specific configs\n    lora_r: int = 8\n    lora_alpha: int = 32\n    lora_dropout: float = 0.1\n\ndef cleanup():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ndef set_seed(seed: int = 42):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        device = next(model.parameters()).device\n        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n                 for k, v in inputs.items()}\n        \n        outputs = model(**inputs)\n        loss = outputs.loss\n        \n        return (loss, outputs) if return_outputs else loss\n\ndef prepare_training_data(df: pd.DataFrame, tokenizer: GemmaTokenizerFast, max_length: int) -> Dataset:\n    def prepare_features(examples):\n        texts = [\n            f\"<prompt>: {prompt}\\n\\n<response_a>: {resp_a}\\n\\n<response_b>: {resp_b}\"\n            for prompt, resp_a, resp_b in zip(\n                examples['prompt'],\n                examples['response_a'],\n                examples['response_b']\n            )\n        ]\n        \n        tokenized = tokenizer(\n            texts,\n            max_length=max_length,\n            padding='max_length',\n            truncation=True,\n        )\n        \n        labels = [0 if winner == 'model_a' else 1 for winner in examples['winner']]\n        \n        return {\n            'input_ids': tokenized['input_ids'],\n            'attention_mask': tokenized['attention_mask'],\n            'labels': labels\n        }\n    \n    dataset = Dataset.from_pandas(df)\n    return dataset.map(\n        prepare_features,\n        batched=True,\n        remove_columns=dataset.column_names,\n        desc=\"Processing dataset\",\n        num_proc=1\n    )\n\ndef create_peft_config():\n    return LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        r=CFG.lora_r,\n        lora_alpha=CFG.lora_alpha,\n        lora_dropout=CFG.lora_dropout,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n        modules_to_save=[\"score\"],\n        inference_mode=False\n    )\n\ndef train():\n    set_seed(CFG.seed)\n    cleanup()\n    \n    print(\"Loading data...\")\n    train_df = pl.read_parquet(PATHS.train_path).to_pandas()\n    \n    train_df, valid_df = train_test_split(\n        train_df, \n        test_size=CFG.valid_size, \n        random_state=CFG.seed,\n        stratify=train_df['winner']\n    )\n    \n    print(\"Loading tokenizer and model...\")\n    tokenizer = GemmaTokenizerFast.from_pretrained(\n        PATHS.model_path,\n        padding_side=\"right\",\n        truncation_side=\"right\",\n    )\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load model with bf16 instead of fp16\n    model = GemmaForSequenceClassification.from_pretrained(\n        PATHS.model_path,\n        num_labels=2,\n        torch_dtype=torch.bfloat16,  # Changed to bfloat16\n        device_map=\"auto\",\n        use_cache=False\n    )\n    model.config.pad_token_id = model.config.eos_token_id\n    \n    print(\"Preparing model for LoRA...\")\n    peft_config = create_peft_config()\n    model = get_peft_model(model, peft_config)\n    \n    model.print_trainable_parameters()\n    \n    print(\"Preparing datasets...\")\n    train_dataset = prepare_training_data(train_df, tokenizer, CFG.max_length)\n    valid_dataset = prepare_training_data(valid_df, tokenizer, CFG.max_length)\n    cleanup()\n    \n    training_args = TrainingArguments(\n        output_dir=PATHS.output_dir,\n        num_train_epochs=CFG.num_epochs,\n        per_device_train_batch_size=CFG.train_batch_size,\n        per_device_eval_batch_size=CFG.eval_batch_size,\n        gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n        warmup_ratio=CFG.warmup_ratio,\n        learning_rate=CFG.learning_rate,\n        bf16=True,  # Changed to bf16\n        fp16=False,  # Disabled fp16\n        logging_steps=100,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"steps\",\n        save_steps=500,\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        report_to='none',\n        remove_unused_columns=False,\n        dataloader_pin_memory=False,\n        gradient_checkpointing=True,\n        optim=\"adamw_torch\",\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,\n        weight_decay=0.01,\n        lr_scheduler_type=\"cosine\",\n        ddp_find_unused_parameters=False,\n    )\n    \n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=valid_dataset,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(\n            tokenizer=tokenizer,\n            padding=True,\n            max_length=CFG.max_length\n        ),\n    )\n    \n    print(\"Starting training...\")\n    trainer.train()\n    \n    model.save_pretrained(PATHS.output_dir)\n    tokenizer.save_pretrained(PATHS.output_dir)\n    print(f\"Model saved to {PATHS.output_dir}\")\n    \n    return model, tokenizer\n\ndef predict(model, tokenizer, test_df: pd.DataFrame):\n    test_dataset = prepare_training_data(test_df, tokenizer, CFG.max_length)\n    \n    trainer = CustomTrainer(\n        model=model,\n        args=TrainingArguments(\n            output_dir=\"./\",\n            per_device_eval_batch_size=CFG.eval_batch_size,\n            report_to='none'\n        ),\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(\n            tokenizer=tokenizer,\n            padding=True,\n            max_length=CFG.max_length\n        ),\n    )\n    \n    predictions = trainer.predict(test_dataset).predictions\n    return np.argmax(predictions, axis=1)\n\ndef main():\n    print(f'PyTorch version: {torch.__version__}')\n    print(f'Transformers version: {transformers.__version__}')\n    \n    try:\n        model, tokenizer = train()\n        \n        test_df = pl.read_parquet(PATHS.test_path).to_pandas()\n        predictions = predict(model, tokenizer, test_df)\n        \n        test_df['winner'] = ['model_a' if pred == 0 else 'model_b' for pred in predictions]\n        test_df[['id', 'winner']].to_csv('submission.csv', index=False)\n        print(\"Predictions saved to submission.csv\")\n        \n    finally:\n        cleanup()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T04:50:37.583889Z","iopub.execute_input":"2025-01-28T04:50:37.584294Z","iopub.status.idle":"2025-01-28T04:53:23.290314Z","shell.execute_reply.started":"2025-01-28T04:50:37.584264Z","shell.execute_reply":"2025-01-28T04:53:23.288438Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.5.1+cu121\nTransformers version: 4.47.0\nLoading data...\nLoading tokenizer and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c19b20c7485474b9a8cc42f5b6c4024"}},"metadata":{}},{"name":"stderr","text":"Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/googlegemma-7b/transformers/default/1/gemma-2b and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Preparing model for LoRA...\ntrainable params: 1,847,296 || all params: 2,508,023,808 || trainable%: 0.0737\nPreparing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing dataset:   0%|          | 0/43595 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded1702fab484473952f38341d81eab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing dataset:   0%|          | 0/4844 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0681a7e43694a72848aefd7943d1a2b"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-2f2c410dfb7c>\u001b[0m in \u001b[0;36m<cell line: 260>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-2f2c410dfb7c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATHS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2f2c410dfb7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATHS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3653\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-2f2c410dfb7c>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                  for k, v in inputs.items()}\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForSequenceClassificationLoss\u001b[0;34m(labels, pooled_logits, config, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}